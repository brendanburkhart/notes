\chapter{Signals and Systems}
\label{ch:signals}

\section{Introduction}

Signals can be broadly classified into discrete and continuous signals. For example, a continuous signal might be a function of a real variable representing time, or real variables representing a position in space, while a discrete signal is indexed by one or more integer variables. The variable(s) the signal is a function of are the \emph{independent variable(s)} of the signal, the values of the function are the dependent variables. For example, the voltage through a resistor could be the dependent variable of a signal where time is the independent variable.

Signals in the real world are usually continuous signals, but are often discretized for representing in computers and data into discrete signals. A \emph{digital} signal is a discrete signal where the dependent variable(s) have finite precision, which is what signals in a digital representation must be.

The value of a continuous signal $f$ at time $t \in \R$ is commonly denoted by $f(t)$, while a discrete signal $f$ at step $n \in \N$ would be denoted by $f[n]$. That is, parentheses are used for continuous signals, and square brackets for discrete.

Systems are processes, either physical ones like electronics and mechanics, or virtual ones like a software algorithm, that have input and output signals, generating the output signals based on the input signals and any internal state.

\section{Geometric sums}

\begin{prop}
    For $r \neq 1$,
    \[\sum_{k=0}^n{ar^k} = a\frac{1-r^{n-1}}{1-r}.\]
\end{prop}

\begin{prop}
    For $\abs{r} < 1$,
    \[\sum_{k=0}^\infty{ar^k} = a\frac{1}{1-r}.\]
\end{prop}

\section{Shifting and scaling signals}

Consider a continuous signal $f(t)$. We can shift and scale the amplitude of this signal, or we can shift and scale the frequency (or both of course).

Given a signal $f(t)$, $af(t)$ will \emph{scale} the amplitude by a factor of $a$, while $f(t) + b$ will \emph{shift} the amplitude by $b$. Figure \ref{fig:amplitude-scale-shifts} shows a signal $f(t)$ in blue, as well as:
\begin{itemize}
    \item $2f(t)$ in red,
    \item $\frac{1}{2}f(t)$ in green,
    \item and $f(t) + \frac{1}{2}$ in orange.
\end{itemize}

\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}[scale=1.0]
        \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            ymin=0,ymax=2.5,ylabel=$y$,
            xmin=-3,xmax=3,xlabel=$x$
            ]

            \addplot[domain=-3:3, blue, ultra thick, samples=100] {e^(-x^2)};
            \addplot[domain=-3:3, green, ultra thick, samples=100] {0.5*e^(-(2*x)^2)};
            \addplot[domain=-3:3, red, ultra thick, samples=100] {2*(e^(-x^2))};
            \addplot[domain=-3:3, orange, ultra thick, samples=100] {e^(-x^2) + 0.5};
        \end{axis}
    \end{tikzpicture}
\caption{Amplitude transformations}
\label{fig:amplitude-scale-shifts}
\end{figure}

Given a signal $f(t)$, $f(at)$ will \emph{scale} the time domain by a factor of $\frac{1}{a}$, while $f(t - b)$ will \emph{shift} the time domain by $b$. Note that the opposite transformation is applied to the signal than is directly applied to the function inputs. Figure \ref{fig:time-scale-shifts} shows a signal $f(t)$ in blue, as well as:
\begin{itemize}
    \item $f(2t)$ in red,
    \item $f(\frac{t}{2})$ in green,
    \item and $f(t - 1)$ in orange.
\end{itemize}

It is important to note the different between scaling and then shifting the time of the signal, versus shifting and then scaling. Figure \ref{fig:time-scale-shifts-2} shows a signal $f(t)$ in blue, as well as:
\begin{itemize}
    \item $f(2(t-1))$ in red,
    \item and $f(2t-1)$ in green.
\end{itemize}

\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}[scale=1.0]
        \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            ymin=0,ymax=1.5,ylabel=$y$,
            xmin=-3,xmax=3,xlabel=$x$
            ]

            \addplot[domain=-3:3, blue, ultra thick, samples=100] {e^(-x^2)};
            \addplot[domain=-3:3, green, ultra thick, samples=100] {e^(-(x/2)^2)};
            \addplot[domain=-3:3, red, ultra thick, samples=100] {(e^(-(2*x)^2))};
            \addplot[domain=-3:3, orange, ultra thick, samples=100] {e^(-(x-1)^2)};
        \end{axis}
    \end{tikzpicture}
\caption{Time transformations}
\label{fig:time-scale-shifts}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}[scale=1.0]
        \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            ymin=0,ymax=1.5,ylabel=$y$,
            xmin=-3,xmax=3,xlabel=$x$
            ]

            \addplot[domain=-3:3, blue, ultra thick, samples=100] {e^(-x^2)};
            \addplot[domain=-3:3, green, ultra thick, samples=100] {e^(-(2*x-1)^2)};
            \addplot[domain=-3:3, red, ultra thick, samples=100] {e^(-(2*(x-1))^2};
        \end{axis}
    \end{tikzpicture}
\caption{Time transformations 2}
\label{fig:time-scale-shifts-2}
\end{figure}

\section{Common signals}

Two of the most common signals are the unit impulse signal, and the unit step signal. The unit impulse is also known as the (Dirac) delta function. In discrete time it is commonly denoted by $\delta[n]$, and is defined by
\[\delta[n] =
\begin{dcases}
    1, & n = 0 \\
    0, & n \neq 0
\end{dcases}.\] The unit step signal $u[n]$ is defined by
\[u[n] = \sum_{m=-\infty}^{n}\delta[m].\] Note that $\delta[n] = u[n] - u[n-1]$.

\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}[scale=1.0]
        \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            ymin=0,ymax=1.5,ylabel=$y$,
            xmin=-3,xmax=3,xlabel=$x$
            ]

            \addplot+[const plot, no marks, ultra thick] coordinates {(-3,0) (0,0) (0,1) (0,0) (3,0)};
        \end{axis}
    \end{tikzpicture}
\caption{Discrete unit impulse signal}
\label{fig:discrete-unit-impulse}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}[scale=1.0]
        \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            ymin=0,ymax=1.5,ylabel=$y$,
            xmin=-3,xmax=3,xlabel=$x$
            ]

            \addplot+[const plot, no marks, ultra thick] coordinates {(-3,0) (0,0) (0,1) (3,1)};
        \end{axis}
    \end{tikzpicture}
\caption{Discrete unit step signal}
\label{fig:discrete-unit-step}
\end{figure}

The delta function can be used to sift out everything but one value of a discrete time signal.
\[x[n]\delta[n-n_0] = x[n_0]\delta[n-n_0]\]
\[\sum_{m=-\infty}^n x[m]\delta[m-n_0] = x[n_0]\]

In continuous time, we can define the unit step function similarly as \[u(t) =
\begin{dcases}
    1, & t > 0 \\
    0, & t < 0
\end{dcases}.\]
Note that $u(0)$ is undefined. Defining the continuous time impulse is a bit more challenging. We would like it to be $\delta(t)$ such that
\[u(t) = \int_{-t}^{t}\delta(\tau)d\tau.\] We can therefore define it has $\delta(t) = \lim_{\Delta\to 0}\delta_{\Delta}(t)$, where $\delta_{\Delta}(t)$ is a step impulse with width $\Delta$ and height $\frac{1}{\Delta}$ beginning at $t = 0$.

Note the similarities between the discrete and continuous time version of these signals -- the first difference and first derivative play the same roles, as do the sums and integrals.

Since the continuous time delta function is not actually a function (it's technically a \emph{distribution}), behaves somewhat differently than may be expected. For example, \[\int_{-\infty}^{\infty}x(t)\delta(at)dt = \frac{1}{\abs{a}}x(0).\]

Another signal is the \emph{unit ramp}, which in continuous time is simply \[r(t) = \int_{-\infty}^{t}u(\tau)d\tau.\] It is defined similarly in discrete time, however the running sum is shifted slightly so that $r[0] = 0$, in order to match the continuous time unit ramp. \[r[n] = \sum_{k=-\infty}^{n-1}u[k]= nu[n] = \begin{dcases}
    n, & n \geq 0 \\ 0, & n < 0
\end{dcases}.\]

\section{Energy and power}

\begin{defn}
    Let $x(t)$ be a continuous time signal. The \emph{energy} of the signal is
    \[E_{\infty} = \lim_{T\to\infty}\int_{-T}^{T}\abs{x(t)}^2dt.\]
\end{defn}

\begin{defn}
    Let $x(t)$ be a continuous time signal. The \emph{power} of the signal is
    \[P_{\infty} = \lim_{T \to \infty}\frac{1}{2T}\int_{-T}^T\abs{x(t)}^2dt.\]
\end{defn}

\begin{defn}
    Let $x[n]$ be a discrete time signal. The \emph{energy} of the signal is
    \[E_{\infty} = \lim_{N\to\infty}\sum_{n=-N}^{N}\abs{x[n]}^2.\]
\end{defn}

\begin{defn}
    Let $x[n]$ be a discrete time signal. The \emph{power} of the signal is
    \[P_{\infty} = \lim_{N \to \infty}\frac{1}{2N+1}\sum_{n=-N}^N\abs{x[n]}^2.\]
\end{defn}

Note that if a signal has finite energy, then the signal must approach zero sufficiently fast as time approaches $\pm\infty$, or equal zero outside some interval.

\begin{defn}
    An \emph{energy signal} is a signal with finite energy. Such signals must have zero power.
\end{defn}

\begin{exmp}
    $x(t) = e^{-\abs{t}}$ is an energy signal.
\end{exmp}

\begin{defn}
    A \emph{power signal} is a signal with finite, non-zero power. Such signals must have infinite energy.
\end{defn}

\begin{exmp}
    $x(t) = 1$ is a power signal, as is $x(t) = \sin(t)$.
\end{exmp}

Some signals are neither energy nor power signals, such as $x(t) = e^t$ or $x(t) = t$.

\begin{exmp}
    Let $x(t) = e^{-at}u(t)$ for some $a > 0$.
    \[E_{\infty} = \int_{-\infty}^{\infty}\abs{x(t)}^2dt = \int_{0}^{\infty}e^{-2at}dt = \frac{1}{-2a}(0 - 1) = \frac{1}{2a}.\] Since $a > 0$, $x(t)$ is therefore an energy signal and has zero power.
\end{exmp}

\begin{exmp}
    Let $x(t) = u(t)$. Then we have
    \[P_{\infty} = \lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}\abs{x(t)}^2dt = \lim_{T\to\infty}\frac{1}{2T}\int_{0}^{T}dt = \frac{1}{2}.\] Therefore, $x(t)$ is a power signal and has infinite energy.
\end{exmp}

\section{Even and odd}

Recall that a function $f$ is even if $f(x) = f(-x)$ for all $x$ and odd if $f(x) = -f(-x)$. Obviously most functions are neither, but all functions can be decomposed into the sum of an even and odd part.

\begin{defn}
    Let $f(x)$ be a function, such as a signal. The \emph{even part} of $f$ is \[f_{\mathrm{ev}}(x) = \frac{f(x) + f(-x)}{2},\] and the \emph{odd part} of $f$ is \[f_{\mathrm{od}}(x) = \frac{f(x) - f(-x)}{2}.\]
\end{defn}

Note that the even part of a function must be even, since \[f_{\mathrm{ev}}(-x) = \frac{f(-x) + f(x)}{2} = \frac{f(x) + f(-x)}{2} = f_{\mathrm{ev}}(x),\] and the odd part must be odd: \[-f_{\mathrm{od}}(-x) = \frac{-f(-x)+f(x)}{2} = \frac{f(x) - f(-x)}{2} = f_{\mathrm{od}}(x).\]

\begin{exmp}
    $5x^2$ is even, while $x^3$ is odd, but $f(x) = x^3 + 5x^2$ is neither. \[f_{\mathrm{ev}}(x) = \frac{(x^3 + 5x^2) + ((-x)^3) + 5(-x)^2)}{2} = \frac{(x^3 - x^3) + (5x^2 + 5x^2)}{2} = 5x^2.\]
    \[f_{\mathrm{od}}(x) = \frac{(x^3 + 5x^2) - ((-x)^3) + 5(-x)^2)}{2} = \frac{(x^3 + x^3) + (5x^2 - 5x^2)}{2} = x^3.\]
\end{exmp}

\begin{prop}
    Let $f(x)$ be a function. Then $f(x) = f_{\mathrm{ev}}(x) + f_{\mathrm{od}}(x)$.
\end{prop}

\begin{proof}
    \begin{align*}
        f_{\mathrm{ev}}(x) + f_{\mathrm{od}}(x) &= \\
        \frac{f(x) + f(-x)}{2} + \frac{f(x) - f(-x)}{2} &= \\
        \frac{f(x) + f(-x) + f(x) - f(-x)}{2} &= \\
        \frac{2f(x)}{2} = f(x)
    \end{align*}
\end{proof}

\section{Period and frequency}

\begin{defn}
    A signal $x(t)$ is \emph{bounded} if there exists some $K$ such that $\abs{x(t)} < K$ for all $t$.
\end{defn}

\begin{defn}
    A signal $x(t)$ is \emph{periodic} if there exists some $T \in \R$ such that $x(t) = x(t + T)$ for all $T$. Note that if $f(t)$ is periodic with period $T$, it must also be periodic with period $kT$ where $k \in \Z^+$. The smallest such $T$ is called $T_0$, the \emph{fundamental period}.
\end{defn}

\begin{rmk}
    For discrete time signals, it is instead required that $T \in \Z^+$.
\end{rmk}

\begin{rmk}
    Constant signals are somewhat of an edge case with respect to periodicity. They are clearly periodic for any period $T > 0$, but have no fundamental period, and hence no fundamental frequency.
\end{rmk}

\begin{defn}
    Given that $x(t)$ is periodic with fundamental period $T_0$, the \emph{fundamental frequency} of $x(t)$ is $\omega_0 = \frac{2\pi}{T_0}$. Some sources instead call $f_0$ the fundamental frequency where $f_0 = \frac{1}{T_0}$.
\end{defn}

\begin{exmp}
    Let $x(t) = \sin(3t)$. If $x(t)$ is periodic with some period $T$, then it must be that $x(t) = x(t + T)$, so $\sin(3t) = \sin(3(t + T)) = \sin(3t + 3T)$. We know that $\sin$ is periodic with fundamental period $2\pi$, so $3T_0 = 2\pi$ and therefore $T_0 = \frac{2\pi}{3}$.
\end{exmp}

\begin{exmp}
    Let $x(t) = e^{\pi(1 + j)t}$. Then we have $x(t) = x(t + T)$ which implies \[e^{\pi(1 + j)t} = e^{\pi(1 + j)(t + T)} = e^{\pi(1 + j)t}e^{\pi(1 + j)T}.\] Since $e^x \neq 0$ for all $x$, it follows that $e^{\pi(1 + j)T} = 1$, and so it must be that $\pi(1 + j)T = 2k{\pi}$ for some $k \in \Z$ by Euler's formula. Therefore, $T = \frac{2k}{1 + j}$. However, $T \notin \R$, so $x(t)$ is not periodic.
\end{exmp}

\begin{rmk}
    If a \emph{non-zero} signal is both \emph{periodic} and \emph{bounded}, it must be a power signal.
\end{rmk}

\begin{rmk}
    Signals which are periodic in continuous time may not be in discrete time. For example, we saw that $x(t) = \sin(3t)$ is periodic, but $x[n] = \sin(3n)$ is not, as there is no $N \in \Z^+$ such that $N = \frac{2\pi}{3}$.
\end{rmk}

\begin{exmp}
    Let $x[n] = \cos(\frac{\pi}{8}n^2)$. If $x[n]$ is periodic, then there exists some constant $N \in \N$ such that $x[n] = x[n+N]$ for all $n \in \Z$. Since cosine has period $2\pi$, it follows that
    $\frac{\pi}{8}n^2 = \frac{\pi}{8}(n+N)^2 + 2k\pi$ for some $k \in \Z$, and since $(n+N)^2 - n^2 = 2nN + N^2$, we have \[\frac{2nN+N^2}{16} = k.\] Since $n$ can be any integer, and $\frac{2nN}{16}$ must be an integer, $N$ must be divisible by $8$. Using $N = 8$ gives $\frac{16n + 4(16)}{16} = 4 + n = k$, so $x[n]$ is periodic with fundamental frequency $N_0 = 8$.
\end{exmp}

\section{Exponential signals}

\subsection{Continuous time}

\begin{defn}
    An \emph{exponential signal} is a signal of the form $x(t) = ce^{at}$, for some $c, a \in \C$ and $t \in \R$.
\end{defn}

When $c, a \in \R$, the exponential signal is similar to the standard exponential function. If $a > 0$, the signals grows exponentially, if $a < 0$ it decreases exponentially, and in any case it is scaled by $c$.

When $c \in \C$ and $a$ is strictly imaginary, the signal is called a \emph{phasor}. By Euler's formula, we have \[ce^{at} = \abs{c}e^{i\phi_0}e^{at} = \abs{c}e^{i(\omega_0t + \phi_0)} = \abs{c}\left(\cos(\omega_0t + \phi_0) + i\sin(\omega_0t + \phi_0)\right),\] where $i\omega_0 = a$ and $\phi_0 = \angle c$. A phasor can be interpreted as a vector-valued function, which always have magnitude $\abs{c}$, and has angular frequency $\omega_0$ (radians/time). At time $t = 0$, the angle is $\phi_0$, which may be called the \emph{phase} of the signal. Since the signal has angular frequency $\omega_0$, it has fundamental period $T_0 = \frac{2\pi}{\abs{w_0}}$.

\begin{thm}
    Let $x(t) = c_1e^{i\omega_1t} + c_2e^{i\omega_2t}$. This signal is periodic if and only if there exists some $\omega > 0$ and $k_1, k_2 \in \Z$ such that $\omega_1 = k_1\omega$ and $\omega_2 = k_2\omega$. If it is periodic, it has fundamental period $T_0 = \frac{2\pi}{\omega_0}$, where $\omega_0$ is the largest possible value for $\omega$.
\end{thm}

\begin{rmk}
    The required condition is equivalent to $\frac{\omega_1}{\omega_2} \in \Q$. Note that this theorem can be generalized to signals that are the sum of $n$ phasors.
\end{rmk}

\begin{exmp}
    Let $x(t) = 4e^{i2t} - 5e^{i3t}$. Since $2 = 2\cdot 1$ and $3 = 3 \cdot 1$, $x(t)$ is periodic with fundamental frequency $w_0 = 1$ and fundamental period $T_0 = \frac{2\pi}{\abs{1}} = 2\pi$.
\end{exmp}

\begin{exmp}
    Let $x(t) = 4e^{i2t} - 5e^{i\pi t}$. Since $\frac{2}{\pi} \not\in \Q$, $x(t)$ is not periodic.
\end{exmp}

\begin{defn}
    If a signal $x(t) = c_1e^{i\omega_1t} + c_2e^{i\omega_2t} + \cdots + c_ke^{i\omega_kt}$ is periodic, then we say the terms are \emph{harmonically related}.
\end{defn}

When $c, a \in \C$, and $a$ is not strictly imaginary, then the resulting exponential signal is a phasor given by $c$ and the imaginary part of $a$, all multiplied by $e^{\sigma_0}$, where $\sigma_0$ is the real part of $a$.

\subsection{Discrete time}

\begin{defn}
    An \emph{exponential signal} is a signal of the form $x[n] = ce^{an}$, for some $c, a \in \C$ and $n \in \N$.
\end{defn}

When $c, a \in \R$, the exponential signal is similar to the standard exponential function. If $a > 0$, the signals grows exponentially, if $a < 0$ it decreases exponentially, and in any case it is scaled by $c$.

When $c \in \R$ and $a = \sigma_0 + i(2k+1)\pi$ for some $\sigma_0 \in \R$ and $k \in \Z$, we have a type of exponential signal that doesn't have a continuous time version. Since $x[n] = ce^{an}$, we have $x[n] = c\left(-e^{\sigma_0}\right)^n$. This signal therefore is simply $ce^{n\sigma_0}$, but alternating between positive and negative on each sample.

When $c \in \C$ and $a$ is strictly imaginary, the signal is a discrete time \emph{phasor}. Unlike continuous time phasors, discrete time phasors are not always periodic. For $x[n] = e^{i\omega_0n}$ to be periodic in discrete time, we need $\frac{2\pi}{\abs{\omega_0}} \in \Q$, so that some integer multiple of this value is an integer period.

\begin{thm}
    Let $x[n] = c_1e^{i\omega_1n} + c_2e^{i\omega_2n}$, where the terms are periodic with fundamental periods $N_1, N_2 \in \N$. This signal is then periodic, and has fundamental period equal to the least common multiple of $N_1$ and $N_2$.
\end{thm}

\begin{rmk}
    Note that this theorem can be generalized to signals that are the sum of $n$ discrete time phasors.
\end{rmk}

\begin{rmk}
    For a continuous time phasors, increasing $\omega_0$ always results in a phasor with higher frequency. For a discrete time phasor however, add integer multiples of $2\pi$ to $\omega_0$ least the phasor unchanged.
\end{rmk}

\section{Systems}

A \emph{system} is a process that takes an input signal and produces an output signal. For a system $S$ with output $y$ and input $x$, we write $y(t) = S[x(t)]$ for a continuous time system, or $y[n] = S[x[n]]$ for a discrete time system.

\begin{exmp}
    A system that models a capacitor with capacitance $C$ is \[y(t) = \frac{1}{C}\int_{-\infty}^{t}x(\tau)d\tau.\]
\end{exmp}

\begin{defn}
    A system is \emph{causal} if the output at time $t_0$ depends \emph{only} on inputs at times $t \leq t_0$. If a system is \emph{not} causal, we say that it is \emph{non-causal}.
\end{defn}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item An ideal predictor is a system $y(t) = x(t+1)$, which is non-causal.
        \item An ideal delay is a system $y(t) = x(t-1)$, which is causal.
        \item A centered moving average filter $y(t) = \frac{x(t-1) + x(t) + x(t+1)}{3}$ is non-causal.
    \end{itemize}
\end{exmp}

\begin{rmk}
    When determining whether a system is causal, the systems input is not necessarily time. For example, many image processing algorithms may look ahead at later pixels. Despite this being later in spatial rather temporal terms, they would be non-causal.
\end{rmk}

\begin{defn}
    A \emph{time-invariant} system is one in which the output depends only on the inputs, not on the time. If a system is not time-invariant, it may be said to be \emph{time-varying}.
\end{defn}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $y[n] = \sin(x[n])$ is time-invariant.
        \item $y(t) = 3x^2(t)u(t)$ is time-varying.
    \end{itemize}
\end{exmp}

\begin{defn}
    A system is \emph{memory-less} if the output at time $t$ depends \emph{only} on the input at time $t$.
\end{defn}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $y(t) = kx(t)$ is memory-less.
        \item $y(t) = \int_{-\infty}^{t}x(\tau)d\tau$ is not memory-less.
    \end{itemize}
\end{exmp}

\begin{rmk}
    If a system is memory-less, it is necessarily causal.
\end{rmk}

\begin{defn}
    A system $S$ is \emph{linear} if it is additive and homogeneous. For input signals $x_1(t), x_2(t)$ with $y_1(t) = S[x_1(t)]$ and $y_2(t) = S[x_2(t)]$, a system is additive if $S[x_1(t) + x_2(t)] = y_1(t) + y_2(t)$ and homogeneous if $S[ax_1(t)] = ay_1(t)$.
\end{defn}

\begin{prop}
    A system $S$ is linear if $S[ax_1(t) + x_2(t)] = ay_1(t) + y_2(t)$.
\end{prop}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $y(t) = t^2x(t)$ is both additive and homogeneous, so it is linear.
        \item $y(t) = x^2(t)$ is neither additive nor homogeneous, so it is non-linear.
        \item $y[n] = \bar{x}[n]$ is additive, but not homogeneous (since $S[ax[n]] = \bar{a}\bar{x}[n] \neq a\bar{x}[n]$), so it is non-linear.
    \end{itemize}
\end{exmp}

\begin{defn}
    A system is \emph{bounded input/bounded output} stable if every bounded input results in a bounded output. That is, if \[\abs{x(t)} < K \forall t \implies \abs{y(t)} < M \forall T,\] for some $K, M$.
\end{defn}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $y(t) = e^{x(t)}$ is stable.
        \item $y(t) = e^tx(t)$ is unstable.
        \item $y(t) = \sin(t)x(t)$ is stable.
        \item $y[n] = 12x[n-1]$ is stable.
    \end{itemize}
\end{exmp}

\begin{defn}
    A system $S$ is \emph{invertible} if there exists $S^{-1}$ such that $S^{-1}[S[x(t)]] = x(t)$ for all $x(t)$.
\end{defn}

\begin{exmp}
    Let $S = x^3[n]$. Then $S$ is invertible, since $\sqrt[3]{y[n]} = x[n]$, and so $S^{-1} = \sqrt[3]{y(t)}$.
\end{exmp}

\begin{rmk}
    A system that is both \emph{linear} and \emph{time-invariant} may be referred to as an \emph{LTI} system.
\end{rmk}

\subsection{Discrete time linear time-invariant}

\begin{defn}
    Let $x_1[n], x_2[n]$ be signals. Then the \emph{convolution} of these signals is
    \[x_1[n] * x_2[n]= \sum_{k=-\infty}^{\infty}x_1[k]x_2[n-k].\]
\end{defn}

\begin{prop}
    For signals $x_1[n], x_2[n], x_3[n]$, and constant $\alpha$:
    \begin{itemize}
        \item $x_1[n]*x_2[n] = x_2[n]*x_1[n]$, so convolution is commutative.
        \item $x_1[n]*\left(x_2[n]*x_3[n]\right) = \left(x_1[n]*x_2[n]\right)*x_3[n]$, so convolution is associative.
        \item $x_1[n]*\left(x_2[n] + x_3[n]\right) = x_1[n]*x_2[n] + x_1[n]*x_3[n]$, so convolution distributes over addition.
        \item $(\alpha x_1[n]) * x_2[n] = x_1[n] * (\alpha x_2[n]) = \alpha(x_1[n]*x_2[n])$, so together with the previous property convolution is linear.
    \end{itemize}
\end{prop}

\begin{proof}
    \[x_1[n]*x_2[n] = \sum_{k=-\infty}^{\infty}x_1[k]x_2[n-k].\] Now let $l = n-k$. Then \[x_1[n]*x_2[n] = \sum_{l=\infty}^{-\infty}x_1[n-l]x_2[l] = \sum_{l=-\infty}^{\infty}x_2[l]x_1[n-l] = x_2[n]*x_1[n].\] The other properties can be similarly.
\end{proof}

\begin{prop}
    Let $S$ be an LTI, and let $h[n]$ be the \emph{impulse response} of the system, so $h[n] = S[\delta[n]]$. Then $y[n] = x[n] * h[n]$.
\end{prop}

\begin{proof}
    We know that $S[\delta[n]] = h[n]$, and since the system is time-invariant, we know that $S[\delta[n - k]] = h[n - k]$. Since it is linear, $S[\alpha\delta[n - k]] =\alpha h[n-k]$, and so $S[x[k]\delta[n-k]] = x[k][n-k]$. Therefore, \[\sum_{k=\infty}^{\infty}x[k]h[n-k] = \sum_{k=-\infty}^{\infty}S[x[k]\delta[n-k]].\] Since $\delta[n-k] \neq 0 \implies n=k$, we then know that \[\sum_{k=\infty}^{\infty}S[x[k]\delta[n-k]] = S[x[n]] = y[n],\] and so it follows that
    \[y[n] = \sum_{k=\infty}^{\infty}x[k]h[n-k] = x[n]*h[n].\]
\end{proof}

\begin{exmp}
    Let $S$ be an LTI system such that $S[\delta[n]] = h[n] = r[n]$. Then given $x[n] = u[n]$, we have \[y[n] = S[x[n]] = r[n]*u[n].\]
    \[y[n] = \sum_{k=-\infty}^{\infty}r[n-k]u[k] = \sum_{k=0}^{\infty}r[n-k].\] When $n < 0$, we then have $y[n] = 0$, and when $n \geq 0$, we have \[y[n] = 0 + 1 + 2 + \cdots + n = \frac{n(n-1)}{2}.\]
\end{exmp}

\begin{exmp}
    Let $S$ be an LTI system such that $S[\delta[n]] = h[n] = u[n]$. Then given $x[n] = 2^{n}u[-n]$, we have \[y[n] = S[x[n]] = 2^{n}u[-n]*u[n].\]
    \[y[n] = \sum_{k=-\infty}^{\infty}2^{k}u[-k]u[n-k] = \sum_{k=-\infty}^{0}2^{k}u[n-k].\]

    If $n \geq 0$, then since $k \leq 0$ we know $n - k \geq 0$, and so
    \[y[n] = \sum_{k=-\infty}^{0}2^k = \lim_{r\to\infty}\frac{1-(1/2)^{r}}{1-1/2} = 2.\]

    If $n < 0$, then $n-k < 0$ only when $k > n$. Therefore, $y[n] = 2 - \left(2^{n+1} + 2^{n+2} + \cdots + 2^0\right)$. Since $n < 0$, \[y[n] = 2 - \frac{1-(1/2)^{-n}}{1-(1/2)} = 2^{n+1}.\]
\end{exmp}

\begin{prop}
    Let $S$ be a discrete time LTI system with impulse response $h[n]$. Then \begin{itemize}
        \item $S$ is causal if and only if $h[n]$ is right-sided -- that is, if $h[n] = 0$ for all $n < 0$.
        \item $S$ is memoryless if and only if $h[k] = c\delta[n]$ for some $c$.
    \end{itemize} 
\end{prop}

\begin{proof}
    If $S$ is causal, then since $y[n] = \sum_{k=-\infty}^{\infty}h[k]x[n-k]$ it must be that $h[k] = 0$ for all $k < 0$. Similarly, if $h[k]$ is right-sided, then \[y[n] = \sum_{k=-\infty}^{\infty}h[k]x[n-k] = \sum_{k=0}^{\infty}x[n-k],\] and so $S$ is causal.

    If $S$ is memoryless, then clearly $h[k] = 0$ when $k \neq 0$, so $h[k] = c\delta[k]$ for some $c$, and if $h[k] = c\delta[k]$, then $y[n] = x[0]h[n]$, so $S$ is memoryless.
\end{proof}

\begin{prop}
    Let $S$ be a DT LTI system with impulse response $h[n]$. $S$ is stable if and only if $h[n]$ is absolutely summable: there exists some finite $M$ such that \[\sum_{n=-\infty}^{\infty}\abs{h[n]} < M.\]
\end{prop}

\begin{proof}
    If $S$ is stable, then for $x[n] = 1$ we have $\abs{y[n]} < M$ for some $M$.
    \[y[n] = \sum_{k=-\infty}^{\infty}h[k]x[n-k] = \sum_{k=-\infty}^{\infty}h[k]x[n-k] < M.\]

    If such $M$ exists, then $\abs{x[n]} < K$ for some $K$ implis that
    \begin{align*}
        \abs{y[n]} \leq \sum_{k=-\infty}\abs{h[k]}\abs{x[n-k]} < K\sum_{k=-\infty}^{\infty} < KM < \infty.
    \end{align*}
\end{proof}

\begin{prop}
    Let $S$ be a DT LTI system with impulse response $h[n]$. $S$ is invertible if there exists $h_{I}[n]$ such that $h[n] * h_{I}[n] = \delta[n]$.
\end{prop}

\begin{proof}
    Since $y[n] = x[n] * h[n]$, we know that
    \begin{align*}
        y[n] * h_{I}[n] &= (x[n] * h[n]) * h_I[n] = x[n] * (h[n] * h_I[n]) \\ &= x[n] * \delta[n] = x[n],
    \end{align*}
    so the system is invertible.
\end{proof}

\begin{exmp}
    Let $S$ be a DT LTI system with impulse response $h[n] = u[n]$. If we can find $h_I[n]$ such that $h[n] * h_I[n] = \delta[n]$, then $S$ is invertible. Therefore, we want
    \[\sum_{k=-\infty}^{\infty}u[k]h_I[n-k] = \delta[n].\] Since $\delta[n] = 0$ when $n \neq 0$, it follows that one possibility for $h_I[n-k]$ is $\delta[n] - \delta[n-1]$, and so $S$ is invertible.
\end{exmp}

\begin{rmk}
    If $x[n]$ can be expressed as a finite number of delta signals, so
    \[x[n] = x[n_0]\delta[n-n_0] + x[n_1]\delta[n-n_1] + \cdots + x[n_k]\delta[n-n_k],\] then clearly
    \begin{align*}
        y[n] &= \sum_{k=-\infty}^{\infty}x[k]h[n-k] \\
        &= x[n_0]\sum_{k=-\infty}^{\infty}\delta[k-n_0]h[n-k] + \cdots + x[n_k]\sum_{k=-\infty}^{\infty}\delta[k-n_k]h[n-k] \\
        &= x[n_0]h[n-n_0] + \cdots + x[n_k]h[n-n_k]
    \end{align*}
\end{rmk}

\begin{defn}
    An \emph{eigenfunction} of a system is a signal that is produces the same signal as output, up to a multiplicative constant. That is, for a system $S$, an eigenfunction is a signal $x$ such that $S[x] = cx$ for some $c$.
\end{defn}

\begin{prop}
    A real growing exponential signal is an eigenfunction of any stable and causal discrete time LTI system.
\end{prop}

\begin{proof}
    Let $S$ be a stable and causal DT LTI system with impulse response $h[n]$, and $x[n] = e^{\sigma_0n}$, where $\sigma_0 \geq 0$. Then
    \begin{align}
        y[n] = \sum_{k=-\infty}^{\infty}h[k]e^{\sigma_0(n-k)} = e^{\sigma_0n}\sum_{k=-\infty}^{\infty}h[k]e^{-\sigma_0k)}.
    \end{align}
    Since the system is causal, we know that $h[n] = 0$ for $n < 0$, so
    \begin{align*}
        y[n] = e^{\sigma_0n}\sum_{k=0}^{\infty}h[k]e^{-\sigma_0k)}.
    \end{align*}
    Since the system is stable, $h[n]$ is absolutely summable and so since $e^{-\sigma_0k} \leq 1$ for $k \geq 0$, we know that there is some constant $M$ such that
    \begin{align*}
        \abs{\sum_{k=0}^{\infty}h[k]e^{-\sigma_0k)}} \leq \sum_{k=0}^{\infty}\abs{h[k]} \leq M.
    \end{align*}
    Let $H(\sigma_0) = \sum_{k=0}^{\infty}h[k]e^{-\sigma_0k)}$, which we just showed converges, and then $y[n] = e^{\sigma_0n}H(\sigma_0) = H(\sigma_0)x[n]$.
\end{proof}

\begin{prop}
    A complex phasor is an eigenfunction of any stable discrete time LTI system.
\end{prop}

\begin{proof}
    Let $S$ be a stable DT LTI system with impulse response $h[n]$, and $x[n] = e^{i\omega_0n}$. Then
    \begin{align*}
        y[n] = \sum_{k=-\infty}^{\infty}h[k]e^{i\omega_0(n-k)} = e^{i\omega_0n}\sum_{k=-\infty}^{\infty}h[k]e^{-i\omega_0k}.
    \end{align*}
    Let \[H(\omega_0) = \sum_{k=-\infty}^{\infty}h[k]e^{-i\omega_0k}.\] Since $\abs{e^{-i\omega_0k}} = 1$, and $h[n]$ is absolutely summable, this sum must converge. Therefore, $y[n] = H(\omega_0)e^{i\omega_0n}$.
\end{proof}

\begin{cor}
    Sine and cosine are eigenfunctions of any stable DT LTI system.
\end{cor}

\begin{proof}
    Since sine and cosine are the imaginary and real parts of $e^{in}$ respectively, when $x[n] = e^{in} = \cos(n) + i\sin(n)$, by linearity we have $H(1)e^{in} = S[\cos(n)] + iS[\sin[n]]$, and so necessarily $S[\cos(n)] = H(1)\cos(n)$ and $S[\sin(n)] = H(1)\sin(n)$.
\end{proof}

\subsection{DT LTI Linear Difference Equations}

\begin{defn}
    A discrete time system of the form $y[n] + ay[n-1] = bx[n]$ is a first order constant coefficient \emph{linear difference equation}.
\end{defn}

\begin{defn}
    $y[n] = x[n] - \frac{1}{2}y[n-1]$.
\end{defn}

\begin{prop}
    If $y[n] = b[x] - ay[n-1]$ is a linear difference equation, and $x[n] = 0$ for all $n < n_0$, then
    \[y[n] = \sum_{k=-\infty}^n(-a)^{n-k}bx[k].\]
\end{prop}

\begin{proof}
    Given \[y[n] = \sum_{k=-\infty}^n(-a)^{n-k}bx[k],\]
    then \[ay[n] = a\sum_{k=-\infty}^n(-a)^{n-k}bx[k] = \sum_{k=-\infty}^n(-a)^{n-1-k}bx[k] = -\sum_{k=-\infty}^{n-1}(-a)^{n-k}bx[k],\] and so
    \[y + ay[n] = \sum_{k=-\infty}^n(-a)^{n-k}bx[k] - \sum_{k=-\infty}^{n-1}(-a)^{n-k}bx[k] = (-a)^0bx[n] = bx[n].\]
\end{proof}

\begin{prop}
    The impulse response $h[n]$ of a first order linear difference equation is
    \[h[n] = (-a)^{n}bu[n].\]
\end{prop}

\begin{proof}
    \begin{align*}
        y[n] &= \sum_{k=-\infty}^{n}(-a)^{n-k}bx[k] = \sum_{k=-\infty}^{\infty}(-a)^{n-k}bu[n-k]x[k] \\
        &= \sum_{k=-\infty}^{\infty}h[n-k]x[k] = h[n] * x[n].
    \end{align*}
\end{proof}

\begin{rmk}
    Since $h[n]$ is clearly right-sided, a first order linear difference equation is causal, and when $a = 0$ it is also memoryless. Since $h[n]$ is absolutely summable when $\abs{a} < 1$, it is also stable whenever $\abs{a} < 1$.
\end{rmk}

\begin{defn}
    The general form of an $n$-th order linear difference equation is
    \[\sum_{k=0}^{N}a_ky[n-k] = \sum_{l=0}^{M}b_lx[b-l],\] or equivalently
    \[y[n] = \frac{1}{a_0}\left(\sum_{l=0}^{M}b_lx[n-l]-\sum_{k=1}^{N}a_ky[n-k]\right),\]
\end{defn}

\begin{exmp}
    $y[n] = x[n] + 2x[n-1] - 3y[n-1]$.
\end{exmp}

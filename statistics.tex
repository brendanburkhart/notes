\setchaptergraphic{}

\chapter{Statistics}
\label{ch:statistics}

\section{Simple Random Sampling}

Consider a finite population of $N$ distinct objects, each with two associated measurements $x$ and $y$. We denote the $k$th object by $z_k = ((x_k, y_k), k)$. Let $\underline{\pi}(z_k) = (x_k, y_k)$, let $\pi_x(z_k) = x_k$, let $\pi_y(z_k) = y_k$, and finally let $\pi_{\mathrm{ind}}(z_k) = k$.

\subsection{Population Parameters}

Population parameters are \emph{deterministic} functions of the measurements associated with objects in a population.

For example, the population mean of the $y$-measurements is
\begin{align*}
    \mu_y = \frac{1}{N}\sum_{k=1}^{N}\pi_{y}(z_k),
\end{align*}
and the population variance of the $y$-measurements is
\begin{align*}
    \sigma_{y}^2 = \frac{1}{N}\sum_{k=1}^{N}\left(y_k - \mu_y\right)^2.
\end{align*}

The population covariance between the $x$ and $y$ measurements is
\begin{align*}
    \sigma_{xy} = \frac{1}{N}\sum_{k=1}^{N}\left(x_k-\mu_x\right)\left(y_k-\mu_y\right).
\end{align*}

\subsection{Sample Parameters}

Suppose we have a finite sample $Z_1, \ldots, Z_n$ of the entire population. Functions of such samples are known as \emph{statistics}, and will be used to construct useful \emph{estimators} of our population parameters. For example, the \emph{sample mean} can be used to estimate the population mean, and is defined by
\begin{align*}
    \samplemeanof{X} = \frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k),\;\; \samplemeanof{Y} = \frac{1}{n}\sum_{k=1}^{n}\pi_{y}(Z_k).
\end{align*}

\begin{defn}
    An \emph{estimator} is a deterministic function of the sample data.
\end{defn}

What do we mean by \emph{useful estimators}? Suppose $\theta$ is a population parameter, and $\hat{\theta}$ is an estimator for $\theta$. We say that
\begin{itemize}
    \item $\hat{\theta}$ is \emph{unbiased} for $\theta$ if $E(\hat{\theta}) = \theta$,
    \item $\hat{\theta}$ is \emph{consistent} for $\theta$ if for all $\varepsilon > 0$, the limit as $n$ approaches infinity of $P\left(\abs{\hat{\theta} - \theta} > \varepsilon\right)$ is zero.
\end{itemize}
We can then define useful estimators to simply be those that are unbiased and consistent.

\begin{prop}
    Let $Z_1, \ldots, Z_n$ be a finite sample drawn uniformly at random \emph{with replacement} from the entire population. Then for any $k_1, \ldots, k_n \in {1, \ldots, N}$,
    \begin{align*}
        P\left(Z_1=z_{k_1}, \ldots, Z_n=z_{k_n}\right) = \prod_{i=1}^{n}P(Z_{i}=z_{k_i}) = \left(\frac{1}{N}\right)^{n}.
    \end{align*}
    The draws are independent and identically distributed.
\end{prop}

\begin{prop}
    Now consider instead $Z_1, \ldots, Z_n$ drawn uniformly at random \emph{without replacement} from the entire population. Then for any distinct $k_1, \ldots, k_n \in {1, \ldots, N}$, where $n \leq N$,
    \begin{align*}
        P\left(Z_1=z_{k_1}, \ldots, Z_n=z_{k_n}\right) = \frac{(N-n)!}{N!}.
    \end{align*}
    The draws are exchangeable but dependent.
\end{prop}

\begin{rmk}
    We can view draws without replacement from a population with $N$ objects as being deterministic viewing of a specific permutation of the population.
\end{rmk}

\begin{exmp}
    Consider some $f$, and let
    \begin{align*}
        \beta = \frac{1}{N}\sum_{k=1}^{N}f(z_k),
    \end{align*}
    and a corresponding estimator
    \begin{align*}
        \frac{1}{n}\sum_{i=1}^{n}f(Z_i).
    \end{align*}
    Note that this estimator is unbiased by $\mathcal{LOTUS}$ \ref{lotus}.
\end{exmp}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ with replacement, then $\varianceof{\samplemeanof{X}} = \frac{\sigma_x^2}{n}$.
\end{prop}

\begin{proof}
    \begin{align*}
        \varianceof{\samplemeanof{X}} &= \varianceof{\frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k)} = \sum_{i=1}^{n}\sum_{j=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\left(0\right) \\
        &= \frac{1}{n^2}\sum_{i=1}^{n}\varianceof{X_i} = \frac{1}{n}\varianceof{X_i} = \frac{\sigma_x^2}{n}.
    \end{align*}
\end{proof}

\begin{rmk}
    We have seen that $\samplemeanof{X}$ is unbiased, and it must be consistent by the Weak Law of Large Numbers \ref{wlln}.
\end{rmk}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ without replacement, then $\varianceof{\samplemeanof{X}} = \frac{\sigma_x^2}{n} + \frac{\covarianceof{X_1}{X_2}}{n}\left(n-1\right)$.
\end{prop}

\begin{proof}
    \begin{align*}
        \varianceof{\samplemeanof{X}} &= \varianceof{\frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k)} = \sum_{i=1}^{n}\sum_{j=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + \frac{\covarianceof{X_1}{X_2}}{n^2}\left(n^2 - n\right) \\
        &= \frac{\sigma_x^2}{n} + \frac{\covarianceof{X_1}{X_2}\left(n-1\right)}{n}.
    \end{align*}
\end{proof}

\begin{cor}
    \begin{align*}
        \covarianceof{X_1}{X_2} = -\frac{\sigma_x^2}{N-1}
    \end{align*}
\end{cor}

\begin{proof}
    We know that when drawing without replacement, if $n = N$ then $\varianceof{\samplemeanof{X}} = 0$. Therefore,
    \begin{align*}
        0 = \frac{\sigma_x^2}{N} + \frac{\covarianceof{X_1}{X_2}\left(N-1\right)}{N}.
    \end{align*}
\end{proof}

\begin{cor}
    \begin{align*}
        \varianceof{\samplemeanof{X}} = \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right).
    \end{align*}
\end{cor}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ without replacement, then $\hat{\sigma}_x^2$ is a biased estimator.
\end{prop}

\begin{proof}
    \begin{align*}
        \hat{\sigma}_x^2 = \frac{1}{n}\sum_{i=1}^{n}\left(X_i - \samplemeanof{X}\right)^{2} = \frac{1}{n}\left[\sum_{i=1}^{n}X_i^2 - 2\sum_{i=1}^{n}X_i\samplemeanof{X} + \sum_{i=1}^{n}\samplemeanof{X}^{2}\right] = \frac{1}{n}\sum_{i=1}^{n}X_i^2 - \samplemeanof{X}^2.
    \end{align*}
    \begin{align*}
        \expectationof{\hat{\sigma}_x^2} &= \expectationof{\frac{1}{n}\sum_{i=1}^{n}X_i^2} - \expectationof{\samplemeanof{X}^2} = \frac{n\left(\sigma_x^2 + \mu_x^2\right))}{n} - \left(\varianceof{\samplemeanof{X}} + \mu_x^2\right) \\
        &= \left(\sigma_x^2 + \mu_x^2\right) - \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right) - \mu_x^2 = \sigma_x^2 - \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right) \\
        &= \sigma_x^2\left[\frac{N\left(n-1\right)}{n\left(N-1\right)}\right].
    \end{align*}
    Clearly $\expectationof{\hat{\sigma}_x^2} \neq \sigma_x^2$ when $n \neq N$.
\end{proof}

\section{Dichotomous Populations}

\begin{defn}
    A \emph{dichotomous} random variable is one which has only two possible values, often $0$ and $1$.
\end{defn}

\begin{exmp}
    Consider a population
    \begin{align*}
        \left\{z_1, \ldots, z_k\right\},
    \end{align*}
    where $z_i = \left((x_i, y_i), i\right)$, and $x_i$ is a dichotomous random variables. Note that the second moment is equal to the first moment, because $0^2 = 0$ and $1^2 = 1$. Therefore, $\sigma_x^2 = \mu_x - \mu_x^2 = \mu_x\left(1 - \mu_x\right)$. Then using $\hat{p_x} = \samplemeanof{X}$,
    \begin{align*}
        \varianceof{\hat{p}_x} = \frac{\sigma_x^2}{2} = \frac{p_x\left(1-p_x\right)}{n}.
    \end{align*}
    However, we need to know $p_x$ to calculate this variance, and if we already knew $p_x$ we likely wouldn't be looking at the variance of an estimator of $p_x$. Can we put a bound on $\varianceof{\hat{p}_x}$ without knowing $p_x$? Since $0 \leq p_{x} \leq 1$, we know that $p_{x}\left(1 - p_{x}\right) \leq \frac{1}{4}$, and so
    \begin{align*}
        \varianceof{\hat{p}_x} \leq \frac{1}{4n}.
    \end{align*}

    Now suppose $y_i$ is also dichotomous, and so
    \begin{align*}
        \sigma_{xy} = \frac{1}{N}\sum_{k=1}^{N}\left(x_k - \mu_x\right)\left(y_k - \mu_y\right) = \frac{1}{N}\sum_{k=1}^{N}x_ky_k - \mu_x\mu_y.
    \end{align*}
    Note that if $\covarianceof{X_1}{Y_1} = 0$, since $\covarianceof{X_1}{Y_1} = \sigma_{xy}$, we have
    \begin{align*}
        \frac{1}{N}\sum_{k=1}^{N}x_ky_k = \mu_x\mu_y,
    \end{align*}
    which is equivalent to $P(X_1 = 1, Y_1 = 1) = P(X_1 = 1)P(Y_1 = 1)$, and so zero covaraince between $X_1$ and $Y_1$ implies they must be independent when they are Bernoulli random variables.
\end{exmp}

\section{Approximation Methods}

Suppose $X: \Omega \to \R$ is a random variable with mean $\mu$ and variance $\sigma^2$, and $g: \R \to \R$ is a deterministic, $C^2$ function. What can we say about $g(X)$?

Let $f$ be the density of $X$, then by $\mathcal{LOTUS}$ \ref{lotus} we have
\begin{align*}
    \expectationof{g(X)} = \int_{\R}g(x)f(x)dx.
\end{align*}
Suppose we know that $X$ is very likely ``close to'' $\mu$, that is, that $\sigma^2$ is small. Consider the Taylor expansion of $g$ about $\mu$. We know that for some $\xi$ between $x$ and $\mu$, we have
\begin{align*}
    g(x) = g(\mu) + g'(\mu)(x-\mu) + \frac{g''(\xi)}{2}(x-\mu)^2
\end{align*}
If $x$ is near $\mu$, then we can use $\xi \approx \mu$. If we can guarantee certain conditions to be explored later, we can guarantee that
\begin{align*}
    \expectationof{g(X)} &\approx \expectationof{g(\mu) + g'(\mu)(x-\mu) + \frac{g''(\mu)}{2}(x-\mu)^2} \\
    &= g(\mu) + g'(\mu)\expectationof{X - \mu} + \frac{g''(\mu)}{2}\expectationof{\left(X - u\right)^2} = g(\mu) + \frac{g''(\mu)}{2}\sigma^2.
\end{align*}

Now let's expand our focus to random variables $X$ and $Y$, and a deterministic function $h: \R \times \R \to \R$. We know that
\begin{align*}
    h(x, y) &\approx h(\mu_x, \mu_y) + \frac{\partial h}{\partial x}\left(\mu_x, \mu_y\right)(x - \mu_x) + \frac{\partial h}{\partial y}\left(\mu_x, \mu_y\right)(y - \mu_y) \\
    &+ \frac{\partial^2h}{2\partial x^2}(\mu_x, \mu_y)\left(x - \mu_x\right)^2 + \frac{\partial^2h}{2\partial y^2}(\mu_x, \mu_y)\left(y - \mu_y\right)^2 + \frac{\partial^2h}{\partial x\partial y}(\mu_x, \mu_y)\left(x - \mu_x\right)\left(y - \mu_y\right).
\end{align*}
Note that
\begin{align*}
    \expectationof{h(x, y)} \approx \expectationof{h(\mu_x, \mu_y)}
\end{align*}

\begin{thm}
    Let $\theta$ be a random variable, and $\hat{\theta}$ be an estimator for $\theta$. Then the \emph{mean squared error} of $\hat{\theta}$ is
    \begin{align*}
        \expectationof{\left(\hat{\theta} - \theta\right)^2} = \varianceof{\hat{\theta}} + \biasof{\hat{\theta}}^2.
    \end{align*}
\end{thm}

\begin{proof}
    \begin{align*}
        \expectationof{\left(\hat{\theta} - \theta\right)^2} &= \expectationof{\left(\hat{\theta} - \expectationof{\hat{\theta}} + \expectationof{\hat{\theta}} - \theta\right)^2} \\
        &= \expectationof{\left(\hat{\theta} - \expectationof{\hat{\theta}}\right)^2 + 2\left(\hat{\theta} - \expectationof{\hat{\theta}}\right)\left(\expectationof{\hat{\theta}} - \theta\right) + \left(\expectationof{\hat{\theta}} - \theta\right)^2} \\
        &= \expectationof{\left(\hat{\theta} - \expectationof{\hat{\theta}}\right)^2} + 2\expectationof{\hat{\theta} - \expectationof{\hat{\theta}}}\left(\expectationof{\hat{\theta}} - \theta\right) + \expectationof{\left(\expectationof{\hat{\theta}} - \theta\right)^2}.
    \end{align*}
    Since $\expectationof{\hat{\theta} - \expectationof{\hat{\theta}}} = \expectationof{\hat{\theta}} - \expectationof{\hat{\theta}} = 0$, we have
    \begin{align*}
        \expectationof{\left(\hat{\theta} - \theta\right)^2} = \expectationof{\left(\hat{\theta} - \expectationof{\hat{\theta}}\right)^2} + \left(\expectationof{\hat{\theta}} - \theta\right)^2.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a bivariate population $z_i = ((x_i, y_i), i)$ for $1 \leq i \leq N$. We take a sample $Z_1, \ldots, Z_n$ (without replacement) from this population, and compute the sample means $\samplemeanof{X}$ and $\samplemeanof{Y}$. Suppose we want to estimate $\mu_y$, and happen to know $\mu_x$, where $\mu_x \neq 0$. Let
    \begin{align*}
        r = \frac{\mu_y}{\mu_y},
    \end{align*}
    which we can estimate as
    \begin{align*}
        R = \frac{\samplemeanof{Y}}{\samplemeanof{X}}.
    \end{align*}
    Then we can choose
    \begin{align*}
        \samplemeanof{Y}_R = \mu_xR = \mu_x\frac{\samplemeanof{Y}}{\samplemeanof{X}}.
    \end{align*}
    Note that $\samplemeanof{Y}_R$ is not $\samplemeanof{Y}$. Let us use our approximation method to obtain approximations of $\expectationof{R}$ and $\varianceof{R}$. We use $R = g(\samplemeanof{X}, \samplemeanof{Y})$, where $g(x, y) = y/x$. Then we have
    \begin{align*}
        g(x, y) &\approx g(\mu_x, \mu_y) + \frac{\partial g}{\partial x}(x - \mu_x) + \frac{\partial g}{\partial y}(y - \mu_y) \\
        &+ \frac{\partial^2g}{2\partial x^2}(x - \mu_x)^2 + \frac{\partial^2g}{2\partial y^2}(y - \mu_y)^2 + \frac{\partial^2g}{\partial xy}(x - \mu_x)(y - \mu_y) \\
        &= g(\mu_x, \mu_y) + \frac{1}{2}\frac{2\mu_y}{\mu_x^3}(x-\mu_x)^2 + \frac{1}{2}(0)(y-\mu_y)^2 - \frac{1}{\mu_x^2}(x-\mu_x)(y-\mu_y).
    \end{align*}
    Therefore,
    \begin{align*}
        \expectationof{R} &\approx g(\mu_x, \mu_y) + \frac{1}{2}\frac{2\mu_y}{\mu_x^2}\varianceof{\samplemeanof{X}} - \frac{1}{\mu_x^2}\covarianceof{\samplemeanof{X}}{\samplemeanof{Y}} \\
        &= \frac{\mu_y}{\mu_x} + \frac{\mu_y}{\mu_x^3}\frac{\sigma_x^2}{n}\left[\frac{N-n}{N-1}\right] - \frac{1}{\mu_x^2}\left[\frac{\sigma_{xy}}{n}\left(\frac{N-n}{N-1}\right)\right].
    \end{align*}

    For the variance, we instead use a first-order Taylor approximation. Note that, in general for
    \begin{align*}
        g(x,y) = c_1 + c_2(x - \mu_x) + c_2(y - \mu_y),
    \end{align*}
    the variance of $g$ is given by
    \begin{align*}
        \varianceof{c_1 + c_2(x - \mu_x) + c_2(y - \mu_y)} = c_2^2\varianceof{x} + c_3^2\varianceof{y}  + 2c_2c_3\covarianceof{x}{y}.
    \end{align*}
    Therefore,
    \begin{align*}
        \varianceof{R} &\approx \left(\frac{-\mu_y}{\mu_x^2}\right)^2\varianceof{\overline{X}} + \left(\frac{1}{\mu_x}\right)^2\varianceof{\overline{Y}} - 2\left(\frac{-\mu_y}{\mu_x^3}\right)\covarianceof{\overline{X}}{\overline{Y}} \\
        &= \left(\frac{-\mu_y}{\mu_x^2}\right)^2\frac{\sigma_{x}^2}{n}\left[\frac{N-n}{N-1}\right] + \left(\frac{1}{\mu_x}\right)^2\frac{\sigma_{y}^2}{n}\left[\frac{N-n}{N-1}\right] - 2\frac{\mu_y}{\mu_x^3}\frac{\sigma_{xy}}{n}\left[\frac{N-n}{N-1}\right].
    \end{align*}
\end{exmp}

\section{Convergence of Random Variables}

\begin{defn}
    Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $\{X_n : n \in \N\}$ be a sequence of random variables defined on $\Omega$, and let $X: \Omega \to \R$ be a random variable.

    We say that $X_n$ converges \emph{pointwise everywhere} to $X$ if for all $\omega \in \Omega$, and for all $\varepsilon > 0$, there exists $N \in \N$ such that $d\left(X_n, X(\omega)\right) < \varepsilon$.

    We say that $X_n$ converges \emph{uniformly} to $X$ if for all $\varepsilon > 0$, there exists $N \in \N$ such that for all 
    $\omega \in \Omega$ we have $d\left(X_n, X(\omega)\right) < \varepsilon$.

    We say that $X_n$ converges \emph{pointwise almost surely} or \emph{pointwise almost everywhere} to $X$ if there exists $
    A \in \mathcal{F}$ where $P(A) = 0$ such that $X_n$ converges pointwise to $X$ on $A^{c}$.
\end{defn}

\begin{defn}
    We say that $X_n$ converges to $X$ \emph{in probability}, denoted by
    \begin{align*}
        X_n \to X
    \end{align*}
    if for all $\delta > 0$, the collection of sets
    \begin{align*}
        A_{n,\delta} = \left\{\omega \in \Omega \compbar d\left(X_n(\omega), X(\omega)\right) > \delta\right\}
    \end{align*}
    satisfies $P(A_{n,\delta}) \to 0$ as $n \to \infty$.
\end{defn}

\begin{defn}
    We say that $X_n$ converges to $X$ using $L^{p}$ convergence when
    \begin{align*}
        \expectationof{\abs{X_n(\omega) - X(\omega)}^{p}} \to 0
    \end{align*}
    as $n \to \infty$.
\end{defn}

\begin{defn}
    We say that $t$ is a \emph{continuity point} of $F$ if $F$ is continuous at $t$.
\end{defn}

\begin{defn}
    Let $X_n$ be a sequence of random variables, not necessarily defined on the same probability space. Let $X$ be a random variable with cumulative distribution function $F$. We say that $X_n$ converges to $X$ \emph{in distribution} if for every continuity point $t$ of $F$, the limit of $F_n(t)$ as $n \to \infty$ is $F(t)$.
\end{defn}

\begin{exmp}
    Let $\Omega = [0, 1]$, $\mathcal{F} = \mathcal{B}(\Omega)$, and $P$ be the uniform probability measure. Let $X_n(\omega)$ be an indicator of whether $\omega$ is contained in $[0, 1/n]$ or not. This sequence has pointwise convergence almost surely to $0$, but it is not pointwise everywhere nor uniform.
\end{exmp}

\begin{exmp}
    Let $\Omega = [0, 1]$, $\mathcal{F} = \mathcal{B}(\Omega)$, and $P$ be the uniform probability measure. Let $X_n(\omega)$ be equal to $n^2$ when $0 \leq \omega \leq 1/n$ and zero otherwise. This sequence has pointwise convergence almost surely to $0$, but it is not pointwise everywhere nor uniform.
\end{exmp}

\begin{thm}{Slutsky's Theorem}\label{slutsky}\proofbreak
    Let $X_n$ and $Y_n$ be sequences of random variables such that $X_n$ converges in distribution to a random variable $X$ and $Y_n$ converges in probability to a real constant $c$. Suppose $X_n$ and $Y_n$ are defined on the same probability space, so that $X_n + Y_n$ and $X_n \cdot Y_n$ are well-defined. Then we have
    \begin{itemize}
        \item $X_n + Y_n$ converges in distribution to $X + c$,
        \item $X_nY_n$ converges in distribution to $X \cdot c$,
        \item if $c \neq 0$ then $X_n / Y_n$ converges in distribution to $X/c$.
    \end{itemize}
\end{thm}

\begin{lemma}
    Let $X_n$ be a sequence of random variables defined on a probability space $\Omega$. If $X_n$ converges in distribution to a real constant $c$, then $X_n$ converges in probability to $c$.
\end{lemma}

\section{Parametric Estimation}

\subsection{Method of Moments}

\begin{defn}
    Consider a collection of independent and identically distributed random variables $X_1, \ldots, X_n$ with a common distribution function $F_{\theta}$, where $\theta \in \R^k$ is a non-random fixed but unknown parameter. We say that the set of all $X$ with distribution $F_{\theta}$ for some $\theta \in \R^k$ forms a \emph{parametric family}.
\end{defn}

\begin{rmk}
    If $\theta \in \R^{k}$ is known, then we know $F_{\theta}$.
\end{rmk}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed Poisson($\lambda$) random variables, where $\lambda > 0$. We have parameter space $\Theta = \{\lambda \in \R: \lambda > 0\}$, and
    \begin{align*}
        P(X_i = \ell) = \frac{e^{-\lambda}\lambda^{\ell}}{\ell!},\;\;\ell \geq 0.
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed uniform random variables on $[0, \theta]$, where $\theta > 0$. Since the support of the distribution function is $[0, \frac{1}{\theta}]$, which is probabilistically equivalent to $(0, \frac{1}{\theta})$. Note that the support then depends on $\theta$.
\end{exmp}

\begin{defn}
    In the \emph{Method of Moments}, we find expressions for the moments of a distribution in terms of the parameters of the distribution, and then invert these functions to estimate the parameters in terms of the \emph{sample moments}.
\end{defn}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed exponential($\lambda$) random variables. Notice that
    \begin{align*}
        \expectationof{X_i} = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx = \frac{1}{\lambda},
    \end{align*}
    and so $\lambda = 1/\mu$. We can therefore estimate
    \begin{align*}
        \hat{\lambda}_{\textrm{MoM}} = \frac{1}{\samplemeanof{X}}.
    \end{align*}

    Since $g(x) = 1/x$ is smooth when $x \neq 0$, and the Weak Law of Large Numbers \ref{wlln} guarantees $\samplemeanof{X}$ converges to $\mu$ in probability, it follows that $1/\samplemeanof{X}$ converges to $1/\mu = \lambda$ in probability, and so this estimator is \emph{consistent}.
\end{exmp}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed normal variables with distribution $N(\mu, \sigma^2)$, so $\theta = (\mu, \sigma^2)$. We find that
    \begin{align*}
        \mu &= \mu_1 \\
        \sigma^2 &= \mu_2 - \mu_1^2.
    \end{align*}
    In follows that we can estimate
    \begin{align*}
        \hat{\mu}_{\textrm{MoM}} &= \samplemeanof{X} \\
        \hat{\sigma^2}_{\textrm{MoM}} &= \varianceof{x} - \samplemeanof{X}^2.
    \end{align*}
\end{exmp}

\subsection{Maximum Likelihood Estimators}

Consider the joint density $f(x_1,\ldots,x_n|\theta)$ of $X_1, \ldots, X_n$. If we can maximize this value over $\theta$, we have an estimate of $\theta$ which has the ``maximum likelihood'' of us having observed the sample data we have.

\begin{defn}
    Let $X_1, \ldots, X_n, \ldots$ random variables drawn from a common distribution with parameter vector $\theta \in \R^k$. Let
    \begin{align*}
        \hat{\theta}(x_1, \ldots, x_n) = \argmax_{\theta}f(x_1,\ldots,x_n|\theta).
    \end{align*}
    We define the \emph{maximum likelihood estimator} of $\theta$ as
    \begin{align*}
        \theta(X_1, \ldots, X_n).
    \end{align*}
\end{defn}

\begin{prop}
    When $X_1, \ldots, X_n$ are independent and identically distributed,
    \begin{align*}
        \hat{\theta}(x_1, \ldots, x_n) = \argmax_{\theta}\sum_{i=1}^{n}\ln f(x_i|\theta).
    \end{align*}
\end{prop}

\begin{proof}
    Since $X_1, \ldots, X_n$ are independent and identically distributed, it follows that
    \begin{align*}
        f(x_1, \ldots, x_n | \theta) = \prod_{i=1}^{n}f(x_i,\theta).
    \end{align*}
    Since $\ln(x)$ is monotonic, the argument at the maximum of $f(x_1, \ldots, x_n|\theta)$ is the argument at the maximum of $\ln f(x_1, \ldots, x_n|\theta)$. Therefore,
    \begin{align*}
        \hat{\theta}(x_1, \ldots, x_n) = \argmax_{\theta}\ln\left[\prod_{i=1}^{n}f(x_i,\theta)\right] = \argmax_{\theta}\sum_{i=1}^{n}\ln f(x_i|\theta).
    \end{align*}
\end{proof}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed exponential($\lambda$) random variables. Then
    \begin{align*}
        f(x_1, \ldots, x_n|\lambda) = \lambda^n\exp\left(-\lambda\sum_{i=1}^{n}x_i\right),
    \end{align*}
    so
    \begin{align*}
        \ln f(x_1, \ldots, x_n|\lambda) = n\ln(\lambda) - \lambda\sum_{i=1}^{n}x_i.
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed normal variables with distribution $N(\mu, \sigma^2)$, so $\theta = (\mu, \sigma^2)$. We find that
    \begin{align*}
        f\left(x_1, \ldots, x_n|\theta\right) = \left[\frac{1}{\sigma\sqrt{2}\pi}\right]^{n}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(x_i-\mu\right)^2\right],
    \end{align*}
    and so
    \begin{align*}
        \ln f\left(x_1, \ldots, x_n|\theta\right) = -n\ln(\sigma\sqrt{2}\pi) -\frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(x_i-\mu\right)^2.
    \end{align*}
    Then we have
    \begin{align*}
        \begin{pmatrix}
            \frac{\partial \ln f\left(x_1, \ldots, x_n|\theta\right)}{\partial \mu} \\
            \frac{\partial \ln f\left(x_1, \ldots, x_n|\theta\right)}{\partial \sigma}
        \end{pmatrix} = \begin{pmatrix}
            \frac{1}{\sigma^2}\sum_{i=1}^{n}\left(x_i-\mu\right) \\
            \frac{-n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^{n}\left(x_i-\mu\right)^2.
        \end{pmatrix}
    \end{align*}
    Since our estimator is smooth, its maximum must occur when this derivative is zero. Therefore, we find
    \begin{align*}
        \sum_{i=1}^{n}(x_i - \mu) &= 0 \implies \hat{\mu}_{\textrm{MLE}} = \samplemeanof{X} \\
        \frac{-n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^{n}\left(x_i-\mu\right)^2 &= 0 \implies \hat{\sigma^2}_{\textrm{MLE}} = \frac{1}{n}\sum_{i=1}^{n}\left(X_i-\samplemeanof{X}\right)^2.
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider $X_1, \ldots, X_n, \ldots$ independent and identically distributed uniform variables on $[0, \theta]$. Then
    \begin{align*}
        f(x_1, \ldots, x_n|\theta) = \left(\frac{1}{\theta}\right)^{n}1_{[0, \theta]}.
    \end{align*}
    We can see that $f(x_1, \ldots, x_n)$ is maximized when $\theta$ is as small as possible, but at least as large as any $X_i$ so that all indicators are one. It follows that
    \begin{align*}
        \hat{\theta}_{\textrm{MLE}} = \max_{i}(X_i).
    \end{align*}
    Contrast this with the Method of Moments estimator for $\theta$, which is
    \begin{align*}
        \hat{\theta}_{\textrm{MoM}} = 2\samplemeanof{X}.
    \end{align*}
\end{exmp}

\subsection{Fisher Information}

\begin{prop}
    Let $X \distributed f(x|\theta)$ be a random variable from a distribution with parameter $\theta \in \R^k$, and let
    \begin{align*}
        Y = \frac{\partial}{\partial \theta}\ln\left(f(X|\theta)\right).
    \end{align*}
    Under sufficient regularity conditions, $\expectationof{Y} = 0$.
\end{prop}

\begin{proof}
    Since $f(x|\theta)$ is a probability density, we know that
    \begin{align*}
        \int_{\Omega}f(x|\theta)d\theta = 1.
    \end{align*}
    It follows that
    \begin{align*}
        \frac{\partial}{\partial \theta}\left[\int_{\Omega}f(x|\theta)d\theta\right] = 0.
    \end{align*}
\end{proof}

\begin{thm}
    Cram\'er-Rao bound
\end{thm}

\begin{thm}
    Let $X_1, \ldots, X_n$ be independent and identically distributed random variables with common density $f(X|\theta)$, and let $\hat{\theta}$ be the maximum likelihood estimator for $\theta$. Then
    \begin{align*}
        \sqrt{nI(\theta)}\left[\hat{\theta} - \theta\right]
    \end{align*}
    converges in distribution to the standard normal distribution $N(0, 1)$.
\end{thm}

\begin{defn}
    Suppose we want to compare two estimators, $\hat{\theta}_1$ and $\hat{\theta}_2$. One possible metric is the mean squared error of each estimator. We call the ratio
    \begin{align*}
        \frac{\meansqerrof{\hat{\theta}_1}}{\meansqerrof{\hat{\theta}_2}}
    \end{align*}
    the \emph{relative efficiency} of $\hat{\theta_1}$ and $\hat{\theta}_2$. In the case that both estimators are unbiased, then since $\textrm{MSE}(\hat{\theta}) = \varianceof{\hat{\theta}} + \textrm{bias}(\hat{\theta})^2$ it follows that their relative efficiency is simply
    \begin{align*}
        \frac{\varianceof{\hat{\theta}_1}}{\varianceof{\hat{\theta}_2}}.
    \end{align*}
\end{defn}

\begin{defn}
    Let $f(X|\theta)$ be a parametric distribution. Then a statistic $T(X_1, \ldots, X_n)$ is \emph{sufficient} for $\theta$ when $X_1, \ldots, X_n$ given $T$ is independent of $\theta$ --- that is, $f(x_1, \ldots, x_n|T)$ does not depend on $T$.
\end{defn}

\begin{exmp}
    Consider independent and identically distributed random variables $X_1, X_2, X_3 \distributed \textrm{Bernoulli}(p)$, and the statistic $T(X_1, X_2, X_3) = X_1 + X_2 + X_3$. Notice that $P(X_1=x_1, X_2=x_2, X_3=x_3|T=t)$ is zero if $T \neq x_1 + x_2 + x_3$, and otherwise
    \begin{align*}
        P(X_1=x_1, X_2=x_2, X_3=x_3|T=t) &= \frac{P(X_1=x_1, X_2=x_2, X_3=x_3,T=t)}{P(T=t)} \\
        &= \frac{p^{t}(1-p)^{3-t}}{\binom{3}{t}p^{t}(1-p)^{3-t}} = \binom{3}{t}^{-1}.
    \end{align*}
    Since this does not depend on $p$, it follows that $T$ is sufficient for $p$.
\end{exmp}

\begin{thm}{Fisher-Neyman Factorization Criterion}\label{sufficiency-factorization-fisher}\proofbreak
    Let $X_1, \ldots, X_n$ be independent and identically distributed random variables with common density $f(X|\theta)$ and let $T$ be an estimator. Then $T$ is sufficient for $\theta$ if and only if there exists \emph{non-negative} $h(x_1, \ldots, x_n)$ and $g(T, \theta)$ such that
    \begin{align*}
        f(x_1, \ldots, x_n|\theta) = g(T, \theta)h(x_1, \ldots, x_n).
    \end{align*}
\end{thm}

\begin{rmk}
    Consider two possibly distinct samples $X_1, \ldots, X_n$ and $Y_1, \ldots, Y_n$ such that $T(X_1, \ldots, X_n) = T(Y_1, \ldots, Y_n)$. When considering the maximum likelihood estimator for $\theta$, $h(x_1, \ldots, x_n)$ can be treated as a constant, and so maximizing the likelihood is equivalent to maximizing $g(T, \theta)$. Since the statistic $T$ is the same for both samples, the maximum likelihood estimator will be as well.
\end{rmk}

\begin{exmp}
    Suppose we want to show that $T = \sum X_i$ is a sufficient statistics for $\lambda$ if $X_i \distributed \textrm{exp}(\lambda)$ are independent and identically distributed. We know that
    \begin{align*}
        f(x_1, \ldots, x_n|\lambda) = \lambda^n\exp\left(-\lambda\sum_{i=1}^{n}x_i\right) = \lambda^n\exp\left(-\lambda T\right).
    \end{align*}
    Let
    \begin{align*}
        g(T,\lambda) &= \lambda^n\exp\left(-\lambda T\right), \\
        h(x_1, \ldots, x_n) &= 1.
    \end{align*}
    Since these functions are non-negative, by Theorem \ref{sufficiency-factorization-fisher} we know that $T$ is sufficient for $\lambda$.
\end{exmp}

\begin{defn}
    Let $W$ be the set of all sufficient statistics for a parameter $\theta$. Then $T \in W$ is a \emph{minimally sufficient statistic} when for any $S \in W$ there exists a function $\ell$ such that $T = \ell(S)$.
\end{defn}

\section{Bayesian Inference}

\begin{rmk}
    In the frequentist paradigm we consider a parametric model with parameter $\theta$, and attempt to estimate the true and fixed value of $\theta$ from our data.

    In the Bayesian paradigm, we instead view $\theta$ itself as a random variable, rather than as a fixed but unknown value. We view $\theta$ as having a \emph{prior} distribution (typically known), which may have been constructed to include information about the experimental process. If the prior distribution has (typically known) parameters, these are called \emph{hyperparameters}. We start from the prior distribution $f_{\theta}(\theta)$ and a likelihood function $f(x|\theta)$, and from this we compute a \emph{posterior distribution} $f_{\theta|X}(\theta|X)$.
\end{rmk}

\begin{defn}{Bayes' Rule}\proofbreak
    \begin{align*}
        P\left(\theta = \theta_0|X\right) = \frac{P\left(X|\theta_0\right)P(\theta_0)}{P\left(X|\theta_0\right)P(\theta_0) + P\left(X|\theta_1\right)P(\theta_1)}
    \end{align*}
\end{defn}

\begin{exmp}
    Suppose we wish to understand some independent and identically distributed Bernoulli($\theta$) data, where $0 \leq \theta \leq 1$. Suppose $\theta$ has the following \emph{discrete prior}
    \begin{align*}
        P\left(\theta = \frac{3}{4}\right) &= \frac{1}{3}, \\
        P\left(\theta = \frac{1}{4}\right) &= \frac{2}{3}.
    \end{align*}
    Next, suppose we observe data $X_1 = 1$, $X_2 = 1$, $X_3 = 0$, $X_4 = 1$, $X_5 = 1$. Then by Bayes' rule we have
    \begin{align*}
        P\left(\theta = 3/4|X\right) &= \frac{P(X|\theta=3/4)P(\theta=3/4)}{P(X|\theta=1/4)P(\theta=1/4) + P(X|\theta=3/4)P(\theta=3/4)} \\
        &= \frac{\left(3/4\right)^{4}(1-3/4)^{1}(1/3)}{\left(3/4\right)^{4}(1-3/4)^{1}(1/3) + \left(1/4\right)^{4}(1-1/4)^{1}(2/3)} \\
        &= \frac{3^3/4^5}{3^3/4^5 + 2/4^5} = \frac{27}{29}.
    \end{align*}
    It follows that $P(\theta = 1/4|X) = 1-P(\theta=3/4|X) = \frac{2}{29}$.
\end{exmp}

\begin{exmp}
    Suppose $X_1, \ldots, X_n$ are independent and identically distributed normal $N(\mu, \sigma^2)$ random variables, where $\mu$ is unknown while $\sigma^2$ is known. Furthermore, suppose the prior distribution of $\mu$ is $N(\mu_{pr}, \sigma_{pr}^2)$, where $\mu_{pr},\sigma_{pr}^{2}$ are known hyperparameters. Then the posterior distribution has PDF
    \begin{align*}
        f_{\mu|X}(\mu, X) = \frac{f(X|\mu,\sigma^2)f(\mu|\mu_{pr},\sigma_{pr}^2)}{\int f(X|\mu,\sigma^2)f(\mu|\mu_{pr},\sigma_{pr}^2)d\mu}.
    \end{align*}
    Note that
    \begin{align*}
        f_{\mu|X}(\mu|X) = \alpha(X)f(X|\mu,\sigma^2)f(\mu|\mu_{pr},\sigma_{pr}^2)
    \end{align*}
\end{exmp}

\section{Hypothesis Testing}

Let $X_1, \ldots, X_n$ be independent and identically distributed $f(x|\theta)$ random variables, where $\theta \in \R^k$, where $f(x|\theta)$ is known (up to the parameter). Suppose we want to evaluate two competing conjectures about this parameter.

\begin{exmp}
    Let $X_1, \ldots, X_n \distributed \textrm{exp}(\lambda)$ be independent and identically distributed random variables. Consider the conjectures $\lambda = \lambda_{0}$, and $\lambda > \lambda_{0}$.
\end{exmp}

Conjectures about the parameter of the distribution are known as \emph{parametric hypotheses}.

We often wish to compare competing hypotheses or conjectures, which we call the \emph{null hypothesis} $H_0$ and alternative hypothesis $H_a$ or $H_1$, where $H_0$ is $\theta \in \Theta_0$ and $H_a$ is $\theta \in \Theta_a$ such that $\Theta_0 \intersection \Theta_a = \emptyset$.

We say a hypothesis is \emph{simple} if it completely specificies the distribution. If a hypothesis is not simple, we say it is \emph{composite}.

\begin{defn}
    A \emph{decision rule} is a function $d: X \to \{0, 1\}$, where $d(X_1, \ldots, X_n) = 1$ indicates we should \emph{reject} the null hypothesis, and $d(X_1, \ldots, X_n)$ indicates we \emph{fail to reject} the null hypothesis.
\end{defn}

\begin{defn}
    Consider a decision rule $d$, which can fail in two distinct ways.
    \begin{itemize}
        \item A \emph{Type I error} is rejecting $H_0$ when it is true.
        \item A \emph{Type II error} is failing to reject $H_0$ when it is false.
    \end{itemize}
\end{defn}

\begin{exmp}
    Consider $H_0: \theta \in \Theta_0$ and $H_a: \theta \in \Theta_A$ where $\Theta_0 \intersection \Theta_a = \emptyset$ and $\Theta_0 \union \Theta_a = \Theta$. The probability of a type I error for a decision rule $d$ given data $X_1, \ldots, X_n$ is
    \begin{align*}
        P\left[d(X_1, \ldots, X_n) = 1 | \Theta_0\right].
    \end{align*}
    Since $\Theta_0$ and $\Theta_a$ are mutually exclusive and exhaustive, the probability of a type II error is
    \begin{align*}
        P\left[d(X_1, \ldots, X_n) = 0|H_a\right].
    \end{align*}
\end{exmp}

\begin{defn}
    Consider a decision rule $d$. The \emph{power} is the probability of rejecting $H_0$ given that $H_0$ is indeed false. If $d$ satisfies $P\left[d(X_1, \ldots, X_n)=1|H_0\right] \leq \alpha$ --- that is, the probability of a type I error is at most $\alpha$, we say $d$ is an \emph{at-most level $\alpha$} decision rule.
\end{defn}

\begin{exmp}
    Consider $H_0: \theta \in \Theta_0$, where $H_a: \theta \in \Theta_a$, and $\Theta_0 \union T_a = \Theta$ are both \emph{simple} hypotheses. Consider the \emph{likelihood ratio}
    \begin{align*}
        \frac{f(x_1, \ldots, x_n|H_0)}{f(x_1, \ldots, x_n|H_a)},
    \end{align*}
    and a chosen constant $c$.
\end{exmp}

\begin{lemma}{Neyman-Pearson}\label{neyman-pearson}\proofbreak
    Let $X_1, \ldots, X_N \distributed f(x|\theta)$ be independent and identically distributed random variables, and let $H_0$ and $H_A$ be simply hypotheses. If $d$ is an $\alpha$-level likelihood ratio test, and $d^{*}$ is \emph{any other} decision rule with significant at most $\alpha$, then the power of $d$ is at least the power of $d^{*}$. That is, $d$ is the most powerful $\alpha$-level decision rule.
\end{lemma}

\begin{proof}
    First, $d(x_1, \ldots, x_n) = 0$ if and only if
    \begin{align*}
        \frac{f(x_1, \ldots, x_n|H_0)}{f(x_1, \ldots, x_n|H_A)} \geq c,
    \end{align*}
    so $d(x_1, \ldots, x_n) = 0$ if and only if
    \begin{align*}
        Q(x_1, \ldots, x_n) = cf(x_1, \ldots, x_n|H_A) - f(x_1, \ldots, x_n|H_0) \leq 0.
    \end{align*}

    We will use this to prove that for any $x_1, \ldots, x_n$ and any decision rule $d^{*}$ (not necessarily with significance at most $\alpha$):
    \begin{equation}\tag{$\star$}\label{neyman-pearson-inequality}
        d^{*}(x_1, \ldots, x_n)Q(x_1, \ldots, x_n) \leq d^{*}(x_1, \ldots, x_n)Q(x_1, \ldots, x_n).
    \end{equation}

    Consider this by cases. In the case that $d*(x_1, \ldots, x_n) = 0$ and $d(x_1, \ldots, x_n) = 0$, the inequality is trivially true. In the case that $d*(x_1, \ldots, x_n) = 1$ and $d(x_1, \ldots, x_n) = 0$, we have $Q(x_1, \ldots, x_n) \leq 0$ by the above, and so
    \begin{align*}
        d^{*}(x_1, \ldots, x_n)Q(x_1, \ldots, x_n) &= Q(x_1, \ldots, x_n) \leq 0 = d(x_1, \ldots, x_n)d^{*}(x_1, \ldots, x_n)Q(x_1, \ldots, x_n).
    \end{align*}

    In the case that $d(x_1, \ldots, x_n) = 1$, we know $Q(x_1, \ldots, x_n|H_0) > 0$. Therefore, if $d^{*}(x_1, \ldots, x_n) = 0$ is once again trivial. If $d^{*}(x_1, \ldots, x_n) = 1$, then we have equality, which does satisfy our non-strict inequality.

    We can now integrate the inequality \ref{neyman-pearson-inequality}, so
    \begin{align*}
        \int d^{*}(x_1, \ldots, x_n)dx_1\cdots dx_n \leq \int d(x_1, \ldots, x_n)dx_1\cdots dx_n,
    \end{align*}
    and so
    \begin{align*}
        &\int d^{*}(x_1, \ldots, x_n)cf(x_1, \ldots, x_n|H_A)d_1\cdots dx_n - \int d^{*}(x_1, \ldots, x_n)f(x_1, \ldots, x_n|H_0)d_1\cdots dx_n \\
        &= \int d(x_1, \ldots, x_n)cf(x_1, \ldots, x_n|H_A)d_1\cdots dx_n - \int d(x_1, \ldots, x_n)f(x_1, \ldots, x_n|H_0)d_1\cdots dx_n.
    \end{align*}
    Notice this is equivalent to
    \begin{align*}
        \expectationof{cd^{*}|H_A} - \expectationof{d^{*}|H_0} \leq \expectationof{cd|H_A} - \expectationof{d|H_0},
    \end{align*}
    which in turn is
    \begin{align*}
        \expectationof{d|H_0}  - \expectationof{d^{*}|H_0} \leq c\expectationof{cd|H_A} - c\expectationof{d^{*}|H_A}.
    \end{align*}
    Let $\alpha$ be the significant of $d$ and $\alpha^{*}$ be the significance of $d^{*}$, and similarly for the powers $1 - \beta$ and $1 - \beta^{*}$. Then our inequality of expectatations state that
    \begin{align*}
        \alpha - \alpha^{*} \leq c(1 - \beta) - c(1 - \beta^{*}).
    \end{align*}
    Finally, we conclude that since $c > 0$, as long as we have $\alpha^{*} \leq \alpha$, we must also have $1 - \beta^{*} \leq 1 - \beta$.
\end{proof}

\begin{exmp}
    Suppose $\overline{X} = T_{\textrm{obs}}$. Under $H_0$, we know that
    \begin{align*}
        \frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}} \distributed N(0, 1).
    \end{align*}
\end{exmp}

\section{ANOVA (Analysis of Variance)}

Suppose we have random variables $Y_{ij}$ for $1 \leq j \leq n_j$, $1 \leq i \leq I$, which are independent, normally distributed data samples from populations $i = 1, \ldots, I$. Suppose further that every population has the same variance $\sigma^2$ and potentially different means $\mu_i$.

Consider the \emph{non-random} constant
\begin{align*}
    \mu = \frac{1}{I}\sum_{i=1}^{I}\mu_i,
\end{align*}
and let $\alpha_i = \mu_i - \mu$. Note that
\begin{align*}
    \sum_{i=1}^{n}\alpha_i = \sum_{i=1}^{n}\mu_i - \sum_{i=1}^{n}\mu_i = 0.
\end{align*}
Therefore, $Y_{ij}$ are independent and normal with distribution $N\left(\mu + \alpha_i, \sigma^2\right)$, or $Y_{ij} = \mu + \alpha_i + \varepsilon_{ij}$, where $\varepsilon_{ij} \distributed N(0, \sigma^2)$ are independent across all $i$ and $j$.

Suppose we wish to test the hypothesis that at least two $a_i$, $a_j$ are different, against the null hypothesis that all $I$ populations have the same mean. In the case that $I = 2$, the generalized likelihood ratio test is equivalent to rejecting the null hypothesis for large values of an F distributed random variable.

Let $\overline{Y}_{\cdot\cdot}$ be the \emph{grand mean} --- that is, the mean of all $Y_{ij}$. Let $s_i^2$ be the sample variance of the $i$th population (provided it has at least two samples). Consider the restricted case where every population contains the same number of samples.

For convienence, let $TSS$ be the total squared deivation from the grand mean:
\begin{align*}
    TSS = \sum_{i}\sum_{j}\left(Y_{ij} - \overline{Y}_{\cdot\cdot}\right)^2.
\end{align*}
Let $SSw$ be the sum of the variability within a single sample:
\begin{align*}
    SSw = \sum_{i}\sum_{j}\left(Y_{ij} - \overline{Y}_{i\cdot}\right)^2.
\end{align*}
Finally, let $SSb$ be the variability of the population sample means:
\begin{align*}
    SSb = \sum_{i}n_{i}\left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right)^2.
\end{align*}
Next, notice that
\begin{align*}
    TSS &= \sum_{i}\sum_{j}\left[\left(Y_{ij} - \overline{Y}_{i\cdot}\right)^2  + 2\left(Y_{ij} - \overline{Y}_{i\cdot}\right)\left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right) + \left(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot}\right)^2\right] \\
    &= SSw + 0 + SSb.
\end{align*}
Under the null hypothesis, $SSb/\sigma^2$ follows a $\chi^2$ distribution with $I - 1$ degrees of freedom. UNder all hypotheses, $SSw/\sigma^2$ follows a $\chi^2$ distribution with
\begin{align*}
    \sum_{i}(n_i - 1)
\end{align*}
degrees of freedom. If all $n_i = J$, then it has $I(J - 1)$ degrees of freedom. Note that $SSb$ and $SSw$ are in fact independent. Let
\begin{align*}
    T = \frac{\frac{SSb}{\sigma^2(I-1)}}{\frac{SSw}{\sigma^2I(J-1)}}.
\end{align*}
Therefore, under the null hypothesis and assuming $n_i = J$, $T$ must follow an $F(I-1, I(J-1))$ distribution.

\subsection{Tukey's test}

Consider the random variable $W$, defined to be the maximum across all distinct pairs $i_1, i_2$ from $1, \ldots, I$, of
\begin{align*}
    \frac{\left(\overline{Y}_{i_1\cdot} - \mu_{i_1}\right) - \left(\overline{Y_{i_2}} - \mu_{i_2}\right)}{\sqrt{Sp^2/J}},
\end{align*}
where
\begin{align*}
    Sp^2 = \frac{(J-1)S_1^2 + (J-1)S_2^2 + \cdots + (J-1)S_I^2}{1}.
\end{align*}
If the ANOVA $F$-test indicates the populations means are \emph{not} all identical, then Tukey's test can help identify which pair of means shows that largest difference.

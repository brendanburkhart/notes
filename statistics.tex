\chapter{Statistics}
\label{ch:statistics}

\section{Simple Random Sampling}

Consider a finite population of $N$ distinct objects, each with two associated measurements $x$ and $y$. We denote the $k$th object by $z_k = ((x_k, y_k), k)$. Let $\underline{\pi}(z_k) = (x_k, y_k)$, let $\pi_x(z_k) = x_k$, let $\pi_y(z_k) = y_k$, and finally let $\pi_{\mathrm{ind}}(z_k) = k$.

\subsection{Population Parameters}

Population parameters are \emph{deterministic} functions of the measurements associated with objects in a population.

For example, the population mean of the $y$-measurements is
\begin{align*}
    \mu_y = \frac{1}{N}\sum_{k=1}^{N}\pi_{y}(z_k),
\end{align*}
and the population variance of the $y$-measurements is
\begin{align*}
    \sigma_{y}^2 = \frac{1}{N}\sum_{k=1}^{N}\left(y_k - \mu_y\right)^2.
\end{align*}

The population covariance between the $x$ and $y$ measurements is
\begin{align*}
    \sigma_{xy} = \frac{1}{N}\sum_{k=1}^{N}\left(x_k-\mu_x\right)\left(y_k-\mu_y\right).
\end{align*}

\subsection{Sample Parameters}

Suppose we have a finite sample $Z_1, \ldots, Z_n$ of the entire population. Functions of such samples are known as \emph{statistics}, and will be used to construct useful \emph{estimators} of our population parameters. For example, the \emph{sample mean} can be used to estimate the population mean, and is defined by
\begin{align*}
    \bar{X} = \frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k),\;\; \bar{Y} = \frac{1}{n}\sum_{k=1}^{n}\pi_{y}(Z_k).
\end{align*}

\begin{defn}
    An \emph{estimator} is a deterministic function of the sample data.
\end{defn}

What do we mean by \emph{useful estimators}? Suppose $\theta$ is a population parameter, and $\hat{\theta}$ is an estimator for $\theta$. We say that
\begin{itemize}
    \item $\hat{\theta}$ is \emph{unbiased} for $\theta$ if $E(\hat{\theta}) = \theta$,
    \item $\hat{\theta}$ is \emph{consistent} for $\theta$ if for all $\varepsilon > 0$, the limit as $n$ approaches infinity of $P\left(\abs{\hat{\theta} - \theta} > \varepsilon\right)$ is zero.
\end{itemize}
We can then define useful estimators to simply be those that are unbiased and consistent.

\begin{prop}
    Let $Z_1, \ldots, Z_n$ be a finite sample drawn uniformly at random \emph{with replacement} from the entire population. Then for any $k_1, \ldots, k_n \in {1, \ldots, N}$,
    \begin{align*}
        P\left(Z_1=z_{k_1}, \ldots, Z_n=z_{k_n}\right) = \prod_{i=1}^{n}P(Z_{i}=z_{k_i}) = \left(\frac{1}{N}\right)^{n}.
    \end{align*}
    The draws are independent and identically distributed.
\end{prop}

\begin{prop}
    Now consider instead $Z_1, \ldots, Z_n$ drawn uniformly at random \emph{without replacement} from the entire population. Then for any distinct $k_1, \ldots, k_n \in {1, \ldots, N}$, where $n \leq N$,
    \begin{align*}
        P\left(Z_1=z_{k_1}, \ldots, Z_n=z_{k_n}\right) = \frac{(N-n)!}{N!}.
    \end{align*}
    The draws are exchangeable but dependent.
\end{prop}

\begin{rmk}
    We can view draws without replacement from a population with $N$ objects as being deterministic viewing of a specific permutation of the population.
\end{rmk}

\begin{exmp}
    Consider some $f$, and let
    \begin{align*}
        \beta = \frac{1}{N}\sum_{k=1}^{N}f(z_k),
    \end{align*}
    and a corresponding estimator
    \begin{align*}
        \frac{1}{n}\sum_{i=1}^{n}f(Z_i).
    \end{align*}
    Note that this estimator is unbiased by $\mathcal{LOTUS}$ \ref{lotus}.
\end{exmp}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ with replacement, then $\varianceof{\bar{X}} = \frac{\sigma_x^2}{n}$.
\end{prop}

\begin{proof}
    \begin{align*}
        \varianceof{\bar{X}} &= \varianceof{\frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k)} = \sum_{i=1}^{n}\sum_{j=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\left(0\right) \\
        &= \frac{1}{n^2}\sum_{i=1}^{n}\varianceof{X_i} = \frac{1}{n}\varianceof{X_i} = \frac{\sigma_x^2}{n}.
    \end{align*}
\end{proof}

\begin{rmk}
    We have seen that $\bar{X}$ is unbiased, and it must be consistent by the Weak Law of Large Numbers \ref{wlln}.
\end{rmk}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ without replacement, then $\varianceof{\bar{X}} = \frac{\sigma_x^2}{n} + \frac{\covarianceof{X_1}{X_2}}{n}\left(n-1\right)$.
\end{prop}

\begin{proof}
    \begin{align*}
        \varianceof{\bar{X}} &= \varianceof{\frac{1}{n}\sum_{k=1}^{n}\pi_{x}(Z_k)} = \sum_{i=1}^{n}\sum_{j=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_j} \\
        &= \sum_{i=1}^{n}\frac{1}{n^2}\covarianceof{X_i}{X_i} + \frac{\covarianceof{X_1}{X_2}}{n^2}\left(n^2 - n\right) \\
        &= \frac{\sigma_x^2}{n} + \frac{\covarianceof{X_1}{X_2}\left(n-1\right)}{n}.
    \end{align*}
\end{proof}

\begin{cor}
    \begin{align*}
        \covarianceof{X_1}{X_2} = -\frac{\sigma_x^2}{N-1}
    \end{align*}
\end{cor}

\begin{proof}
    We know that when drawing without replacement, if $n = N$ then $\varianceof{\bar{X}} = 0$. Therefore,
    \begin{align*}
        0 = \frac{\sigma_x^2}{N} + \frac{\covarianceof{X_1}{X_2}\left(N-1\right)}{N}.
    \end{align*}
\end{proof}

\begin{cor}
    \begin{align*}
        \varianceof{\bar{X}} = \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right).
    \end{align*}
\end{cor}

\begin{prop}
    If we are sampling $X_1, \ldots, X_n$ without replacement, then $\hat{\sigma}_x^2$ is a biased estimator.
\end{prop}

\begin{proof}
    \begin{align*}
        \hat{\sigma}_x^2 = \frac{1}{n}\sum_{i=1}^{n}\left(X_i - \bar{X}\right)^{2} = \frac{1}{n}\left[\sum_{i=1}^{n}X_i^2 - 2\sum_{i=1}^{n}X_i\bar{X} + \sum_{i=1}^{n}\bar{X}^{2}\right] = \frac{1}{n}\sum_{i=1}^{n}X_i^2 - \bar{X}^2.
    \end{align*}
    \begin{align*}
        \expectationof{\hat{\sigma}_x^2} &= \expectationof{\frac{1}{n}\sum_{i=1}^{n}X_i^2} - \expectationof{\bar{X}^2} = \frac{n\left(\sigma_x^2 + \mu_x^2\right))}{n} - \left(\varianceof{\bar{X}} + \mu_x^2\right) \\
        &= \left(\sigma_x^2 + \mu_x^2\right) - \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right) - \mu_x^2 = \sigma_x^2 - \frac{\sigma_x^2}{n}\left(\frac{N-n}{N-1}\right) \\
        &= \sigma_x^2\left[\frac{N\left(n-1\right)}{n\left(N-1\right)}\right].
    \end{align*}
    Clearly $\expectationof{\hat{\sigma}_x^2} \neq \sigma_x^2$ when $n \neq N$.
\end{proof}

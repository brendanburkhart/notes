\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{centernot}
\usepackage{fullpage}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[hypcap=false]{caption}
\usepackage{tikz, tkz-euclide}
\usetikzlibrary{decorations.pathreplacing,arrows}
\usetikzlibrary{quotes,angles,calc,intersections}

\usepackage{titling}
\usepackage{pdfpages}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{bm}

\usepackage{linear}
\usepackage{common}

\begin{document}

\title{Ordinary Differential Equations}
\author{Brendan Burkhart}
\maketitle

\tableofcontents
\newpage

\section{Classification}

\begin{defn}
    A \emph{differential equation} is an equation relating one or more independent variables, functions of those variables, and derivatives of those functions.
\end{defn}

\begin{exmp}\label{first-order-ode}
    \[\frac{\mathrm{d}x}{\mathrm{d}t} = x\]
\end{exmp}

\begin{defn}
    A \emph{solution} to a differential equation is an expression for the dependent functions of the differential equation in terms of the independent variables.
\end{defn}

Differential equations can be broadly classified into \emph{ordinary} and \emph{partial} differential equations.

\begin{defn}
    An \emph{ordinary} differential equation, or \emph{ODE} is a differential equation involving a single independent variable, and all derivatives are with respect to this variable.
\end{defn}

\begin{defn}
    A \emph{partial} differential equation, or \emph{PDE} is a differential equation involving functions of more than one independent variables, and derivatives may be with respect to any of those variables.
\end{defn}

\begin{defn}
    The \emph{general form} of an ODE is
    \[F(x, y, y', \ldots, y^{(n)}) = 0,\] that is some expression involving the independent variable $x$, the dependent variable $y$, and the derivatives of $y$.
\end{defn}

\begin{defn}
    The \emph{order} of a differential equation is the order of the highest derivative.
\end{defn}

\begin{exmp}
    \[\frac{\mathrm{d}^2\theta}{\mathrm{d}t^2} + \frac{g}{L}\sin\theta = 0\]
    This is a second order differential equation, while Example \ref{first-order-ode} is first order.
\end{exmp}

\begin{defn}
    A differential equation of a dependent variable $y$ and its derivatives is said to be \emph{linear} if it is an affine map with regard to $y$ and its derivatives.
\end{defn}

\begin{exmp}
    $\sin(x)y' + 2x^2y'' = x$ is linear.
\end{exmp}

\begin{exmp}
    $yy' + y'' = 0$ is non-linear.
\end{exmp}

\begin{defn}
    An \emph{autonomous} differential equation is one in which the independent variable does not explicitly appear. When the independent variable represents time in some way, these may also be called \emph{time-invariant}.
\end{defn}

\begin{exmp}
    $\frac{\mathrm{d}y}{\mathrm{d}x} = 5y - 20$ is an autonomous differential equation as the independent variable $x$ is not explicitly present.
\end{exmp}

\section{First order differential equations}

\begin{defn}
    The \emph{standard form} of a linear first order differential equation (with independent variable $t$ and dependent variable $y$) is \[\frac{\mathrm{d}y}{\mathrm{d}t} + p(t)y = g(t),\] for some arbitrary functions $p(t), g(t)$.
\end{defn}

Any linear first order autonomous ODE with independent variable $x$ and dependent variable $y$ can trivially be put into the form \[\frac{\mathrm{d}y}{\mathrm{d}x} = ay - b.\] This form can always be solved.

\begin{align*}
    \frac{\mathrm{d}y}{\mathrm{d}x} &= ay - b \\
    &= a(y - \frac{b}{a})
\end{align*}

Note that $\frac{\mathrm{d}}{\mathrm{d}x}{y - \frac{b}{a}} = \frac{\mathrm{d}}{\mathrm{d}x}y$. If $y \neq \frac{b}{a}$, it follows that $\ln(y - \frac{b}{a}) = ax + C$ for some constant of integration $C$, so $\abs{y - \frac{b}{a}} = ke^{ax}$ for some $k$. If $y > \frac{b}{a}$, then $y = \frac{b}{a} + ke^{ax}$, and similarly if $y < \frac{b}{a}$, then $y = \frac{b}{a} + ke^{ax}$ for some $k$. In the case that $y = \frac{b}{a}$, it follows that $\frac{\mathrm{d}y}{\mathrm{d}x} = 0$, so $y = \frac{b}{a} + ke^{ax}$ where $k = 0$.

Therefore, $y = \frac{b}{a} + ke^{ax}$ is a solution to any linear first order autonomous differential equation.

\subsection{Linear equations}

Linear first order ODEs which are not autonomous can sometimes be solved through the use of an \emph{integrating factor}. We can write a linear first order ODE as \[y' + p(t)y = g(t),\] where $p(t), g(t)$ are arbitrary functions. Our goal here is to find a function $\mu(t)$, called the \emph{integrating factor}, such that
\[\frac{\mathrm{d}}{\mathrm{d}t}\mu(t)y = \mu(t)y' + p(t)\mu(t)y.\] That is, we want $\mu'(t) = p(t)\mu(t)$. Once such $\mu(t)$ is found, since \[\mu(t)y' + \mu(t)p(t)y = \mu(t)g(t),\] we clearly have \[\frac{\mathrm{d}}{\mathrm{d}t}\mu(t)y = \mu(t)g(t).\] It follows that \[y = \frac{1}{\mu(t)}\int_{t_0}^t \mu(s)g(s)ds + C\] for some constants $t_0$ and $C$.

We of course need to also be able to find such a $\mu(t)$. Note that since $\mu'(t) = p(t)\mu(t)$ we have \[\frac{\mu'(t)}{\mu(t)} = p(t).\] Since $\frac{\mathrm{d}}{\mathrm{d}t}\ln(\mu(t)) = \frac{\mu'(t)}{\mu(t)}$, it follows that $\ln(\mu(t)) = \int p(t)$. Therefore, $\mu(t) = e^{\int p(t)dt}$.

\begin{exmp}
    Using an integrating factor to solve $ty' + 2y = 4t^2$ with the initial condition $y(1) = 2$.

    First, we need the differential equation in the standard form $y' + p(t)y = g(t)$. Doing this, we obtain $y' + \frac{2}{t}y = 4t$, provided that $t \neq 0$. Then we have $\ln(\mu(t)) = \int \frac{2}{t}dt = 2\ln(t) = \ln(t^2)$, so $\mu(t) = t^2$. We know have $\frac{\mathrm{d}}{\mathrm{d}t}\mu(t)y = 4t^3$, which we may integrate to obtain $t^2y = t^4 + c$, and so $y = t^2 + \frac{c}{t^2}$. Applying the initial condition $y(1) = 2$, we have $2 = 1 + c$, so $c = 1$ and our final particular solution is \[y = t^2 + \frac{1}{t^2}, t > 0.\]
\end{exmp}

\subsection{Separable equations}

A separable first order differential equation is of the form
\[\frac{\mathrm{d}y}{\mathrm{d}t} = g(t)h(y),\] so that we may ``separate'' the independent and dependent variables to obtain
\[\frac{1}{h(y)}\frac{\mathrm{d}y}{\mathrm{d}t} = g(t).\] This can be often be integrated, with the particularly nice property that due to the chain rule, we can simply integrate the LHS with respect to $y$.
\[\int \frac{1}{h(y)}dy = \int g(t)dt.\] These integrals may or may not be computable, and even if they are the resulting equation may not be solvable for $y$. However, it can sometimes be done and is a simple and straightforward technique.

\subsection{Autonomous equations}

In general, any first order autonomous ODE is of the form $\frac{dy}{dt} = f(y)$. Note that this form is always separable.

\begin{rmk}
    Note that the slope field of every autonomous ODE has the same slope for any given value of $y$, regardless of $t$.
\end{rmk}

\begin{defn}
    Let $y' = f(y)$ be a first order autonomous ODE, and consider the set $\left\{y \in \R \compbar f(y) = 0 \right\}$. This is the \emph{critical point} set of the ODE.
\end{defn}

\begin{defn}
    Let $y' = f(y)$ be a first order autonomous ODE. For any $y_{\star}$ in the critical point set, $y(t) = y_{\star}$ is an \emph{equilibrium solution} to the ODE.
\end{defn}

\begin{exmp}
    Let $y' = y(1-y)$. The polynomial $y(1-y)$ is zero when $y = 0$ or $y = 1$, so $y(t) = 0$ and $y(t) = 1$ are equilibrium solutions.
\end{exmp}

\begin{rmk}
    In between equilibria, the sign of $y'$ does not change.
\end{rmk}

\begin{rmk}
    Given a first order autonomous ODE $y' = f(y)$, since the sign of $y'$ goes not change between critical points, we can use the sign of $y''$(which is also $f'(y)$) to learn more about the behavior of solutions around critical points.
\end{rmk}

\begin{defn}
    Let $y' = f(y)$ be a first order autonomous ODE. If $f(y) > 0$ at a critical point $y_{\star}$, then when $y > y_{\star}$ the solution increases and when $y < y_{\star}$ the solution decreases. That is, solutions near the equilibrium of the critical point diverge from it, so we call these \emph{unstable} equilibria. Similarly, if $f'(y_{\star}) < 0$ then nearby solutions converge towards $y_{\star}$ and so we call it a \emph{stable} equilibrium.
\end{defn}

\begin{rmk}
    Stable equilibria are also called \emph{sinks}, while unstable equilibria may be called \emph{sources}.
\end{rmk}

\begin{exmp}
    Let $y' = (1-y)^2(y-4)$, and so $y'' = -2(1-y)(y-4) + (1-y)^2$. Since both $y'$ and $y''$ are continuous on $\R$, by Theorem \ref{general-first-order-existence} we know that a unique solution must exist for any given initial value. It follows that no solution curves cross each other anywhere.

    Furthermore, we have see that the critical points are $1$ and $4$, and since $y''(4) = 9$ and $y''(1) = 0$ it follows that $4$ is an unstable equilibrium. We know that $1$ is an equilibrium as well, and that $y''' < 0$, so $1$ is semi-stable -- solutions converge to it from above, and diverge from it below.
\end{exmp}

\subsection{Existence and uniqueness of solutions}

\begin{thm}\label{linear-first-order-existence}
    Let $I = (\alpha, \beta)$ be an open interval containing $t = t_0$, and let $p, g$ be functions that are continuous on $I$. There exists a unique function $y(t)$ that satisfies both the differential equation
    \[\frac{dy}{dt} + p(t)y = g(t)\] and the initial condition $y(t_0) = y_0$ for some $y_0$.
\end{thm}

\begin{proof}
    Since the differential equation is linear, it can be solved using an integrating factor, and the initial condition uniquely determines the constant of integration.
\end{proof}

\begin{thm}\label{general-first-order-existence}
    Let the functions $f$ and $\frac{\partial{f}}{\partial{y}}$ be continuous in some rectangular region $\alpha < t < \beta, \gamma < y < \delta$ containing a point $(t_0, y_0)$. Then, for some interval $(t_0 - h, t_0 + h)$ within $(\alpha, \beta)$, there exists a unique function $y(t)$ that satisfies both the differential equation
    \[\frac{dy}{dt} = f(t, y)\] and the initial condition $y(t_0) = y_0$.
\end{thm}

\begin{cor}
    The continuity of $f$ alone is sufficient for the existence of solutions, but not their uniqueness.
\end{cor}

\begin{rmk}
    Note that Theorem \ref{general-first-order-existence} ensures two solutions cannot intersect each other, since the differential equation with initial condition corresponding to the intersection would be satisfied by both solutions.
\end{rmk}

\begin{exmp}
    Consider $\dot{z} = z(1-z)$. Since $z(1-z)$ is a polynomial, it is continuous for all $z$, and so is $\frac{d}{dz}z(1-z)$. By Theorem \ref{general-first-order-existence}, it therefore has a unique solution for any given initial value.
\end{exmp}

\begin{exmp}
    Consider $y(t) = y^{2/3}$. Since $y^{2/3}$ is continuous for all $y$, the \emph{existence} of a solution is guaranteed for all initial values by Theorem $\ref{general-first-order-existence}$. However, since $\frac{d}{dy}y^{2/3} = \frac{2}{3}y^{-1/3}$ is discontinuous at $y = 0$, the \emph{uniqueness} of the solution is only guaranteed for initial values $y \neq 0$.
\end{exmp}

\begin{exmp}
    Consider $\dot{x} = \sqrt{x}$. Since $\sqrt{x}$ is only continuous for $x > 0$, given the initial condition $x(0) = x_0$, neither the existence nor uniqueness of a solution is not guaranteed.
\end{exmp}

\subsection{Exact equations}

\begin{defn}
    A first order ODE of the form $M(x, y) + N(x, y)y' = 0$ (where $y$ is a function of $x$) is \emph{exact} if there exists some function $\psi(x, y)$ such that $\frac{d\psi}{dx} = M(x, y) + N(x, y)y'$.
\end{defn}

\begin{exmp}
    $2x + y^2 + 2xyy' = 0$ is an exact first order ODE, since $\psi(x, y) = x^2 + xy^2$ gives $\frac{d\psi}{dx} = 2x + y^2 + 2xyy' = 0$. It follows that $\psi(x, y) = \int{0}dx$, and so $x^2 + xy^2 = c$ for some constant $c$ is a solution.
\end{exmp}

\begin{thm}
    Let functions $M, N, M_y, N_x$ be continuous on a simply connected region in the $xy$ plane. Then $M(x, y) + N(x, y)y' = 0$ is an exact differential equation if and only if $M_x(x, y) = N_y(x, y)$ for all points on the region.
\end{thm}

\begin{proof}\proofbreak
    ($\implies$) If it is an exact differential equation, then by definition there exists some $\psi(x, y)$ such that $\psi_x(x, y) = M(x, y)$ and $\psi_y(x, y) = N(x, y)$. Therefore, $\psi_{xy}(x, y) = M_y(x, y)$ and $\psi_{yx} = N_x(x, y)$. Since $M_y, N_x$ are continuous are some region, $\psi$ is $C^2$ are that region, and so $\psi_{xy} = \psi_{yx}$, implying $M_y = N_x$.

    ($\impliedby$) Since $M$ is continuous on the region, we can integrate to obtain $\varPhi(x, y) = Q(x, y) + h(y)$, where $\frac{d}{dx}Q = M$ and $h(y)$ is an arbitrary function. We need $\frac{d}{dx}\varPhi(x, y) = M(x, y) + N(x, y)y'$, so we need to be able to choose $h(y)$ such that $\frac{d}{dy}\varPhi = N(x,y)$. We therefore have $\frac{d}{dy}\varPhi = \frac{d}{dy}Q + h'(y) = N(y)$, and so $h'(y) = N(y) - \frac{d}{dy}Q$. This equation is integrable (giving a solution for $h(y)$) when $N(x,y) - \frac{d}{dy}Q(x,y)$ is solely a function of $y$. Since $\frac{d}{dx}\left(N(x,y) - \frac{d}{dy}Q(x,y)\right) = N_x - M_y = 0$, it is not a function of $x$, and so there is function $\varPhi(x,y)$ such that $\frac{d}{dy}\varPhi = N(x,y)$ and $\frac{d}{dx}\varPhi = M(x,y)$.
\end{proof}

\begin{exmp}
    Consider the first order differential equation \[\left(3x^2 - 2xy + 2\right) + \left(6y^2 - x^2 + 3\right)y' = 0.\] Since $\frac{d}{dy}\left(3x^2 - 2xy + 2\right) = -2x$ and $\frac{d}{dx}\left(6y^2 - x^2 + 3\right) = -2x$, this is an exact differential equation.
    \[\int \left(3x^2 - 2xy + 2\right)dx = x^3 - x^2y + 2x + h(y).\] Since $\frac{d}{dy}\left(x^3 - x^2y + 2x\right) = -x^2$, we then want $h'(y) = \left(6y^2 - x^2 + 3\right) - (-x^2) = 6y^2 + 3$. Then, $h(y) = \int h'(y)dy = 2y^3 + 3y$, and so the general solution is \[\psi(x, y) = x^3 - x^2y + 2x + 2y^3 + 3y = c.\]
\end{exmp}

\begin{rmk}
    Some non-exact first order differential equations can be made exact by way of an integrating factor.
\end{rmk}

\begin{exmp}
    Consider the differential equation $1 + \left(\frac{x}{y}- \sin(y)\right)y' = 0$. This is clearly not exact, but multiplying the equation by the integrating factor $y$ results in $y + \left(x - y\sin(y)\right)y' = 0$, which is exact and can be solved.
\end{exmp}

\begin{prop}
    All separable first order differential equations are exact.
\end{prop}

\begin{proof}
    Let $y' = g(y)h(x)$ be a separable equation. Then $-h(x) + \frac{1}{g(y)}y' = 0$. Since $\frac{\partial}{\partial y}\left(-h(x)\right) = 0 = \frac{\partial}{\partial}\frac{1}{g(y)}$, this is an exact differential equation.
\end{proof}

\section{Linear second order equations}

\subsection{Homogeneous equations with constant coefficients}

Let $ay'' + by' + cy = 0$ be a homogeneous second order differential equation, where $a, b, c$ are constants. Notice that if $y = e^{rt}$ for some $r$, then $a'' + by' + cy = ar^2e^{rt} + bre^{rt} + ce^{rt} = (ar^2 + br + c)e^{rt} = 0$. Since $e^{rt} \neq 0$, it follows that $ar^2 + br + c = 0$, and so $y = e^{rt}$ is a solution when $r$ is a root of $ar^2 + br + c = 0$. Furthermore, notice that any linear combination of solutions is also a solution, since the differential equation is homogeneous.

\begin{defn}
    For a differential equation of the form $ay'' + by' + cy = 0$, the equation $ar^2 + br + c = 0$ is called the \emph{characteristic eqution}.
\end{defn}

\begin{exmp}
    Consider the constant coefficient homogeneous differential equation $2y'' + 8y' - 10y = 10$. We know that $y = e^{rt}$ is a solution when $r$ is a root of $2r^2 + 8r - 10$, and since $2r^2 + 8r - 10 = (2r-2)(r+5)$, this gives us $r = 1, -5$. The general solution is then $y(t) = c_1e^t + c_2e^{-5t}$ for some constants $c_1, c_2 \in \R$.

    Given two initial conditions, we can solve for $c_1$ and $c_2$. For example, if $y(0) = 2$ and $y'(0) = -4$, we have $2 = c_1 + c_2$ and $-4 = c_1 - 5c_2$. This system can be solved, yielding $6 = 6c_2$, so $c_2 = 1$ and so $c_1 = 1$ as well.
\end{exmp}

\begin{defn}
    For functions $p(t), q(t)$, define the operator $L$ as
    \[L[\phi] = \frac{d^2}{dt^2}\phi + p(t)\frac{d}{dt}\phi + q(t)\phi.\]
\end{defn}

\begin{thm}\label{linear-second-order-existence}Existence and Uniqueness\proofbreak
    Consider the initial value problem $L[y] = g(t)$, with $y(t_0) = y_0$ and $y'(t_0) = y_0'$. If $p(t)$, $q(t)$, and $g(t)$ are continuous on an open interval $I$ where $t_0 \in I$, then there is exactly one solution to this problem, and it is defined for all of $I$.
\end{thm}

\begin{exmp}
    Consider $L[y] = 0$, $y(t_0) = 0$, and $y'(t_0) = 0$, where $p(t), q(t)$ are continuous on an open interval containing $t_0$. Then since $\phi(t) = 0$ is a solution, by Theorem \ref{linear-second-order-existence} this is the only solution.
\end{exmp}

\begin{thm}\label{linear-superposition}Superposition\proofbreak
    For $L[y] = 0$, let $y_1, y_2$ be solutions. Then $c_1y_1 + c_2y_2$ are solutions for any $c_1, c_2 \in \R$.
\end{thm}

\begin{proof}
    Let $\phi = c_1y_1 + c_2y_2$, then $\phi' = c_1y_1' + c_2y_2'$ and $\phi'' = c_1y_1'' + c_2y_2''$. Therefore,
    \begin{align*}
        L[\phi] &= \phi'' + p(t)\phi' + q(t)\phi \\
        &= (c_1y_1'' + c_2y_2'') + p(t)(c_1y_1' + c_2y_2') + q(t)(c_1y_1 + c_2y_2) \\
        &= c_1(y_1'' + p(t)y_1' + q(t)y_1) + c_2(y_2'' + p(t)y_2' + q(t)y_2) \\
        &= c_1L[y_1] + c_2L[y_2].
    \end{align*}
    Since $y_1, y_2$ are solutions to $L[y] = 0$, we then know that $L[\phi] = c_1(0) + c_2(0) = 0$, and so it is also a solution.
\end{proof}

\begin{defn}
    For functions $y_1, y_2$, the \emph{Wronskian determinant} is the determinant of the coefficient matrix of the system of equations
    \begin{align*}
        c_1y_1(t_0) + c_2y_2(t_0) = y_0, \\
        c_1y_1'(t_0) + c_2y_2'(t_0) = y_0',
    \end{align*}
    and is given by
    \[W = y_1y_2' - y_1'y_2\] at $t_0$.
\end{defn}

\begin{thm}\label{linear-initial-conditions-existence}
    For $L[y] = 0$, let $y_1, y_2$ be solutions. Given the initial conditions $y(t_0) = y_0$ and $y'(t_0) = y_0'$, there exists $c_1, c_2$ such that $y = c_1y_1 + c_2y_2$ is a solution satisfying the initial conditions if and only if the Wronskian $W = y_1y_2' - y_1'y_2$ is non-zero at $t_0$.
\end{thm}

\begin{thm}\label{linear-second-order-solution-family}
    For $L[y] = 0$, let $y_1, y_2$ be solutions. The family of solutions described by $y = c_1y_1 + c_2y_2$ includes every possible solution to $L[y] = 0$ if and only if there exists $t_0$ such that the Wronskian is non-zero at $t_0$.
\end{thm}

\begin{proof}
    Let $t_0$ be such that the Wronskian is non-zero at $t_0$, and let $\phi$ be any solution to $L[y] = 0$. Let $y_0 = \phi(t_0)$ and $y_0' = \phi'(t_0)$. Then by Theorem \ref{linear-initial-conditions-existence}, there exists some $c_1, c_2$ such that $c_1y_1 + c_2y_2$ is a solution to $L[y] = 0$ satisfying $y(t_0) = y_0$ and $y'(t_0) = y_0'$. Then by Theorem \ref{linear-second-order-existence}, this is the only such solution, and so it must be that $\phi = c_1y_1 + c_2y_2$.

    Now consider the case where no such $t_0$ exists. Then by Theorem \ref{linear-initial-conditions-existence} there exist initial conditions such that no linear combination of $y_1, y_2$ satisfy them. However, by Theorem \ref{linear-second-order-existence} there is a unique solution satisfying these conditions, and so that solution cannot be in the family $c_1y_1 + c_2y_2$.
\end{proof}

\begin{prop}
    Let $y_1 = e^{r_1t}, y_2 = e^{r_2t}$ be solutions to $L[y] = 0$. Then $c_1y_1 + c_2y_2$ describe all possible solutions if and only if $r_1 \neq r_2$.
\end{prop}

\begin{proof}
    The Wronskian is given by $y_1y_2' - y_1'y_2 = r_2e^{r_1t}e^{r_2t} = r_1e^{r_1t}e^{r_2t}$, so the Wronskian is non-zero precisely when $r_1 \neq r_2$. Then by Theorem \ref{linear-second-order-solution-family}, $c_1y_1 + c_2y_2$ describe all possible solutions if and only if $r_1 \neq r_2$.
\end{proof}

\begin{thm}\label{family-basis-exists}
    Consider $L[y] = 0$ where $p(t), q(t)$ are continuous on some open interval $I$. Fix some $p_0 \in I$. Let $y_1$ be the unique solution that satisfies $y(t_0) = 1$ and $y'(t_0) = 0$, and let $y_2$ be the unique solution which satisfies $y(t_0) = 0$ and $y'(t_0) = 1$. Then linear combinations of $y_1$ and $y_2$ describe all possible solutions to $L[y]$.
\end{thm}

\begin{proof}
    Note that $y_1, y_2$ are guaranteed to exist and be unique on $I$ by Theorem \ref{linear-second-order-existence}. At $t_0$, the Wronskian is given by $W = y_1(t_0)y_2'(t_0) - y_1'(t_1)y_2(t_0) = 1(1) - 0(0) = 1$. Since $W \neq 0$, by Theorem \ref{linear-second-order-solution-family} linear combinations of $y_1, y_2$ describe all possible solutions.
\end{proof}

\begin{thm}\label{abels-thm}Abel's Theorem\proofbreak
    Let $y_1, y_2$ be solutions to $L[y] = 0$, where $p(t, q(t))$ are continuous on some open interval $I$. Then the Wronskian is given by \[W(t) = c\exp\left(-\int p(t)dt\right)\] for some constant $c$.
\end{thm}

\begin{proof}
    We know that
    \begin{align*}
        y_1'' + p(t)y_1' + q(t)y_1 &= 0, \\
        y_2'' + p(t)y_2' + q(t)y_2 &= 0.
    \end{align*}
    Multiplying these equations by $y_2$ and $y_1$ respectively and then subtracting, we obtain
    \[(y_1''y_2 - y_1y_2'') + p(t)(y_1'y_2 - y_1y_2') + q(t)(y_1y_2 - y_1y_2).\] Note that the Wronskian is $W = y_1'y_2 - y_1y_2'$, and that
    \[W' = y_1''y_2 + y_1'y_2' - y_1'y_2' - y_1y_2'' = y_1''y_2 - y_1y_2''.\]
    Therefore, we have $W' + p(t)W = 0$. This is a first order linear differential equation, which we can solve with the integrating factor $\exp\left(\int p(t)dt\right)$, giving us $W(t) = c\exp\left(-\int p(t)dt\right)$ for some constant $c$.
\end{proof}

\begin{cor}\label{wronskian-always-zero}
    The Wronskian is either always zero or always non-zero.
\end{cor}

\begin{proof}
    Since $\exp\left(-\int p(t)dt\right) \neq 0$, we know that the Wronskian is only zero if $c = 0$, in which case it is always zero.
\end{proof}

\subsection{Complex roots of the characteristic equation}

\begin{thm}\label{complex-solution-decomposition}
    Consider $L[y] = 0$. If $y(t) = u(t) + iv(t)$ is a complex-valued solution, then $u(t)$ and $v(t)$ are also solutions.
\end{thm}

\begin{proof}
    Since $y(t)$ is a solution, we know that
    \begin{align*}
        0 = L[y] &= (u''(t) + iv''(t)) + p(t)(u'(t) + iv'(t)) + q(t)(u(t) + iv(t)) \\
        &= (u''(t) + p(t)u'(t) + q(t)u(t)) + i(v''(t) + p(t)v'(t) + q(t)v(t)) \\
        &= L[u] + iL[v].
    \end{align*}
    It follows that $L[u] = 0 = L[v]$.
\end{proof}

\begin{exmp}
    Consider $y'' + y' + \frac{37}{4}y = 0$, with the initial conditions $y(0) = 2$ and $y'(0) = 8$.

    The roots of the characteristic equation are \[r = \frac{-1 \pm \sqrt[]{1 - 37}}{2} = -\frac{1}{2} \pm 3i.\] Since $e^{rt}$ is a solution, we know that $\exp\left(-\frac{1}{2}t + 3it\right)$ is a solution. By Euler's formula, \[\exp\left(-\frac{1}{2}t + 3it\right) = e^{-t/2}e^{3it} = e^{-t/2}\cos(3t) + ie^{-t/2}\sin(3t).\] Then by Theorem \ref{complex-solution-decomposition}, $u(t) = e^{-t/2}\cos(3t)$ and $v(t) = e^{-t/2}\sin(3t)$ are real-valued solutions. The Wronskian of these solutions is $W(u, v) = 3e^{-t}$, which is non-zero so by Theorem \ref{linear-second-order-solution-family} linear combinations are $u$ and $v$ describe all possible (even complex-valued) solutions.

    For the specific initial conditions $y(0) = 2$ and $y'(0) = 8$, we find $c_1e^{0}\cos(0) + c_2e^{0}\sin(0) = 2$ implies $c_1 = 2$, and $c_1(-\frac{1}{2}e^{0}\cos(0) - 3e^{0}\sin(0)) + c_2(-\frac{1}{2}e^{0}\sin(0) + 3e^{0}\cos(0))$, so $-1 + 3c_2 = 8$, and so $c_2 = 3$. Therefore, our solution is \[y = 2e^{-t/2}\cos(3t) + 3e^{-t/2}\sin(3t).\]
\end{exmp}

\begin{prop}
    Let $ay'' + by' + cy = 0$ be a differential equation whose characteristic equation has complex roots $r_1, r_2$. If we decompose $r_1$ (or $r_2$) into $\lambda + i\mu$, then linear combinations of $e^{\lambda}\cos(\mu{t})$ and $e^{\lambda}\sin(\mu{t})$ describe all possible solutions.
\end{prop}

\begin{proof}
    Since $e^{r_1t}$ is a solution, by Theorem \ref{complex-solution-decomposition} we know that $e^{\lambda}\cos(\mu{t})$ and $e^{\lambda}\sin(\mu{t})$ are also solutions. The Wronskian is
    \[W = e^{2\lambda}\mu.\] Since $\lambda \in \R$, we know that $e^{2\lambda} < 0$, and since $r_1 \notin \R$, we know that $\mu \neq 0$. Therefore, $W \neq 0$. By Theorem \ref{linear-second-order-solution-family}, linear combinations of $e^{\lambda}\cos(\mu{t})$ and $e^{\lambda}\sin(\mu{t})$ describe all positions solutions.
\end{proof}

\subsection{Repeated roots of the characteristic equation}

Given the differential equation $ay'' + by' + cy = 0$, when $b^2 - 4ac = 0$ the characteristic equation has a repeated root $r$, giving the solution $e^{rt}$. To describe all possible solutions we need two distinct solutions whose Wronskian is non-zero.

For any solution $y_1 = e^{rt}$ and arbitrary constant $c$, we know that $ce^{rt}$ is also a solution. To find a second solution, we can try generalizing this to $\phi(t) = v(t)e^{rt}$ and attempt to find a function $v(t)$ such that $\phi(t)$ is a solution. We can see that $\phi' = v'(t)e^{rt} + rv(t)e^{rt}$ and $\phi'' = v''(t)e^{rt} + 2rv'(t)e^{rt} + r^2v(t)e^{rt}$. Note that $r = -\frac{b}{2a}$, so
\begin{align*}
    \phi &= e^{rt}v(t) \\
    \phi' &= e^{rt}(v'(t) - \frac{b}{2a}v(t)) \\
    \phi'' &= e^{rt}(v''(t) - \frac{b}{a}v'(t) + \frac{b^2}{4a^2}v(t)).
\end{align*}
For $\phi$ to be a solution, it must satisfy $a\phi'' + b\phi' + c\phi = 0$. Since $e^{rt}$ is a common factor to each term and is non-zero, we can divide it out and rearrange to obtain
\[av''(t) - bv'(t) + \frac{b^2}{4a}v(t) + bv'(t) - \frac{b^2}{2a}v(t) + cv(t) = 0,\] and so $av''(t) - \left(\frac{b^2}{4a} + c\right)v(t) = 0$. Note that since $ar^2 + br + c = 0$ has a repeated root, $b^2 - 4ac = 0$, and so $\frac{b^2}{4a} - c = 0$. Therefore, $av''(t) = 0$, and so $v''(t) = 0$. It follows that $v(t) = c_1t + c_2$ for some constants $c_1, c_2$. Therefore, $y = c_1te^{rt} + c_2e^{rt}$ is a solution for any $c_1, c_2$.

\begin{thm}
    Let $ay'' + by' + cy = 0$ be a differential equation whose characteristic equation has a repeated root $r$. Then linear combinations of $e^{rt}$ and $te^{rt}$ describe all possible solutions.
\end{thm}

\begin{proof}
    The Wronskian is \[W = e^{rt}(e^{rt} + te^{rt}) - te^{rt}(e^{rt}) = e^{2rt},\] which is non-zero since $r \in \R$. Therefore, by Theorem \ref{linear-second-order-solution-family} linear combinations of $e^{rt}$ and $te^{rt}$ describe all possible solutions.
\end{proof}

\end{document}

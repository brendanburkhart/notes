\chapter{Numerical Analysis}
\label{ch:numerical}

\section{Floating Point Approximations}

Given $x \in \R$, and a floating point representation $\hat{x}$, we define the relative error to be $r = \frac{\abs{\hat{x}-x}}{x}$. We then define the \emph{significant figures} of $\hat{x}$ to be maximum $m \in \N$ such that $10^{m}r \leq 5$.

Consider a method to calculate from value. Let
\begin{itemize}
    \item $x$ be the input,
    \item $\hat{x}$ be the approximated input,
    \item $f(x)$ be the correct value of the output,
    \item $\hat{f}$ approximate of $f$.
\end{itemize}
Then the total error is
\begin{align*}
    \hat{f}\left(\hat{x}\right) - f(x) = \underbrace{\hat{f}\left(\hat{x}\right) - f(\hat{x})}_{\textrm{Computational error}} + \overbrace{f\left(\hat{x}\right) - f(x)}^{\textrm{Propagated error}}.
\end{align*}

\begin{exmp}
    Consider a $\mathbb{C}^2$ function $f: \R \to \R$. Using a small value for $h$, we can approximate the derivative of $f$ as
    \begin{align*}
        D_{h}f(x) = \frac{f(x + h) - f(x)}{h}.
    \end{align*}

    The computational error in this approximation is
    \begin{align*}
        D_{h}f(x) - f'(x).
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a function $f: \R \times \R \to \R$ given by $f(x, y) = xy$. If the approximation error for $x$ and $y$ is $\delta_x$ and $\delta_y$ respectively, then
    \begin{align*}
        \hat{x} &= x + \delta_x, \\
        \hat{y} &= y + \delta_y.
    \end{align*}

    Therefore, the propagated error is simply
    \begin{align*}
        f\left(\hat{x}, \hat{y}\right) - f(x, y) &= x\delta_y + y\delta_x + \delta_x\delta_y,
    \end{align*}
    and so the relative propagated error is
    \begin{align*}
        \frac{f\left(\hat{x}, \hat{y}\right) - f(x, y)}{f(x, y)} = \frac{x\delta_y + y\delta_x + \delta_x\delta_y}{xy} = \frac{\delta_x}{x} + \frac{\delta_y}{y} + \frac{\delta_x\delta_y}{xy}.
    \end{align*}
    Under the reasonable assumption that $\delta_x \ll x$ and $\delta_y \ll y$, the term $\frac{\delta_x\delta_y}{xy}$ is negligible, and so the relative error of $xy$ is roughly $R(x) + R(y)$.
\end{exmp}

\section{Algorithms and Convergence}

\begin{defn}
    A mathematical problem is \emph{well-posed} or \emph{stable} when a solution \emph{exists}, is \emph{unique}, and is \emph{continuous} with respect to the input.
\end{defn}

\begin{defn}
    Given a function $f$, the \emph{condition number} is the ratio of the relative change in output to the relative change in input.
\end{defn}

\begin{exmp}
    Let $f: \R \to \R$ be a continuous function. Then the condition number, denoted by $K_f(x)$, is
    \begin{align*}
        K_f(x) = \lim_{x' \to x}\abs{\frac{\left[f(x') - f(x)\right]/f(x)}{\left(x'-x\right)/x}} = \abs{\frac{x}{f(x)}f'(x)}.
    \end{align*}
\end{exmp}

\begin{defn}
    Now consider $f: \R^d \to R$ where $f$ is $C^2$. The absolute change in the output can be approximated at the first order as
    \begin{align*}
        \delta_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\delta_{x_i} = \left\langle \nabla f(x), \delta_{x} \right\rangle.
    \end{align*}
    The relative change is then
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{\delta_{x_i}}{f(x)}.
    \end{align*}
    Let $\varepsilon_{x_i} = \frac{x_i'-x_i}{x_i}$, then $\delta_{x_i} = \varepsilon_{x_i}x_i$, so
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{x_i}{f(x)}\varepsilon_{x_i}.
    \end{align*}
\end{defn}

\begin{exmp}
    Consider $f(x_1, x_2) = x_1 - x_2$. Then
    \begin{align*}
        K_f(x_1, x_2) = \abs{\frac{x_1}{x_1 - x_2}} + \abs{\frac{x_2}{x_1 - x_2}}.
    \end{align*}
\end{exmp}

\begin{defn}
    Let $f(x)$ be a function with condition number $K_f$, and let $x_y = g(x_n)$ be an iterative algorithm, where $K_n(y_n)$ is the condition number of $g(x_n)$. Then we define
    \begin{align*}
        \hat{K}(y) = \sup_{n \leq N}K_n(y_n),
    \end{align*}
    and say that an algorithm is \emph{numerically stable} when
    \begin{align*}
        \hat{K}(y) \lesssim 2K_f.
    \end{align*}
\end{defn}

\begin{defn}
    The transformation of an ill-posed problem into a well-posed one is \emph{regularization}.
\end{defn}

\begin{exmp}
    Consider a linear system $Ax = b$, where $\det A = 0$. If $b$ is in the null space of $A$, then there are infinite solutions, and otherwise there are no solutions. Therefore it is not a well-posed problem. However, if we consider
    \begin{align*}
        \min_{x}\norm{Ax - b},
    \end{align*}
    this clearly always has a solution. Furthermore, when a solution exists to $Ax - b$ it will coincide.
\end{exmp}

\begin{defn}
    Consider functions $f(h)$ and $g(h)$ such that $g(h)$ goes to zero as $h$ does. We use Landau big-$O$ notation and say that
    \begin{align*}
        f(h) = L + O(g(h))
    \end{align*}
    if there exists $h_0$ and $C$ such that $\abs{f(h) - L} < Cg(h)$ for all $\abs{h} < h_0$.
\end{defn}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. We say that it is a \emph{$p$th order approximation} when
    \begin{align*}
        \abs{f_n(x_n) - f(x)} = O\left(\frac{1}{n^p}\right).
    \end{align*}
\end{defn}

\begin{rmk}
    When we increase $n$ (the number of iterations used) by a factor of ten, a $p$th order approximation will gain $p$ significant figures.
\end{rmk}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. If
    \begin{align*}
        \lim_{n\to\infty}\frac{\abs{f_{n+1}(x_{n+1}) - f(x)}}{\abs{f_{n}(x_{n}) - f(x)}^{p}} = \mu
    \end{align*}
    for some $\mu > 0$, we say that $f_n$ converges with rate of order $p$.
\end{defn}

\begin{rmk}
    When an algorithm converges with a rate of order $p$, we say that we have
    \begin{itemize}
        \item \emph{linear convergence} if $p = 1$ and $\mu < 1$,
        \item \emph{quadratic convergence} if $p = 2$,
        \item \emph{cubic convergence} if $p = 3$.
    \end{itemize}
\end{rmk}

\section{Matrix Factorization}

\begin{defn}
    LU composition
\end{defn}

\begin{defn}
    Diagonally dominant
\end{defn}

\begin{thm}
    Positive definite $\iff$ $\forall k A_k$ is positive definite.
    Positive definite $\iff$ $\forall k \det A_k > 0$ \emph{and} $A$ is Hermitian.
    Positive definite $\iff$ $A$ is Hermitian and Gaussian elimination can be performed without permutations.
\end{thm}

\begin{thm}{Cholesky Factorization}\label{cholesky-factorization}\proofbreak
    Let $A \in M_{n \times n}(\C)$ be a positive definite matrix. There exists a unique lower triangular matrix $L \in M_{n \times n}(\C)$ such that $L_{ii} > 0$ such that $A = LL^{*}$. 
\end{thm}

\begin{proof}
    We will proceed by induction on $n$. In the base case $n=1$, we simply have $A = [a]$, and so for $L = [l]$, $LL^{*} = l\overline{l}$, and since $l_{ii}$ must be real we have $l = \sqrt{a}$. Since $A$ is positive definite, by \ref{positive-semidefinite-criteria} it follows that $a > 0$ and so $\sqrt{a} \in \R$.

    Assume that a unique decomposition exists for all $0 \leq n < k$ for some $k$. Consider any positive definite matrix $A \in M_{k \times k}(\C)$. We can write
    \[
        A = \begin{pmatrix}
            A_{k-1} & b \\
            \overline{b} & a_{kk}
        \end{pmatrix}
    \]
\end{proof}

\begin{defn}
    A \emph{band matrix} is a particular type of sparse matrix in which the non-zero entries occur only within $p$ diagonals above the major diagonal, and $q$ diagonals above. Formally, $A \in M_{n \times n}$ is a \emph{band matrix} if there exists \emph{unique} $p, q \in \{1, \ldots, n\}$ such that $A_{ij}$ is zero whenever $j - i \geq p$ or $i - j \geq q$.
\end{defn}

\begin{rmk}
    Gaussian elimination is always successful as long as a pivot can be found.
\end{rmk}

\begin{defn}
    In \emph{partial pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        i_{k} = \argmax_{k\leq i\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, and then proceed as normal. We can construct a permutation matrix $P$ such that $PA = LU$.
\end{defn}

\begin{defn}
    In \emph{complete pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        (i_{k}, j_{k}) = \argmax_{k\leq i, j\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, if $j_k > k$, we swap columns $k$ and $j_k$, and then proceed as normal. We can construct permutation matrices $P$ and $Q$ such that $PAQ = LU$.
\end{defn}

\begin{rmk}
    While complete pivoting may be theoretically superior, partial pivoting yields similar results and so is preferred in practice.
\end{rmk}

\begin{thm}
    Let $A \in M_{n \times n}(\C)$ be Hermitian. There exists a unitary matrix $U$ such that $A = UDU^{*}$, where
    \begin{align*}
        D = \begin{pmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n \\
        \end{pmatrix}
    \end{align*}
    and $\lambda_1, \ldots, \lambda_n$ are the eigenvalues (with multiplicity) of $A$.
\end{thm}

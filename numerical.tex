\setchaptergraphic{}

\chapter{Numerical Analysis}
\label{ch:numerical}

\section{Floating Point Approximations}

Given $x \in \R$, and a floating point representation $\hat{x}$, we define the relative error to be $r = \frac{\abs{\hat{x}-x}}{x}$. We then define the \emph{significant figures} of $\hat{x}$ to be maximum $m \in \N$ such that $10^{m}r \leq 5$.

Consider a method to calculate from value. Let
\begin{itemize}
    \item $x$ be the input,
    \item $\hat{x}$ be the approximated input,
    \item $f(x)$ be the correct value of the output,
    \item $\hat{f}$ approximate of $f$.
\end{itemize}
Then the total error is
\begin{align*}
    \hat{f}\left(\hat{x}\right) - f(x) = \underbrace{\hat{f}\left(\hat{x}\right) - f(\hat{x})}_{\textrm{Computational error}} + \overbrace{f\left(\hat{x}\right) - f(x)}^{\textrm{Propagated error}}.
\end{align*}

\begin{exmp}
    Consider a $\mathbb{C}^2$ function $f: \R \to \R$. Using a small value for $h$, we can approximate the derivative of $f$ as
    \begin{align*}
        D_{h}f(x) = \frac{f(x + h) - f(x)}{h}.
    \end{align*}

    The computational error in this approximation is
    \begin{align*}
        D_{h}f(x) - f'(x).
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a function $f: \R \times \R \to \R$ given by $f(x, y) = xy$. If the approximation error for $x$ and $y$ is $\delta_x$ and $\delta_y$ respectively, then
    \begin{align*}
        \hat{x} &= x + \delta_x, \\
        \hat{y} &= y + \delta_y.
    \end{align*}

    Therefore, the propagated error is simply
    \begin{align*}
        f\left(\hat{x}, \hat{y}\right) - f(x, y) &= x\delta_y + y\delta_x + \delta_x\delta_y,
    \end{align*}
    and so the relative propagated error is
    \begin{align*}
        \frac{f\left(\hat{x}, \hat{y}\right) - f(x, y)}{f(x, y)} = \frac{x\delta_y + y\delta_x + \delta_x\delta_y}{xy} = \frac{\delta_x}{x} + \frac{\delta_y}{y} + \frac{\delta_x\delta_y}{xy}.
    \end{align*}
    Under the reasonable assumption that $\delta_x \ll x$ and $\delta_y \ll y$, the term $\frac{\delta_x\delta_y}{xy}$ is negligible, and so the relative error of $xy$ is roughly $R(x) + R(y)$.
\end{exmp}

\section{Algorithms and Convergence}

\begin{defn}
    A mathematical problem is \emph{well-posed} or \emph{stable} when a solution \emph{exists}, is \emph{unique}, and is \emph{continuous} with respect to the input.
\end{defn}

\begin{defn}
    Given a function $f$, the \emph{condition number}, or \emph{conditioning number}, is the supremum of ratio of the relative change in output to the relative change in input.
\end{defn}

\begin{exmp}
    Let $f: \R \to \R$ be a continuous function. Then the condition number, denoted by $K_f(x)$, is
    \begin{align*}
        K_f(x) = \lim_{x' \to x}\abs{\frac{\left[f(x') - f(x)\right]/f(x)}{\left(x'-x\right)/x}} = \abs{\frac{x}{f(x)}f'(x)}.
    \end{align*}
\end{exmp}

\begin{defn}
    Now consider $f: \R^d \to R$ where $f$ is $C^2$. The absolute change in the output can be approximated at the first order as
    \begin{align*}
        \delta_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\delta_{x_i} = \left\langle \nabla f(x), \delta_{x} \right\rangle.
    \end{align*}
    The relative change is then
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{\delta_{x_i}}{f(x)}.
    \end{align*}
    Let $\varepsilon_{x_i} = \frac{x_i'-x_i}{x_i}$, then $\delta_{x_i} = \varepsilon_{x_i}x_i$, so
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{x_i}{f(x)}\varepsilon_{x_i}.
    \end{align*}
\end{defn}

\begin{thm}\label{matrix-condition-number}
    Let $A$ be a non-singular matrix, and $\norm{\cdot}$ be an induced norm. Then
    \begin{align*}
        \kappa(A) = \norm{A}\norm{A^{-1}}.
    \end{align*}
\end{thm}

\begin{proof}
    By definition, the conditioning number at $b$ is
    \begin{align*}
        \sup_{\delta b \neq 0}\frac{\norm{A^{-1}(b + \delta b) - A^{-1}b}}{\norm{A^{-1}b}}\frac{\norm{b}}{\norm{\delta b}}.
    \end{align*}
    Notice that
    \begin{align*}
        \frac{\norm{A^{-1}(b + \delta b) - A^{-1}b}}{\norm{A^{-1}b}} = \frac{\norm{A^{-1}\delta b}}{\norm{A^{-1}b}}.
    \end{align*}
    Therefore, the supremum of this across all $b$ is
    \begin{align*}
        \kappa(A) = \sup_{b \neq 0,\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{A^{-1}b}}\frac{\norm{b}}{\norm{\delta b}}.
    \end{align*}
    Let $c = A^{-1}b$, then we can separate the supremums over $c$ and $\delta b$ to obtain
    \begin{align*}
        \kappa(A) = \sup_{c \neq 0,\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{c}}\frac{\norm{Ac}}{\norm{\delta b}} = \left[\sup_{c \neq 0}\frac{\norm{Ac}}{\norm{c}}\right]\left[\sup_{\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{\delta b}}\right].
    \end{align*}
    Then by Theorem \ref{induced-norm} we have
    \begin{align*}
        \kappa(A) = \norm{A}\norm{A^{-1}}.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider $f(x_1, x_2) = x_1 - x_2$. Then
    \begin{align*}
        K_f(x_1, x_2) = \abs{\frac{x_1}{x_1 - x_2}} + \abs{\frac{x_2}{x_1 - x_2}}.
    \end{align*}
\end{exmp}

\begin{defn}
    Let $f(x)$ be a function with condition number $K_f$, and let $x_y = g(x_n)$ be an iterative algorithm, where $K_n(y_n)$ is the condition number of $g(x_n)$. Then we define
    \begin{align*}
        \hat{K}(y) = \sup_{n \leq N}K_n(y_n),
    \end{align*}
    and say that an algorithm is \emph{numerically stable} when
    \begin{align*}
        \hat{K}(y) \lesssim 2K_f.
    \end{align*}
\end{defn}

\begin{defn}
    The transformation of an ill-posed problem into a well-posed one is \emph{regularization}.
\end{defn}

\begin{exmp}
    Consider a linear system $Ax = b$, where $\det A = 0$. If $b$ is in the null space of $A$, then there are infinite solutions, and otherwise there are no solutions. Therefore it is not a well-posed problem. However, if we consider
    \begin{align*}
        \min_{x}\norm{Ax - b},
    \end{align*}
    this clearly always has a solution. Furthermore, when a solution exists to $Ax - b$ it will coincide.
\end{exmp}

\begin{defn}
    Consider functions $f(h)$ and $g(h)$ such that $g(h)$ goes to zero as $h$ does. We use Landau big-$O$ notation and say that
    \begin{align*}
        f(h) = L + O(g(h))
    \end{align*}
    if there exists $h_0$ and $C$ such that $\abs{f(h) - L} < Cg(h)$ for all $\abs{h} < h_0$.
\end{defn}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. We say that it is a \emph{$p$th order approximation} when
    \begin{align*}
        \abs{f_n(x_n) - f(x)} = O\left(\frac{1}{n^p}\right).
    \end{align*}
\end{defn}

\begin{rmk}
    When we increase $n$ (the number of iterations used) by a factor of ten, a $p$th order approximation will gain $p$ significant figures.
\end{rmk}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. If
    \begin{align*}
        \lim_{n\to\infty}\frac{\abs{f_{n+1}(x_{n+1}) - f(x)}}{\abs{f_{n}(x_{n}) - f(x)}^{p}} = \mu
    \end{align*}
    for some $\mu > 0$, we say that $f_n$ converges with rate of order $p$.
\end{defn}

\begin{rmk}
    When an algorithm converges with a rate of order $p$, we say that we have
    \begin{itemize}
        \item \emph{linear convergence} if $p = 1$ and $\mu < 1$,
        \item \emph{quadratic convergence} if $p = 2$,
        \item \emph{cubic convergence} if $p = 3$.
    \end{itemize}
\end{rmk}

\section{Matrix Factorization}

\begin{defn}
    LU composition
\end{defn}

\begin{defn}
    An $n \times n$ matrix $A$ is said to be \emph{diagonally dominant} if, for all $1 \leq i \leq n$,
    \begin{align*}
        \abs{A_{ii}} \geq \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
    $A$ is furthermore \emph{strictly} diagonally dominant if, for all $1 \leq i \leq n$,
    \begin{align*}
        \abs{A_{ii}} > \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
\end{defn}

\begin{thm}
    Positive definite $\iff$ $\forall k A_k$ is positive definite.
    Positive definite $\iff$ $\forall k \det A_k > 0$ \emph{and} $A$ is Hermitian.
    Positive definite $\iff$ $A$ is Hermitian and Gaussian elimination can be performed without permutations.
\end{thm}

\begin{thm}{Cholesky Factorization}\label{cholesky-factorization}\proofbreak
    Let $A \in M_{n \times n}(\C)$ be a positive definite matrix. There exists a unique lower triangular matrix $L \in M_{n \times n}(\C)$ such that $L_{ii} > 0$ such that $A = LL^{*}$. 
\end{thm}

\begin{proof}
    We will proceed by induction on $n$. In the base case $n=1$, we simply have $A = [a]$, and so for $L = [l]$, $LL^{*} = l\overline{l}$, and since $l_{ii}$ must be real we have $l = \sqrt{a}$. Since $A$ is positive definite, by \ref{positive-semidefinite-criteria} it follows that $a > 0$ and so $\sqrt{a} \in \R$.

    Assume that a unique decomposition exists for all $0 \leq n < k$ for some $k$. Consider any positive definite matrix $A \in M_{k \times k}(\C)$. We can write
    \[
        A = \begin{pmatrix}
            A_{k-1} & b \\
            \overline{b} & a_{kk}
        \end{pmatrix}
    \]
\end{proof}

\begin{defn}
    A \emph{band matrix} is a particular type of sparse matrix in which the non-zero entries occur only within $p$ diagonals above the major diagonal, and $q$ diagonals above. Formally, $A \in M_{n \times n}$ is a \emph{band matrix} if there exists \emph{unique} $p, q \in \{1, \ldots, n\}$ such that $A_{ij}$ is zero whenever $j - i \geq p$ or $i - j \geq q$.
\end{defn}

\begin{rmk}
    Gaussian elimination is always successful as long as a pivot can be found.
\end{rmk}

\begin{defn}
    In \emph{partial pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        i_{k} = \argmax_{k\leq i\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, and then proceed as normal. We can construct a permutation matrix $P$ such that $PA = LU$.
\end{defn}

\begin{defn}
    In \emph{complete pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        (i_{k}, j_{k}) = \argmax_{k\leq i, j\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, if $j_k > k$, we swap columns $k$ and $j_k$, and then proceed as normal. We can construct permutation matrices $P$ and $Q$ such that $PAQ = LU$.
\end{defn}

\begin{rmk}
    While complete pivoting may be theoretically superior, partial pivoting yields similar results and so is preferred in practice.
\end{rmk}

\begin{thm}
    Let $A \in M_{n \times n}(\C)$ be Hermitian. There exists a unitary matrix $U$ such that $A = UDU^{*}$, where
    \begin{align*}
        D = \begin{pmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n \\
        \end{pmatrix}
    \end{align*}
    and $\lambda_1, \ldots, \lambda_n$ are the eigenvalues (with multiplicity) of $A$.
\end{thm}

\begin{lemma}\label{singular-non-difference-inequality}
    Consider any singular matrix $B$, and a proper (and therefore induced) norm $\norm{\cdot}$, then
    \begin{align*}
        \norm{A^{-1}}\norm{A - B} \geq 1.
    \end{align*}
\end{lemma}

\begin{proof}
    Since $\norm{\cdot}$ is proper by assumption,
    \begin{align*}
        \norm{A^{-1}B - I_n} = \norm{A^{-1}(A-B)} \leq \norm{A^{-1}}\norm{A-B}.
    \end{align*}
    For convenience, let $C = A^{-1}B - I_n$. If we can show $\norm{C} \geq 1$ we are done. For any $x \in F^n$, note that since $C$ is induced we have
    \begin{align*}
        \norm{C} \geq \frac{\norm{Cx}}{\norm{x}} = \frac{\norm{A^{-1}Bx - x}}{\norm{x}} \geq \abs{\frac{\norm{A^{-1}Bx}}{\norm{x}} - \frac{\norm{x}}{\norm{x}}}.
    \end{align*}
    Since $B$ is singular, there exists $x$ such that $Bx = 0$ and $x \neq 0$ so $\norm{x} \neq 0$. Then for $x \in \mathcal{N}(B)$
    \begin{align*}
        \norm{C} \geq \abs{\frac{0}{\norm{x}} - \frac{\norm{x}}{\norm{x}}} = 1.
    \end{align*}
\end{proof}

\begin{thm}
    The condition number of a matrix $A$ can be characterized by
    \begin{align*}
        \frac{1}{\kappa(A)} = \min\left\{\frac{\norm{A - B}}{\norm{A}}: \det(B) = 0\right\}.
    \end{align*}
\end{thm}

\begin{proof}
    Let $A$ be a non-singular matrix, and $B$ be any singular matrix, and $\norm{\cdot}$ an induced norm. Then by Lemma \ref{singular-non-difference-inequality}
    \begin{align*}
        \frac{\norm{A - B}}{\norm{A}} \geq \frac{1}{\norm{A}\norm{A^{-1}}}.
    \end{align*}
    and since $\norm{A}\norm{A^{-1}} = \kappa(A)$ by Theorem \ref{matrix-condition-number} we find that $1/\kappa(A)$ is at most the infimum of the set.

    Proving that this minimum is actually attained requires further tools from functional analysis, see ``Numerical Linear Algebra'' by W. Kahan for the proof.
\end{proof}

\begin{defn}
    Given a linear system $Ax = b$ with solution $x_0$ and an approximate solution $\tilde{x}$, let the \emph{residual} of $\tilde{x}$ be $r = A\tilde{x} - Ax_0 = A\tilde{x} - b$ and let $e = \tilde{x} - x_0 = A^{-1}r$.
\end{defn}

\begin{prop}
    Let $Ax = b$ be a linear system with solution $x$, and let $r$ be the residual of some approximate solution. Then by consistency of the norm, we obtain
    \begin{itemize}
        \item $\norm{r} \leq \norm{A}\norm{e}$,
        \item $\norm{e} \leq \norm{A^{-1}}\norm{r}$,
        \item $\norm{b} \leq \norm{A}\norm{x}$,
        \item $\norm{x} \leq \norm{A^{-1}}\norm{b}$.
    \end{itemize}
\end{prop}

\begin{prop}
    The relative error in an approximate solution $\tilde{x}$ can be founded by
    \begin{align*}
        \frac{\norm{e}}{\norm{x_0}} \leq \kappa(A)\frac{\norm{r}}{\norm{b}}.
    \end{align*}
\end{prop}

\begin{proof}
    Note that $\norm{e} \leq \norm{A^{-1}}\norm{r}$ and $\norm{b} \leq \norm{A}\norm{x_0}$ so $\norm{x_0} \geq \norm{b}/\norm{A}$. Therefore,
    \begin{align*}
        \frac{\norm{e}}{\norm{x_0}} \leq \norm{A}\norm{A^{-1}}\frac{\norm{r}}{\norm{b}} = \kappa(A)\frac{\norm{r}}{\norm{b}}.
    \end{align*}
\end{proof}

\begin{defn}
    Consider a matrix $A$. The \emph{singular values} of $A$, denoted by $\sigma(A)$, are the eigenvalues of $A^{*}A$.
\end{defn}

\begin{rmk}
    We know that $\norm{A}_2 = \sqrt{\rho(A^{*}A)}$, in which case $\kappa(A)$ is
    \begin{align*}
        \frac{\max_{\lambda \in \sigma(A)}\abs{\lambda}}{\min_{\lambda \in \sigma(A)}\abs{\lambda}}.
    \end{align*}
\end{rmk}

\begin{thm}
    Consider a linear system $Ax = b$ with a induced matrix norm and $A$ non-singular. For perturbations $\delta A$ and $\delta b$ such that
    \begin{align*}
        \norm{\delta A} < \frac{1}{\norm{A^{-1}}},
    \end{align*}
    the following hold:
    \begin{itemize}
        \item $A + \delta A$ is non-singular,
        \item $(A + \delta A)(x + \delta x) = (b + \delta b)$,
    \end{itemize}
    and
    \begin{align*}
        \frac{\norm{\delta x}}{\norm{x}} \leq \frac{\kappa(A)}{1 - \kappa(A)\frac{\norm{\delta A}}{\norm{A}}}\left(\frac{\norm{\delta A}}{\norm{A}} + \frac{\norm{\delta b}}{\norm{b}}\right).
    \end{align*}
\end{thm}

\begin{proof}
    Notice that
    \begin{align*}
        \norm{\delta A} \leq \frac{1}{\norm{A^{-1}}} \implies \norm{A^{-1}\delta A} \leq \norm{A^{-1}}\norm{\delta A} < 1,
    \end{align*}
    and
    \begin{align*}
        A + \delta A = A\left(I_n + A^{-1}\delta A\right)
    \end{align*}
    Let $M = -A^{-2}\delta A$, so $\norm{A} < 1$, and then
    \begin{align*}
        I_n + A^{-1}\delta A)
    \end{align*}

    {\large\color{red}Finish proof}
\end{proof}

\begin{defn}
    Consider an $n \times n$ invertible matrix $A$, then the \emph{growth factor} of $A$ is
    \begin{align*}
        \rho = \frac{1}{\norm{A}_{\infty}}\max_{1 \leq i,j,k \leq n} \abs{A_{ij}^{(k)}},
    \end{align*}
    where $A^{(k)}$ is the $k$th iterate obtained via Gaussian elimination.
\end{defn}

\begin{thm}
    Consider the linear system $Ax = b$, where $A$ is an $n \times n$ invertible matrix, and its $LU$ decomposition obtained via Gaussian elimination. Assume presence of numerical error, so
    \begin{align*}
        LU = A + E
    \end{align*}
    for some $n \times n$ matrix $E$. Then
    \begin{align*}
        \frac{\norm{E}_{\infty}}{\norm{A}_{\infty}} \leq n^2\rho\delta,
    \end{align*}
    where $\rho$ is the growth factor of $A$ and $\delta$ is the machine epsilon.
\end{thm}

\begin{rmk}
    Using different pivoting strategies, the growth factor $\rho$ may be controlled to an extent. Using \emph{complete pivoting}, we can bound it as
    \begin{align*}
        \rho \leq (1.8)n^{\ln(n)/4}.
    \end{align*}
\end{rmk}

\section{Fixed-Point Methods}

We will attempt to transform the linear system $Ax = b$ into the form $x = Tx + c$, the so-called ``fixed-point'' form of $x$, since the solutions are the fixed-points of the affine transformation.

Note that for all invertible $n \times n$ matrices $B$, where we have $Ax = b$ for some vector $b$, we can write $Ax = b$ as $Bx + (A - B)x = b$, which in turn may be rewritten as $x = B^{-1}(B-A)x + B^{-1}b = (I_n - B^{-1}A)x + B^{-1}b$.

\begin{defn}
    Select (arbitrary) point $x^{(0)} \in \R^{d}$ and recursively define
    \begin{align*}
        x^{(k+1)} &= Tx^{(k)} + c = \left(I_n - B^{-1}A\right)x^{(k)} + B^{-1}b
    \end{align*}
    which is equivalent to
    \begin{align*}
        B\left(x^{(k+1)} - x^{(k)}\right) = b - Ax^{(k)}.
    \end{align*}
    We call the expression on the right hand side the \emph{residual}. Various \emph{stoppping criterion} for termination of this algorithm are possible, such as when the relative (or absolute) residual falls beneath a chosen threshold.
\end{defn}

\begin{defn}{Jacobi method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{Jacobi method}, we choose $B = D$, and so
    \begin{align*}
        T_{J} = I - B^{-1}A = I - D^{-1}A = I-D^{-1}(D - L - U) = D^{-1}(L + U).
    \end{align*}
    Notice that we then have
    \begin{align*}
        x^{(k + 1)}_{i} = \frac{-\sum_{j\neq i}A_{ij}x^{(k_j)} + b_i}{A_{ii}}.
    \end{align*}
\end{defn}

\begin{defn}{Gauss-Seidel method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{Gauss-Seidel method}, we choose $B = D - L$, and so
    \begin{align*}
        T_{G} = I - \left(D - L\right)^{-1}\left(D - L - U\right) = I - I - \left(D - L\right)^{1}\left(-U\right) = \left(D-L\right)^{-1}U.
    \end{align*}
    Notice that we then have
    \begin{align*}
        x^{(k + 1)}_{i} = \frac{-\sum_{j=1}^{i-1}A_{ij}x^{(k_j+1)} - \sum_{j= i+1}^{n}A_{ij}x^{(k_j)} + b_i}{A_{ii}}.
    \end{align*}
\end{defn}

\begin{rmk}
    In general, the Gauss-Seidel method tends to perform better than the Jacobi method.
\end{rmk}

\begin{thm}\label{fixed-point-convergence}
    Let $T \in M_{n \times n}(\R)$ be invertible, and let $x_0 \in \R^n$. The sequence $x^{(k+1)} = Tx^{(k)} + c$ will converge to a solution for \emph{any} choice of $x_0$ if and only if $\rho(T) < 1$. This solution is unique and does not depend on choice of initial $x^{(0)}$.
\end{thm}

\begin{proof}
    Let $\norm{\cdot}$ be a proper norm. First, we will see that a unique fixed-point always exists and is unique. Since $\rho(T) < 1$, then $I - T$ is invertible and so $(I - T)x = c$ has a unique solution. Now let us turn to convergence.

    ($\implies$) Let $x$ be a solution, and consider the largest eigenvalue in absolute value $\rho(A)$ with associated eigenvector $u$. Then let $x_0 = x - u$, and so $x - x^{(k)} = \rho(A)^{(k)}u$, so $x{(k)}$ converges only if $\rho(A) < 1$.

    ($\impliedby$) First, we will prove that $x - x^{(k)} = T^{k}(x - x_0)$ by induction. In the base case, this is trivially true since $T^0 = I$. Then, assuming $x - x^{(k)} = T^{k}(x - x_0)$, we then have
    \[T\left[x - x^{(k)}\right] = T\left[T^{k}(x - x_0)\right],\]
    and so
    \[Tx + c - Tx^{(k)} - c = T^{k+1}(x - x_0).\]
    Since $x$ is by construction a fixed point of $f(v) = Tv + c$, it follows that $Tx + c = x$, and by definition $Tx^{(k)} + c = x^{(k+1)}$, so we have
    \begin{align*}
        x - x^{(k+1)} = T^{k+1}(x - x_0).
    \end{align*}
    Now that we have shown that $x - x^{(k)} = T^{k}(x - x_0)$ for $k \geq 0$, it follows since the norm is proper that $\norm{T^{k}(x - x_0)} \leq \norm{T^k}\norm{x-x_0}$. We know this converges to zero as $k$ goes to infinity since $\rho(T) < 1$, and so $x^{(k)}$ converges to $x$.
\end{proof}

\begin{prop}
    Let $\norm{\cdot}$ be a induced norm such that $\norm{T} < 1$ then $x^{(k)}$ converges and $\norm{x - x^{k}} \leq \norm{T}^{k}\norm{x - x_0}$ and $\norm{x - x^{k}} \leq \frac{\norm{T}^{k}}{1 - \norm{T}}\norm{x^{(1)} - x^{0}}$.
\end{prop}

\begin{thm}
    Let $A$ be an $n \times n$ matrix. If $A$ is strictly diagonally dominant, then the Jacobi and Gauss-Seidel methods converges for all $x^{(0)} \in \R^n$, and furthermore
    \begin{align*}
        \norm{T_{G}}_{\infty} \leq \norm{T_{J}}_{\infty} < 1.
    \end{align*}
\end{thm}

\begin{proof}
    Recall $T_{J}$ is defined to be $D^{-1}(L + U)$, so
    \begin{align*}
        \norm{T_{J}}_{\infty} = \max_{1\leq i \leq n}\frac{1}{D_{ii}}\sum_{j=1}^{n}(L + U)_{ji} = \max_{1\leq i \leq n}\frac{1}{A_{ii}}\sum_{j \neq i}A_{ji}.
    \end{align*}
    Since $A$ is strictly diagonally dominant, $A_{ii}$ is strictly greater than the sum for all $i$, and so
    \begin{align*}
        \norm{T_{J}}_{\infty} < 1.
    \end{align*}
\end{proof}

\begin{defn}
    Let $A$ be an $n \times n$ matrix. We say that $A$ is \emph{irreducible} when there \emph{does not} exist a permutation matrix $P$ such that $\transposeof{P}AP$ is a block upper triangular matrix
    \begin{align*}
        \left[\begin{array}{c|c}
            \tilde{A}_{11} & \tilde{A}_{12} \\
            \hline
            \raisebox{0pt}[12pt][0pt]{\scalebox{1.5}{$0$}} & \tilde{A}_{22}
        \end{array}\right]
    \end{align*}
    such that $\tilde{A}_{11} \in M_{p\times p}$ and $\tilde{A}_{22} \in M_{q \times q}$ where $p + q = n$, and $n > p, q \geq 1$. Note that $\tilde{A}_{ij}$ may be anything -- in particular they may contain zero entries.
\end{defn}

\begin{prop}
    An $n \times n$ matrix $A$ is irreducible if and only if the directed graph $G(A)$, of which $A$ is the adjacency matrix, is strongly connected.
\end{prop}

\begin{proof}
    First, note that $G(\transposeof{P}AP)$ is isomorphic to $G(A)$ --- that is, the two matrices are the adjacency matrices of the same graphs, up to order of vertices. To see this, let $\sigma$ be the permutation induced by $P$ --- that is, $P_{ij} = \delta_{i\sigma(j)}$. Then
    \begin{align*}
        AP_{\sigma_{i}j} &= \sum_{k=1}^{n}A_{\sigma(i)k}P(kj) = A_{\sigma(i)\sigma(j)}, \\
        \transposeof{P}AP_{ij} &= \sum_{k=1}^{n}\transposeof{P}_{ik}AP_{kj} = AP_{\sigma(i)j}.
    \end{align*}
    Therefore, $\transposeof{P}AP_{ij} = A_{\sigma(i)\sigma(j)}$, so $G(A)$ and $G(\transposeof{P}AP)$ are isomorphic since permutations are bijections. We know that if $A$ is isomorphic to $B$, then $A$ is strongly connected if and only if $B$ is strongly connected.

    To construct the adjacency matrix, we must have an index on the vertices $V_i$. Then we may consider a path from $V_i$ to $V_j$. Such a path must correspond to a sequence $a_1, a_2, \ldots, a_k$ such that $a_1 = n$, $a_k = 1$, and $\transposeof{P}AP_{a_{\ell}a_{\ell+1}} = 1$ for all $\ell \in \{1, \ldots, k-1\}$. Since $\transposeof{P}AP_{a_{\ell}a_{\ell+1}} = 0$ if $a_{\ell} > p$ and $a_{\ell+1} \leq n-q = p$, it follows that $a_{\ell+1} > p$ whenever $a_{\ell} > p$. Since $a_1 = n > p$ it would then follow that $a_k > p$. Therefore, no such path may exist if $\transposeof{P}AP$ is block upper triangular, and so if \emph{any} permutation matrix $P$ makes $\transposeof{P}AP$ block upper triangular then $G(A)$ cannot be strongly connected.

    Assume that $G(A)$ is not strongly connected, so there exists vertices $\alpha$ and $\beta$ such that there is no path from $\alpha$ to $\beta$. Let $V_{\alpha}$ be the set of all vertices $v$ such that there is a path from $\alpha$ to $v$, and let $V_{\beta}$ be the set of all vertices $v$ such that there is a path from $v$ to $\beta$. Note that $V_{\alpha}$ and $V_{\beta}$ must be disjoint by assumption. Let $p = \abs{V_{\beta}}$, and consider a permutation $\sigma$ which sends $V_{\beta}$ to $\{1, \ldots, p\}$ and $V_{\alpha}$ to $\{p+1, \ldots, n\}$. Notice that ${\transposeof{P_{\sigma}}AP_{\sigma}}_{ij} = 0$ whenever $p+1 \leq i \leq n$ and $1 \leq j \leq p$. Therefore, there exists $\sigma$ such that $\transposeof{P_{\sigma}}AP_{\sigma}$ is block upper triangular whenever $G(A)$ is not strongly connected.
\end{proof}

\begin{defn}
    An $n \times n$ matrix is \emph{irreducibly diagonally dominant} when it is irreducible, (weakly) diagonally dominant, and additionally there exists $i$ such that
    \begin{align*}
        \abs{A_{ii}} > \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
\end{defn}

\begin{prop}
    Let $A$ be an $n \times n$ matrix. If $A$ is irreducibly diagonally dominant, then the Jacobi method converges for all $x^{(0)} \in \R^n$.
\end{prop}

\begin{defn}{SOR method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{SOR method} (successive over-relaxtion), we choose
    \begin{align*}
        B(\omega) = \frac{1}{\omega}\left(D - \omega L\right).
    \end{align*}
    \begin{align*}
        T(\omega) = I - \left(\frac{1}{\omega}\left(D - \omega L\right)\right)^{-1}\left(D - L - U\right) = \left(D - \omega L\right)^{-1}\left[(1-\omega)D + \omega U\right].
    \end{align*}
\end{defn}

\begin{thm}{Kahan's Theorem}\label{kahans-theorem}\proofbreak
    If $\det D \neq 0$, then
    \begin{align*}
        \rho\left(T(\omega)\right) \geq \abs{\omega - 1}.
    \end{align*}
\end{thm}

\begin{proof}
    \begin{align*}
        \det\left(T(\omega)\right) = \frac{\det\left((1-\omega)D + \omega U\right)}{\det\left(D - \omega L\right)} = \frac{\left(1-\omega\right)^{n}\det\left(D\right)}{\det\left(D\right)} = \left(1 - \omega\right)^{n}.
    \end{align*}
\end{proof}

\begin{thm}{Ostrowski-Reich}\label{ostrowski-reich}\proofbreak
    If $A$ is positive definite and $0 < \omega < 2$, then $p(T(\omega)) < 1$.
\end{thm}

\section{Matrix Normal Forms}

\begin{thm}{Schur Normal Form}\proofbreak
    Let $A$ be an $n \times n$ matrix over $\C$. There exists a unitary $n \times n$ matrix $Q$ such that $Q^{*}AQ$ is upper triangular.
\end{thm}

\begin{proof}
    We proceed by induction on $n$. In the base case, $A$ is trivially upper triangular so we may take $Q = [1]$. Now assume this holds for some $n-1$, and consider $A$ an $n \times n$ matrix. Since $A$ is over $\C$, it must have an eigenvalue $\lambda_1$ with eigenvector $v_1$ such that $\norm{v_1}_2 = 1$. Complete $v_1$ to an orthonormal basis $\{v_1, \ldots, v_n\}$ for $\C^n$. Then take $U$ to be the matrix whose $i$th column is $v_i$, and so $U$ is a unitary matrix such that
    \begin{align*}
        U^{*}AU = \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{a}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$A_1$}}
        \end{array}\right].
    \end{align*}
    By assumption, there exists unitary $U_1$ such that $U_1^{*}A_1U_1$ is upper triangular. Let
    \begin{align*}
        Q = U\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right].
    \end{align*}
    Note that
    \begin{align*}
        Q^{*}Q = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right]^{*}U^{*}U\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1^{*}$}}
        \end{array}\right]\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = I_n,
    \end{align*}
    and so $Q$ is unitary. Furthermore,
    \begin{align*}
        Q^{*}AQ = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right]^{*}
        \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{a}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$A_1$}}
        \end{array}\right]
        \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{b}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$R_1$}}
        \end{array}\right] = R,
    \end{align*}
    which must be upper triangular since $R_1$ is upper triangular by assumption.
\end{proof}

\chapter{Numerical Analysis}
\label{ch:numerical}

\section{Floating Point Approximations}

Given $x \in \R$, and a floating point representation $\hat{x}$, we define the relative error to be $r = \frac{\abs{\hat{x}-x}}{x}$. We then define the \emph{significant figures} of $\hat{x}$ to be maximum $m \in \N$ such that $10^{m}r \leq 5$.

Consider a method to calculate from value. Let
\begin{itemize}
    \item $x$ be the input,
    \item $\hat{x}$ be the approximated input,
    \item $f(x)$ be the correct value of the output,
    \item $\hat{f}$ approximate of $f$.
\end{itemize}
Then the total error is
\begin{align*}
    \hat{f}\left(\hat{x}\right) - f(x) = \underbrace{\hat{f}\left(\hat{x}\right) - f(\hat{x})}_{\textrm{Computational error}} + \overbrace{f\left(\hat{x}\right) - f(x)}^{\textrm{Propagated error}}.
\end{align*}

\begin{exmp}
    Consider a $\mathbb{C}^2$ function $f: \R \to \R$. Using a small value for $h$, we can approximate the derivative of $f$ as
    \begin{align*}
        D_{h}f(x) = \frac{f(x + h) - f(x)}{h}.
    \end{align*}

    The computational error in this approximation is
    \begin{align*}
        D_{h}f(x) - f'(x).
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a function $f: \R \times \R \to \R$ given by $f(x, y) = xy$. If the approximation error for $x$ and $y$ is $\delta_x$ and $\delta_y$ respectively, then
    \begin{align*}
        \hat{x} &= x + \delta_x, \\
        \hat{y} &= y + \delta_y.
    \end{align*}

    Therefore, the propagated error is simply
    \begin{align*}
        f\left(\hat{x}, \hat{y}\right) - f(x, y) &= x\delta_y + y\delta_x + \delta_x\delta_y,
    \end{align*}
    and so the relative propagated error is
    \begin{align*}
        \frac{f\left(\hat{x}, \hat{y}\right) - f(x, y)}{f(x, y)} = \frac{x\delta_y + y\delta_x + \delta_x\delta_y}{xy} = \frac{\delta_x}{x} + \frac{\delta_y}{y} + \frac{\delta_x\delta_y}{xy}.
    \end{align*}
    Under the reasonable assumption that $\delta_x \ll x$ and $\delta_y \ll y$, the term $\frac{\delta_x\delta_y}{xy}$ is negligible, and so the relative error of $xy$ is roughly $R(x) + R(y)$.
\end{exmp}

\section{Algorithms and Convergence}

\begin{defn}
    A mathematical problem is \emph{well-posed} or \emph{stable} when a solution \emph{exists}, is \emph{unique}, and is \emph{continuous} with respect to the input.
\end{defn}

\begin{defn}
    Given a function $f$, the \emph{condition number} is the ratio of the relative change in output to the relative change in input.
\end{defn}

\begin{exmp}
    Let $f: \R \to \R$ be a continuous function. Then the condition number, denoted by $K_f(x)$, is
    \begin{align*}
        K_f(x) = \lim_{x' \to x}\abs{\frac{\left[f(x') - f(x)\right]/f(x)}{\left(x'-x\right)/x}} = \abs{\frac{x}{f(x)}f'(x)}.
    \end{align*}
\end{exmp}

\begin{defn}
    Now consider $f: \R^d \to R$ where $f$ is $C^2$. The absolute change in the output can be approximated at the first order as
    \begin{align*}
        \delta_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\delta_{x_i} = \left\langle \nabla f(x), \delta_{x} \right\rangle.
    \end{align*}
    The relative change is then
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{\delta_{x_i}}{f(x)}.
    \end{align*}
    Let $\varepsilon_{x_i} = \frac{x_i'-x_i}{x_i}$, then $\delta_{x_i} = \varepsilon_{x_i}x_i$, so
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{x_i}{f(x)}\varepsilon_{x_i}.
    \end{align*}
\end{defn}

\begin{exmp}
    Consider $f(x_1, x_2) = x_1 - x_2$. Then
    \begin{align*}
        K_f(x_1, x_2) = \abs{\frac{x_1}{x_1 - x_2}} + \abs{\frac{x_2}{x_1 - x_2}}.
    \end{align*}
\end{exmp}

\begin{defn}
    Let $f(x)$ be a function with condition number $K_f$, and let $x_y = g(x_n)$ be an iterative algorithm, where $K_n(y_n)$ is the condition number of $g(x_n)$. Then we define
    \begin{align*}
        \hat{K}(y) = \sup_{n \leq N}K_n(y_n),
    \end{align*}
    and say that an algorithm is \emph{numerically stable} when
    \begin{align*}
        \hat{K}(y) \lesssim 2K_f.
    \end{align*}
\end{defn}

\begin{defn}
    The transformation of an ill-posed problem into a well-posed one is \emph{regularization}.
\end{defn}

\begin{exmp}
    Consider a linear system $Ax = b$, where $\det A = 0$. If $b$ is in the null space of $A$, then there are infinite solutions, and otherwise there are no solutions. Therefore it is not a well-posed problem. However, if we consider
    \begin{align*}
        \min_{x}\norm{Ax - b},
    \end{align*}
    this clearly always has a solution. Furthermore, when a solution exists to $Ax - b$ it will coincide.
\end{exmp}

\begin{defn}
    Consider functions $f(h)$ and $g(h)$ such that $g(h)$ goes to zero as $h$ does. We use Landau big-$O$ notation and say that
    \begin{align*}
        f(h) = L + O(g(h))
    \end{align*}
    if there exists $h_0$ and $C$ such that $\abs{f(h) - L} < Cg(h)$ for all $\abs{h} < h_0$.
\end{defn}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. We say that it is a \emph{$p$th order approximation} when
    \begin{align*}
        \abs{f_n(x_n) - f(x)} = O\left(\frac{1}{n^p}\right).
    \end{align*}
\end{defn}

\begin{rmk}
    When we increase $n$ (the number of iterations used) by a factor of ten, a $p$th order approximation will gain $p$ significant figures.
\end{rmk}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. If
    \begin{align*}
        \lim_{n\to\infty}\frac{\abs{f_{n+1}(x_{n+1}) - f(x)}}{\abs{f_{n}(x_{n}) - f(x)}^{p}} = \mu
    \end{align*}
    for some $\mu > 0$, we say that $f_n$ converges with rate of order $p$.
\end{defn}

\begin{rmk}
    When an algorithm converges with a rate of order $p$, we say that we have
    \begin{itemize}
        \item \emph{linear convergence} if $p = 1$ and $\mu < 1$,
        \item \emph{quadratic convergence} if $p = 2$,
        \item \emph{cubic convergence} if $p = 3$.
    \end{itemize}
\end{rmk}

\setchaptergraphic{}

\chapter{Numerical Analysis}
\label{ch:numerical}

\section{Floating Point Approximations}

Given $x \in \R$, and a floating point representation $\hat{x}$, we define the \emph{relative error} to be $r = \frac{\abs{\hat{x}-x}}{x}$. We then define the \emph{significant figures} of $\hat{x}$ to be the maximum $m \in \N$ such that $10^{m}r \leq 5$.

Consider a method to calculate some value. Let
\begin{itemize}
    \item $x$ be the input,
    \item $\hat{x}$ be the approximated input,
    \item $f(x)$ be the correct value of the output,
    \item $\hat{f}$ be the approximated output.
\end{itemize}
Then the total error is
\begin{align*}
    \hat{f}\left(\hat{x}\right) - f(x) = \underbrace{\hat{f}\left(\hat{x}\right) - f(\hat{x})}_{\textrm{Computational error}} + \overbrace{f\left(\hat{x}\right) - f(x)}^{\textrm{Propagated error}}.
\end{align*}

\begin{exmp}
    Consider a $C^2$ (twice-continuously differentiable) function $f: \R \to \R$. Using a small value for $h$, we can approximate the derivative of $f$ as
    \begin{align*}
        D_{h}f(x) = \frac{f(x + h) - f(x)}{h}.
    \end{align*}

    The computational error in this approximation is
    \begin{align*}
        D_{h}f(x) - f'(x).
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a function $f: \R \times \R \to \R$ given by $f(x, y) = xy$. If the approximation error for $x$ and $y$ is $\delta_x$ and $\delta_y$ respectively, then
    \begin{align*}
        \hat{x} &= x + \delta_x, \\
        \hat{y} &= y + \delta_y.
    \end{align*}

    Therefore, the propagated error is simply
    \begin{align*}
        f\left(\hat{x}, \hat{y}\right) - f(x, y) &= x\delta_y + y\delta_x + \delta_x\delta_y,
    \end{align*}
    and so the relative propagated error is
    \begin{align*}
        \frac{f\left(\hat{x}, \hat{y}\right) - f(x, y)}{f(x, y)} = \frac{x\delta_y + y\delta_x + \delta_x\delta_y}{xy} = \frac{\delta_x}{x} + \frac{\delta_y}{y} + \frac{\delta_x\delta_y}{xy}.
    \end{align*}
    Under the reasonable assumption that $\delta_x \ll x$ and $\delta_y \ll y$, the term $\frac{\delta_x\delta_y}{xy}$ is negligible, and so the relative error of $xy$ is roughly $R(x) + R(y)$.
\end{exmp}

\section{Algorithms and Convergence}

\begin{defn}
    A mathematical problem is \emph{well-posed} or \emph{stable} when a solution \emph{exists}, is \emph{unique}, and is \emph{continuous} with respect to the input.
\end{defn}

\begin{defn}
    Given a function $f$, the \emph{condition number}, or \emph{conditioning number}, is the supremum of the ratio of the relative change in output to the relative change in input.
\end{defn}

\begin{exmp}
    Let $f: \R \to \R$ be a continuous function. Then the condition number, denoted by $K_f(x)$, is
    \begin{align*}
        K_f(x) = \lim_{\varepsilon\to 0}\sup_{\abs{\delta_x} \leq \varepsilon}\abs{\frac{\left[f(x') - f(x)\right]/f(x)}{\left(x'-x\right)/x}} = \lim_{x' \to x}\abs{\frac{\left[f(x') - f(x)\right]/f(x)}{\left(x'-x\right)/x}} = \abs{\frac{x}{f(x)}f'(x)}.
    \end{align*}
    Note that changing the limit of the supremum to just a limit is only possible because $f$ is continuous.
\end{exmp}

\begin{defn}
    Now consider $f: \R^d \to R$ where $f$ is $C^2$. The absolute change in the output can be approximated at the first order as
    \begin{align*}
        \delta_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\delta_{x_i} = \left\langle \nabla f(x), \delta_{x} \right\rangle.
    \end{align*}
    The relative change is then
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{\delta_{x_i}}{f(x)}.
    \end{align*}
    Let $\varepsilon_{x_i} = \frac{x_i'-x_i}{x_i}$, then $\delta_{x_i} = \varepsilon_{x_i}x_i$, so
    \begin{align*}
        \varepsilon_{y} \approx \sum_{i=1}^{d}\frac{\partial f}{\partial x_i}(x)\frac{x_i}{f(x)}\varepsilon_{x_i}.
    \end{align*}
\end{defn}

\begin{thm}\label{matrix-condition-number}
    Let $A$ be a non-singular matrix, and $\norm{\cdot}$ be an induced norm. Then
    \begin{align*}
        \kappa(A) = \norm{A}\norm{A^{-1}}.
    \end{align*}
\end{thm}

\begin{proof}
    By definition, the conditioning number at $b$ is
    \begin{align*}
        \sup_{\delta b \neq 0}\frac{\norm{A^{-1}(b + \delta b) - A^{-1}b}}{\norm{A^{-1}b}}\frac{\norm{b}}{\norm{\delta b}}.
    \end{align*}
    Notice that
    \begin{align*}
        \frac{\norm{A^{-1}(b + \delta b) - A^{-1}b}}{\norm{A^{-1}b}} = \frac{\norm{A^{-1}\delta b}}{\norm{A^{-1}b}}.
    \end{align*}
    Therefore, the supremum of this across all $b$ is
    \begin{align*}
        \kappa(A) = \sup_{b \neq 0,\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{A^{-1}b}}\frac{\norm{b}}{\norm{\delta b}}.
    \end{align*}
    Let $c = A^{-1}b$, then we can separate the supremums over $c$ and $\delta b$ to obtain
    \begin{align*}
        \kappa(A) = \sup_{c \neq 0,\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{c}}\frac{\norm{Ac}}{\norm{\delta b}} = \left[\sup_{c \neq 0}\frac{\norm{Ac}}{\norm{c}}\right]\left[\sup_{\delta b \neq 0}\frac{\norm{A^{-1}\delta b}}{\norm{\delta b}}\right].
    \end{align*}
    Then by Theorem \ref{induced-norm} we have
    \begin{align*}
        \kappa(A) = \norm{A}\norm{A^{-1}}.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider $f(x_1, x_2) = x_1 - x_2$. Then
    \begin{align*}
        K_f(x_1, x_2) = \abs{\frac{x_1}{x_1 - x_2}} + \abs{\frac{x_2}{x_1 - x_2}}.
    \end{align*}
\end{exmp}

\begin{rmk}
    Since $\norm{I} \geq 1$, it follows that $\kappa(A) = \norm{A}\norm{A^{-1}} = \norm{I} \geq 1$.

    If $U$ is unitary, it's condition number under the $\ell_2$ induced norm is one, since $\norm{U}_{2} = \sqrt{\rho(U^*U)} = \sqrt{\rho(I)} = 1$. If $\kappa(U) = 1$, then $U$ is unitary.
\end{rmk}

\begin{defn}
    Let $f(x)$ be a function with condition number $K_f$, and let $x_y = g(x_n)$ be an iterative algorithm, where $K_n(y_n)$ is the condition number of $g(x_n)$. Then we define
    \begin{align*}
        \hat{K}(y) = \sup_{n \leq N}K_n(y_n),
    \end{align*}
    and say that an algorithm is \emph{numerically stable} when
    \begin{align*}
        \hat{K}(y) \lesssim 2K_f.
    \end{align*}
\end{defn}

\begin{defn}
    The transformation of an ill-posed problem into a well-posed one is \emph{regularization}.
\end{defn}

\begin{exmp}
    Consider a linear system $Ax = b$, where $\det A = 0$. If $b$ is in the null space of $A$, then there are infinite solutions, and otherwise there are no solutions. Therefore it is not a well-posed problem. However, if we consider
    \begin{align*}
        \min_{x}\norm{Ax - b},
    \end{align*}
    this clearly always has a solution. Furthermore, when a solution exists to $Ax - b$ it will coincide.
\end{exmp}

\begin{defn}
    Consider functions $f(h)$ and $g(h)$ such that $g(h)$ goes to zero as $h$ does. We use Landau big-$O$ notation and say that
    \begin{align*}
        f(h) = L + O(g(h))
    \end{align*}
    if there exists $h_0$ and $C$ such that $\abs{f(h) - L} < Cg(h)$ for all $\abs{h} < h_0$.
\end{defn}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. We say that it is a \emph{$p$th order approximation} when
    \begin{align*}
        \abs{f_n(x_n) - f(x)} = O\left(\frac{1}{n^p}\right).
    \end{align*}
\end{defn}

\begin{rmk}
    When we increase $n$ (the number of iterations used) by a factor of ten, a $p$th order approximation will gain $p$ significant figures.
\end{rmk}

\begin{defn}
    Consider an iterative algorithm $f_n(x_n)$ to compute a function $f(x)$. If
    \begin{align*}
        \lim_{n\to\infty}\frac{\abs{f_{n+1}(x_{n+1}) - f(x)}}{\abs{f_{n}(x_{n}) - f(x)}^{p}} = \mu
    \end{align*}
    for some $\mu > 0$, we say that $f_n$ converges with rate of order $p$.
\end{defn}

\begin{rmk}
    When an algorithm converges with a rate of order $p$, we say that we have
    \begin{itemize}
        \item \emph{linear convergence} if $p = 1$ and $\mu < 1$,
        \item \emph{quadratic convergence} if $p = 2$,
        \item \emph{cubic convergence} if $p = 3$.
    \end{itemize}
\end{rmk}

\section{Matrix Factorization}

\begin{thm}
    For any $n \times n$ matrix $A$ and any $\varepsilon > 0$, there exists an invertible matrix $S$ and diagonal matrix $D$, and arbitrary matrix $E$ such that $\norm{E}_{F} < \varepsilon$, and
    \begin{align*}
        A = S\left(D + E\right)S^{-1}.
    \end{align*}
\end{thm}

\begin{proof}
    Let $A = UTU^{*}$ be a Schur decomposition of $A$. For any $\delta > 0$, notice that
    \begin{align*}
        \left[\begin{pmatrix}
            \delta^{-1} & 0 & \cdots & 0 \\
            0 & \delta^{-2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{-n}
        \end{pmatrix}T\begin{pmatrix}
            \delta^{1} & 0 & \cdots & 0 \\
            0 & \delta^{2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{n}
        \end{pmatrix}\right]_{ij} = \delta_{j-i}T_{ij}.
    \end{align*}
    Therefore,
    \begin{align*}
        A = U\begin{pmatrix}
            \delta^{1} & 0 & \cdots & 0 \\
            0 & \delta^{2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{n}
        \end{pmatrix}\begin{pmatrix}
            \delta^{-1} & 0 & \cdots & 0 \\
            0 & \delta^{-2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{-n}
        \end{pmatrix}T\begin{pmatrix}
            \delta^{1} & 0 & \cdots & 0 \\
            0 & \delta^{2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{n}
        \end{pmatrix}\begin{pmatrix}
            \delta^{-1} & 0 & \cdots & 0 \\
            0 & \delta^{-2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \delta^{-n}
        \end{pmatrix}U^{*}.
    \end{align*}
    By choosing $\delta$ small enough, the super-diagonal entries are multiplied by a factor of at least $\delta$, and so can be made arbitrarily small.
\end{proof}

\begin{thm}
    For any matrix $A$ and $\varepsilon > 0$, there exists $E$ such that $\norm{E}_{F} < \varepsilon$ and $A + E$ is diagonalizable.
\end{thm}

\begin{proof}
    Let $UTU^{*}$ be a Schur decomposition of $A$, and consider a diagonal perturbation $D$ such that $T + D$ has distinct eigenvalues, and $\norm{D}_{F} < \varepsilon$. Take $A + UDU^{*}$, which is necessarily diagonalizable since it has distinct eigenvalues. Then $\norm{UDU^{*}}_{F} = \norm{D}_F < \varepsilon$.
\end{proof}

\begin{lemma}\label{lemma:normal-triangular}
    If $T$ is an upper-triangular matrix, then $T$ is normal if and only if $T$ is diagonal.
\end{lemma}

\begin{proof}
    Suppose that $T$ is normal. Then $(TT^{*})_{ij} = (T^*T)_{ij}$, and so
    \begin{align*}
        \sum_{k=1}^{n}T_{ik}\bar{T}_{jk} = \sum_{k=1}^{n}\bar{T}_{ki}T_{kj}.
    \end{align*}
    Since $T$ is upper triangular, we have
    \begin{align*}
        \sum_{k=\max(i, j)}^{n}T_{ik}\bar{T}_{jk} = \sum_{k=1}^{\min(i, j)}\bar{T}_{ki}T_{kj}.
    \end{align*}
    In particular, let $T_{i}$ be the $i$th column of $T$, and then taking $1 = i = j$ in the above equations implies $\abs{T_{11}}^2 = \norm{T_{1}}_2^2$, so $T_{1j} = 0$ for $j > 1$. By induction, it follows that $T$ is diagonal.
\end{proof}

\begin{lemma}\label{lemma:normality-unitary-similarity}
    Matrix normality is preserved by unitary similarity.
\end{lemma}

\begin{thm}\label{thm:spectral-normal}
    Consider a matrix $A$. The following are equivalent:
    \begin{itemize}
        \item $A$ is normal,
        \item $A$ is unitarily diagonalizable,
        \item $\norm{A}_F^2 = \sum_{\lambda \in \sigma(A)}\abs{\lambda}^2$.
    \end{itemize}
\end{thm}

\begin{proof}
    Suppose that $A$ is unitarily diagonalizable, so $A = UDU^{*}$. Since diagonal matrices commute, we know $D$ is normal, and so by Lemma \ref{lemma:normality-unitary-similarity} it follows $A$ is normal. Furthermore,
    \begin{align*}
        \norm{A}_F^2 = \norm{UDU^{*}}_F^{2} = \norm{D}_F^2 = \sum_{i=1}^{n}\abs{\lambda_i}^2.
    \end{align*}

    Next, instead suppose that $A$ is normal, and consider a Schur decomposition $A = UTU^{*}$. Since $A$ is normal, $T$ is normal, and so must be diagonal by Lemma \ref{lemma:normal-triangular}. Therefore, $A$ is unitarily diagonalizable.

    Finally, suppose $A$ satisfies $\norm{A}_F^2 = \sum_{\lambda}\abs{\lambda}^2$. Let $A = UTU^{*}$ be a Schur decomposition. Then $\norm{A}_F^2 = \norm{T}_F^2$. It follows that $T_{ij} = 0$ for $i < j$ or else we would have $\norm{T}_F^2 > \sum_{\lambda}\abs{\lambda}^2$. Therefore, $T$ is diagonal, and so $A$ is unitarily diagonalizable.
\end{proof}

\begin{cor}\label{thm:spectral-hermitian}
    A matrix $A$ is Hermitian if and only if $A$ is unitarily diagonalizable and $\sigma(A) \subseteq \R$.
\end{cor}

\begin{proof}
    Since $A$ is Hermitian, $AA^{*} = A^2 = A^{*}A$ so it is normal. Therefore, by the Spectral Theorem for normal matrices \ref{thm:spectral-normal} it is unitarily diagonalizable as $A = UDU^{*}$. Since $A$ is Hermitian, we must have $D = D^{*}$ and so $D$ has only real entries.

    If $A$ is unitarily diagonalizable, it is normal by the Spectral Theorem \ref{thm:spectral-normal}. If it furthermore has real eigenvalues, then $D = D^{*}$ and so $A$ is Hermitian.
\end{proof}

\begin{cor}
    A real matrix $A$ is symmetric if and only if it is orthogonally diagonalizable.
\end{cor}

\begin{proof}
    If it is orthogonally diagonalizable, it must be symmetric.

    If it is symmetric, it is Hermitian and therefore by Corollary \ref{thm:spectral-hermitian} has real eigenvalues. Therefore $A$ has a real Schur decomposition $A = QTQ^{\transpose}$. But then $A$ is normal so $T$ is normal, and so by Lemma \ref{lemma:normal-triangular} we know $T$ is diagonal. It follows $QTQ^{\transpose}$ is an orthogonal diagonalization of $A$.
\end{proof}

\begin{cor}\label{cor:skew-hermitian-imaginary-eigenvalues}
    A matrix $B$ is skew-Hermitian if and only if the eigenvalues of $B$ are purely imaginary.
\end{cor}

\begin{proof}
    Suppose $B$ is skew-Hermitian, so $BB^{*} = -B^2 = B^*B$, and so by the Spectral Theorem \ref{thm:spectral-normal} we find that $B = UDU^{*}$. Therefore, $D$ is also skew-Hermitian, and so $B$ must have purely imaginary eigenvalues.

    Conversely, suppose $B$ has purely imaginary eigenvalues. Consider its Schur decomposition $B = QTQ^{*}$, so $T = Q^*BQ$. Then $T^* = Q^*(-B)Q = -T$, so $T$ is skew-Hermitian, and so $B^* = Q^*T^*Q = -Q^*TQ = -B$.
\end{proof}

\begin{thm}
    A matrix $A$ is unitary if and only if there exists a skew-Hermitian matrix $B$ such that $A = e^{B}$.
\end{thm}

\begin{proof}
    If $B$ is skew-Hermitian we know it is normal, and therefore unitarily diagonalizable as $B = UDU^{*}$. Therefore, $A = \exp(B) = U\exp(D)U^{*}$, where $D_{kk} = \theta_k i$ for some real $\theta_k$. Therefore, $\exp(D)$ is unitary, and so $A$ is as well.

    Suppose instead $A$ is unitary, so $A$ is normal and therefore unitarily diagonalizable as $A = UDU^{*}$. It follows that $D$ must be unitary, and so $\abs{D_{kk}} = 1$. Therefore, $D_{kk} = \exp(\theta_k i)$ for real $\theta_k \in \R$. It follows that $A = U\exp(B)U^{*}$, where $B$ is diagonal and $B_{kk} = \theta_k i$. Then $B$ is skew-Hermitian by Corollary \ref{cor:skew-hermitian-imaginary-eigenvalues}, and so $A = \exp(UBU^{*})$, and $UBU^{*}$ is skew-Hermitian since $B$ is.
\end{proof}

\begin{defn}
    LU composition
\end{defn}

\begin{defn}
    An $n \times n$ matrix $A$ is said to be \emph{diagonally dominant} if, for all $1 \leq i \leq n$,
    \begin{align*}
        \abs{A_{ii}} \geq \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
    $A$ is furthermore \emph{strictly} diagonally dominant if, for all $1 \leq i \leq n$,
    \begin{align*}
        \abs{A_{ii}} > \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
\end{defn}

\begin{thm}
    Positive definite $\iff$ $\forall k A_k$ is positive definite.

    Positive definite $\iff$ $\forall k \det A_k > 0$ \emph{and} $A$ is Hermitian.

    Positive definite $\iff$ $A$ is Hermitian and Gaussian elimination can be performed without permutations.
\end{thm}

\begin{thm}{Cholesky Factorization}\label{cholesky-factorization}\proofbreak
    Let $A \in M_{n \times n}(\C)$ be a positive definite matrix. There exists a unique lower triangular matrix $L \in M_{n \times n}(\C)$ such that $L_{ii} > 0$ such that $A = LL^{*}$.
\end{thm}

\begin{proof}
    We will proceed by induction on $n$. In the base case $n=1$, we simply have $A = [a]$, and so for $L = [l]$, $LL^{*} = l\overline{l}$, and since $l_{ii}$ must be real we have $l = \sqrt{a}$. Since $A$ is positive definite, by \ref{positive-semidefinite-criteria} it follows that $a > 0$ and so $\sqrt{a} \in \R$.

    Assume that a unique decomposition exists for all $0 \leq n < k$ for some $k$. Consider any positive definite matrix $A \in M_{k \times k}(\C)$. We can write
    \[
        A = \begin{pmatrix}
            A_{k-1} & b \\
            \overline{b} & a_{kk}
        \end{pmatrix}
    \]
\end{proof}

\begin{defn}
    A \emph{band matrix} is a particular type of sparse matrix in which the non-zero entries occur only within $p$ diagonals above the major diagonal, and $q$ diagonals above. Formally, $A \in M_{n \times n}$ is a \emph{band matrix} if there exists \emph{unique} $p, q \in \{1, \ldots, n\}$ such that $A_{ij}$ is zero whenever $j - i \geq p$ or $i - j \geq q$.
\end{defn}

\begin{rmk}
    Gaussian elimination is always successful as long as a pivot can be found.
\end{rmk}

\begin{defn}
    In \emph{partial pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        i_{k} = \argmax_{k\leq i\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, and then proceed as normal. We can construct a permutation matrix $P$ such that $PA = LU$.
\end{defn}

\begin{defn}
    In \emph{complete pivoting}, at step $k$ of Gaussian elimination, we choose
    \begin{align*}
        (i_{k}, j_{k}) = \argmax_{k\leq i, j\leq n}\abs{a_{ik}^{(k)}}
    \end{align*}
    as the pivot. If $i_k > k$, we swap rows $k$ and $i_k$, if $j_k > k$, we swap columns $k$ and $j_k$, and then proceed as normal. We can construct permutation matrices $P$ and $Q$ such that $PAQ = LU$.
\end{defn}

\begin{rmk}
    While complete pivoting may be theoretically superior, partial pivoting yields similar results and so is preferred in practice.
\end{rmk}

\begin{thm}
    Let $A \in M_{n \times n}(\C)$ be Hermitian. There exists a unitary matrix $U$ such that $A = UDU^{*}$, where
    \begin{align*}
        D = \begin{pmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n \\
        \end{pmatrix}
    \end{align*}
    and $\lambda_1, \ldots, \lambda_n$ are the eigenvalues (with multiplicity) of $A$.
\end{thm}

\begin{lemma}\label{singular-non-difference-inequality}
    Consider any singular matrix $B$, and a proper (and therefore induced) norm $\norm{\cdot}$, then
    \begin{align*}
        \norm{A^{-1}}\norm{A - B} \geq 1.
    \end{align*}
\end{lemma}

\begin{proof}
    Since $\norm{\cdot}$ is proper by assumption,
    \begin{align*}
        \norm{A^{-1}B - I_n} = \norm{A^{-1}(A-B)} \leq \norm{A^{-1}}\norm{A-B}.
    \end{align*}
    For convenience, let $C = A^{-1}B - I_n$. If we can show $\norm{C} \geq 1$ we are done. For any $x \in F^n$, note that since $C$ is induced we have
    \begin{align*}
        \norm{C} \geq \frac{\norm{Cx}}{\norm{x}} = \frac{\norm{A^{-1}Bx - x}}{\norm{x}} \geq \abs{\frac{\norm{A^{-1}Bx}}{\norm{x}} - \frac{\norm{x}}{\norm{x}}}.
    \end{align*}
    Since $B$ is singular, there exists $x$ such that $Bx = 0$ and $x \neq 0$ so $\norm{x} \neq 0$. Then for $x \in \mathcal{N}(B)$
    \begin{align*}
        \norm{C} \geq \abs{\frac{0}{\norm{x}} - \frac{\norm{x}}{\norm{x}}} = 1.
    \end{align*}

    Alternatively, if we suppose that $\norm{C} < 1$, then $I-C$ is invertible. But $B = A(I-C)$, so $B$ must be invertible.
\end{proof}

\begin{thm}
    The condition number of a matrix $A$ can be characterized by
    \begin{align*}
        \frac{1}{\kappa(A)} = \min\left\{\frac{\norm{A - B}}{\norm{A}}: \det(B) = 0\right\}.
    \end{align*}
\end{thm}

\begin{proof}
    Let $A$ be a non-singular matrix, and $B$ be any singular matrix, and $\norm{\cdot}$ an induced norm. Then by Lemma \ref{singular-non-difference-inequality}
    \begin{align*}
        \frac{\norm{A - B}}{\norm{A}} \geq \frac{1}{\norm{A}\norm{A^{-1}}}.
    \end{align*}
    and since $\norm{A}\norm{A^{-1}} = \kappa(A)$ by Theorem \ref{matrix-condition-number} we find that $1/\kappa(A)$ is at most the infimum of the set.

    Proving that this minimum is actually attained requires further tools from functional analysis, see ``Numerical Linear Algebra'' by W. Kahan for the proof.
\end{proof}

\begin{defn}
    Given a linear system $Ax = b$ with solution $x_0$ and an approximate solution $\tilde{x}$, let the \emph{residual} of $\tilde{x}$ be $r = A\tilde{x} - Ax_0 = A\tilde{x} - b$ and let $e = \tilde{x} - x_0 = A^{-1}r$.
\end{defn}

\begin{prop}
    Suppose $Ax = b$ for some $b \neq \vec{0}$. If $A(x + \Delta x) = b + \Delta B$, and we have a compatible matrix and vector norm, then we can bound $\Delta x$ as follows:
    \begin{align*}
        \frac{1}{\kappa(A)} \leq \frac{\norm{\Delta x}}{\norm{x}} \leq \kappa(A)\frac{\norm{\Delta b}}{\norm{b}}.
    \end{align*}
\end{prop}

\begin{proof}
    Since $\Delta x = A^{-1}\Delta b$ and $b = Ax$, we have $\norm{\Delta x} \leq \norm{A^{-1}}\norm{\Delta b}$, and $\norm{b} \leq \norm{A}\norm{x}$. Therefore, $\norm{x} \geq \norm{b}/\norm{A}$, and so we obtain
    \begin{align*}
        \frac{\norm{\Delta x}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\frac{\norm{\Delta b}}{\norm{b}} = \kappa(A)\frac{\norm{\Delta b}}{\norm{b}}.
    \end{align*}
    The lower bound follows by reversing the roles of $A$ and $A^{-1}$.
\end{proof}

\begin{defn}
    Consider a matrix $A$. The \emph{singular values} of $A$, denoted by $\sigma(A)$, are the square roots of the positive eigenvalues of $A^{*}A$.
\end{defn}

\begin{rmk}
    We know that $\norm{A}_2 = \sqrt{\rho(A^{*}A)}$, in which case $\kappa(A)$ is
    \begin{align*}
        \frac{\max_{s \in \sigma(A)}s}{\min_{s \in \sigma(A)}s}.
    \end{align*}
\end{rmk}

\begin{thm}
    Consider a linear system $Ax = b$ with a induced matrix norm where $A$ is non-singular. If $A$ is perturbed by $\delta A$ such that
    \begin{align*}
        \norm{\delta A} < \frac{1}{\norm{A^{-1}}},
    \end{align*}
    then $A + \delta A$ is non-singular.
\end{thm}

\begin{proof}
    Let $B = A + \delta A$, so by assumption $\norm{B - A}\norm{A^{-1}} < 1$. It follows that $\norm{(B-A)A^{-1}} < 1$, and $(B-A)A^{-1} = I-BA^{-1}$. We therefore have $\norm{I-BA^{-1}} \leq 1$, and so $BA^{-1}$ must be invertible. Notice that
    \begin{align*}
        A^{-1}\left(BA^{-1}\right)^{-1} = B^{-1},
    \end{align*}
    and since we have shown that $A^{-1}$ and $BA^{-1}$ are both invertible, it follows that $B = A + \delta A$ is invertible.
\end{proof}

\begin{defn}
    Consider an $n \times n$ invertible matrix $A$, then the \emph{growth factor} of $A$ is
    \begin{align*}
        \rho = \frac{1}{\norm{A}_{\infty}}\max_{1 \leq i,j,k \leq n} \abs{A_{ij}^{(k)}},
    \end{align*}
    where $A^{(k)}$ is the $k$th iterate obtained via Gaussian elimination.
\end{defn}

\begin{thm}
    Consider the linear system $Ax = b$, where $A$ is an $n \times n$ invertible matrix, and its $LU$ decomposition obtained via Gaussian elimination. Assume presence of numerical error, so
    \begin{align*}
        LU = A + E
    \end{align*}
    for some $n \times n$ matrix $E$. Then
    \begin{align*}
        \frac{\norm{E}_{\infty}}{\norm{A}_{\infty}} \leq n^2\rho\delta,
    \end{align*}
    where $\rho$ is the growth factor of $A$ and $\delta$ is the machine epsilon.
\end{thm}

\begin{rmk}
    Using different pivoting strategies, the growth factor $\rho$ may be controlled to an extent. Using \emph{complete pivoting}, we can bound it as
    \begin{align*}
        \rho \leq (1.8)n^{\ln(n)/4}.
    \end{align*}
\end{rmk}

\section{Fixed-Point Methods}

We will attempt to transform the linear system $Ax = b$ into the form $x = Tx + c$, the so-called ``fixed-point'' form of $x$, since the solutions are the fixed-points of the affine transformation.

Note that for all invertible $n \times n$ matrices $B$, where we have $Ax = b$ for some vector $b$, we can write $Ax = b$ as $Bx + (A - B)x = b$, which in turn may be rewritten as $x = B^{-1}(B-A)x + B^{-1}b = (I_n - B^{-1}A)x + B^{-1}b$.

\begin{defn}
    Select (arbitrary) point $x^{(0)} \in \R^{d}$ and recursively define
    \begin{align*}
        x^{(k+1)} &= Tx^{(k)} + c = \left(I_n - B^{-1}A\right)x^{(k)} + B^{-1}b
    \end{align*}
    which is equivalent to
    \begin{align*}
        B\left(x^{(k+1)} - x^{(k)}\right) = b - Ax^{(k)}.
    \end{align*}
    We call the expression on the right hand side the \emph{residual}. Various \emph{stoppping criterion} for termination of this algorithm are possible, such as when the relative (or absolute) residual falls beneath a chosen threshold.
\end{defn}

\begin{defn}{Jacobi method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{Jacobi method}, we choose $B = D$, and so
    \begin{align*}
        T_{J} = I - B^{-1}A = I - D^{-1}A = I-D^{-1}(D - L - U) = D^{-1}(L + U).
    \end{align*}
    Notice that we then have
    \begin{align*}
        x^{(k + 1)}_{i} = \frac{-\sum_{j\neq i}A_{ij}x^{(k)}_j + b_i}{A_{ii}}.
    \end{align*}
\end{defn}

\begin{defn}{Gauss-Seidel method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{Gauss-Seidel method}, we choose $B = D - L$, and so
    \begin{align*}
        T_{G} = I - \left(D - L\right)^{-1}\left(D - L - U\right) = I - I - \left(D - L\right)^{1}\left(-U\right) = \left(D-L\right)^{-1}U.
    \end{align*}
    Notice that we then have
    \begin{align*}
        x^{(k + 1)}_{i} = \frac{-\sum_{j=1}^{i-1}A_{ij}x^{(k_j+1)} - \sum_{j= i+1}^{n}A_{ij}x^{(k_j)} + b_i}{A_{ii}}.
    \end{align*}
\end{defn}

\begin{rmk}
    In general, the Gauss-Seidel method tends to perform better than the Jacobi method.
\end{rmk}

\begin{thm}\label{fixed-point-convergence}
    Let $T \in M_{n \times n}(\R)$ be invertible, and let $x_0 \in \R^n$. The sequence $x^{(k+1)} = Tx^{(k)} + c$ will converge to a solution for \emph{any} choice of $x_0$ if and only if $\rho(T) < 1$. This solution is unique and does not depend on choice of initial $x^{(0)}$.
\end{thm}

\begin{proof}
    Let $\norm{\cdot}$ be a proper norm. First, we will see that a unique fixed-point always exists and is unique. Since $\rho(T) < 1$, then $I - T$ is invertible and so $(I - T)x = c$ has a unique solution. Now let us turn to convergence.

    ($\implies$) Let $x$ be a solution, and consider the largest eigenvalue in absolute value $\rho(A)$ with associated eigenvector $u$. Then let $x_0 = x - u$, and so $x - x^{(k)} = \rho(A)^{(k)}u$, so $x{(k)}$ converges only if $\rho(A) < 1$.

    ($\impliedby$) First, we will prove that $x - x^{(k)} = T^{k}(x - x_0)$ by induction. In the base case, this is trivially true since $T^0 = I$. Then, assuming $x - x^{(k)} = T^{k}(x - x_0)$, we then have
    \[T\left[x - x^{(k)}\right] = T\left[T^{k}(x - x_0)\right],\]
    and so
    \[Tx + c - Tx^{(k)} - c = T^{k+1}(x - x_0).\]
    Since $x$ is by construction a fixed point of $f(v) = Tv + c$, it follows that $Tx + c = x$, and by definition $Tx^{(k)} + c = x^{(k+1)}$, so we have
    \begin{align*}
        x - x^{(k+1)} = T^{k+1}(x - x_0).
    \end{align*}
    Now that we have shown that $x - x^{(k)} = T^{k}(x - x_0)$ for $k \geq 0$, it follows since the norm is proper that $\norm{T^{k}(x - x_0)} \leq \norm{T^k}\norm{x-x_0}$. We know this converges to zero as $k$ goes to infinity since $\rho(T) < 1$, and so $x^{(k)}$ converges to $x$.
\end{proof}

\begin{prop}
    Let $\norm{\cdot}$ be a induced norm such that $\norm{T} < 1$ then $x^{(k)}$ converges and $\norm{x - x^{k}} \leq \norm{T}^{k}\norm{x - x_0}$ and $\norm{x - x^{k}} \leq \frac{\norm{T}^{k}}{1 - \norm{T}}\norm{x^{(1)} - x^{0}}$.
\end{prop}

\begin{thm}
    Let $A$ be an $n \times n$ matrix. If $A$ is strictly diagonally dominant, then the Jacobi and Gauss-Seidel methods converges for all $x^{(0)} \in \R^n$, and furthermore
    \begin{align*}
        \norm{T_{G}}_{\infty} \leq \norm{T_{J}}_{\infty} < 1.
    \end{align*}
\end{thm}

\begin{proof}
    Recall $T_{J}$ is defined to be $D^{-1}(L + U)$, so
    \begin{align*}
        \norm{T_{J}}_{\infty} = \max_{1\leq i \leq n}\frac{1}{D_{ii}}\sum_{j=1}^{n}(L + U)_{ji} = \max_{1\leq i \leq n}\frac{1}{A_{ii}}\sum_{j \neq i}A_{ji}.
    \end{align*}
    Since $A$ is strictly diagonally dominant, $A_{ii}$ is strictly greater than the sum for all $i$, and so
    \begin{align*}
        \norm{T_{J}}_{\infty} < 1.
    \end{align*}
\end{proof}

\begin{defn}
    Let $A$ be an $n \times n$ matrix. We say that $A$ is \emph{irreducible} when there \emph{does not} exist a permutation matrix $P$ such that $\transposeof{P}AP$ is a block upper triangular matrix
    \begin{align*}
        \left[\begin{array}{c|c}
            \tilde{A}_{11} & \tilde{A}_{12} \\
            \hline
            \raisebox{0pt}[12pt][0pt]{\scalebox{1.5}{$0$}} & \tilde{A}_{22}
        \end{array}\right]
    \end{align*}
    such that $\tilde{A}_{11} \in M_{p\times p}$ and $\tilde{A}_{22} \in M_{q \times q}$ where $p + q = n$, and $n > p, q \geq 1$. Note that $\tilde{A}_{ij}$ may be anything -- in particular they may contain zero entries.
\end{defn}

\begin{prop}
    An $n \times n$ matrix $A$ is irreducible if and only if the directed graph $G(A)$, of which $A$ is the adjacency matrix, is strongly connected.
\end{prop}

\begin{proof}
    First, note that $G(\transposeof{P}AP)$ is isomorphic to $G(A)$ --- that is, the two matrices are the adjacency matrices of the same graphs, up to order of vertices. To see this, let $\sigma$ be the permutation induced by $P$ --- that is, $P_{ij} = \delta_{i\sigma(j)}$. Then
    \begin{align*}
        AP_{\sigma_{i}j} &= \sum_{k=1}^{n}A_{\sigma(i)k}P(kj) = A_{\sigma(i)\sigma(j)}, \\
        \transposeof{P}AP_{ij} &= \sum_{k=1}^{n}\transposeof{P}_{ik}AP_{kj} = AP_{\sigma(i)j}.
    \end{align*}
    Therefore, $\transposeof{P}AP_{ij} = A_{\sigma(i)\sigma(j)}$, so $G(A)$ and $G(\transposeof{P}AP)$ are isomorphic since permutations are bijections. We know that if $A$ is isomorphic to $B$, then $A$ is strongly connected if and only if $B$ is strongly connected.

    To construct the adjacency matrix, we must have an index on the vertices $V_i$. Then we may consider a path from $V_i$ to $V_j$. Such a path must correspond to a sequence $a_1, a_2, \ldots, a_k$ such that $a_1 = n$, $a_k = 1$, and $\transposeof{P}AP_{a_{\ell}a_{\ell+1}} = 1$ for all $\ell \in \{1, \ldots, k-1\}$. Since $\transposeof{P}AP_{a_{\ell}a_{\ell+1}} = 0$ if $a_{\ell} > p$ and $a_{\ell+1} \leq n-q = p$, it follows that $a_{\ell+1} > p$ whenever $a_{\ell} > p$. Since $a_1 = n > p$ it would then follow that $a_k > p$. Therefore, no such path may exist if $\transposeof{P}AP$ is block upper triangular, and so if \emph{any} permutation matrix $P$ makes $\transposeof{P}AP$ block upper triangular then $G(A)$ cannot be strongly connected.

    Assume that $G(A)$ is not strongly connected, so there exists vertices $\alpha$ and $\beta$ such that there is no path from $\alpha$ to $\beta$. Let $V_{\alpha}$ be the set of all vertices $v$ such that there is a path from $\alpha$ to $v$, and let $V_{\beta}$ be the set of all vertices $v$ such that there is a path from $v$ to $\beta$. Note that $V_{\alpha}$ and $V_{\beta}$ must be disjoint by assumption. Let $p = \abs{V_{\beta}}$, and consider a permutation $\sigma$ which sends $V_{\beta}$ to $\{1, \ldots, p\}$ and $V_{\alpha}$ to $\{p+1, \ldots, n\}$. Notice that ${\transposeof{P_{\sigma}}AP_{\sigma}}_{ij} = 0$ whenever $p+1 \leq i \leq n$ and $1 \leq j \leq p$. Therefore, there exists $\sigma$ such that $\transposeof{P_{\sigma}}AP_{\sigma}$ is block upper triangular whenever $G(A)$ is not strongly connected.
\end{proof}

\begin{defn}
    An $n \times n$ matrix is \emph{irreducibly diagonally dominant} when it is irreducible, (weakly) diagonally dominant, and additionally there exists $i$ such that
    \begin{align*}
        \abs{A_{ii}} > \sum_{j\neq i}\abs{A_{ij}}.
    \end{align*}
\end{defn}

\begin{prop}
    Let $A$ be an $n \times n$ matrix. If $A$ is irreducibly diagonally dominant, then the Jacobi method converges for all $x^{(0)} \in \R^n$.
\end{prop}

\begin{proof}
    If $A$ is diagonally dominant, then $\norm{T_j}_{\infty} \leq 1$. If $\norm{T_j} < 1$, then we are done. Otherwise, we must have strictly inequality in some row $p$, and so we know that
    \begin{align*}
        \sum_{j}\abs{T_{pj}} = \frac{1}{\abs{A_{pp}}}\left[\left(\sum_{j}\abs{A_{pj}}\right) - \abs{A_{pp}}\right] < 1 = \norm{A}_{\infty}.
    \end{align*}
    Therefore, by Corollary \ref{cor:irreducible-spectral-norm-gap} it follows that $\rho(T_j) < \norm{T_j} = 1$, and so the Jacobi method converges.
\end{proof}

\begin{defn}{SOR method}\proofbreak
    Consider the decomposition
    \begin{align*}
        A = -L + D - U,
    \end{align*}
    where $-L$, $D$, $-U$ are lower triangular, diagonal, and upper triangular matrices respectively. In the \emph{SOR method} (successive over-relaxtion), we choose
    \begin{align*}
        B(\omega) = \frac{1}{\omega}\left(D - \omega L\right).
    \end{align*}
    \begin{align*}
        T(\omega) = I - \left(\frac{1}{\omega}\left(D - \omega L\right)\right)^{-1}\left(D - L - U\right) = \left(D - \omega L\right)^{-1}\left[(1-\omega)D + \omega U\right].
    \end{align*}
\end{defn}

\begin{thm}{Kahan's Theorem}\label{kahans-theorem}\proofbreak
    If $\det D \neq 0$, then
    \begin{align*}
        \rho\left(T(\omega)\right) \geq \abs{\omega - 1}.
    \end{align*}
\end{thm}

\begin{proof}
    \begin{align*}
        \det\left(T(\omega)\right) = \frac{\det\left((1-\omega)D + \omega U\right)}{\det\left(D - \omega L\right)} = \frac{\left(1-\omega\right)^{n}\det\left(D\right)}{\det\left(D\right)} = \left(1 - \omega\right)^{n}.
    \end{align*}
\end{proof}

\begin{thm}{Ostrowski-Reich}\label{ostrowski-reich}\proofbreak
    If $A$ is positive definite and $0 < \omega < 2$, then $p(T(\omega)) < 1$.
\end{thm}

\section{Matrix Normal Forms}

\begin{thm}
    Let $A$ be an invertible $n \times n$ matrix over $\C$. There exists a unitary $n \times n$ matrix $Q$ and an upper triangular $n \times n$ matrix $R$ such that $A = QR$.
\end{thm}

\begin{proof}
    Since $A$ is invertible, its columns are linearly independent by Theorem \ref{row-column-rank-equivalence}. Let $\langle x_1, \ldots, x_n \rangle$ be the basis formed by these columns, and apply the Gram-Schimdt process to produce an orthonormal basis $\langle e_1, \ldots, e_n \rangle $ where $e_i$ is computed as a linearly combination of $x_j$'s for $j \leq i$. Therefore, there exists an upper triangular matrix $U$ such that
    \begin{align*}
        AU =
        \begin{bmatrix}
            \vdots & \vdots & \cdots & \vdots \\
            x_1 & x_2 & \cdots & x_n \\
            \vdots & \vdots & \cdots & \vdots
        \end{bmatrix}U = \begin{bmatrix}
            \vdots & \vdots & \cdots & \vdots \\
            e_1 & e_2 & \cdots & e_n \\
            \vdots & \vdots & \cdots & \vdots
        \end{bmatrix} = Q.
    \end{align*}
    Notice that $U_{ii}$ must be non-zero, or else $e_i$ would be linearly dependent on $e_j$'s for $j < i$, and so $U$ is invertible with inverse $R$ that must also be upper triangular. Since $Q$ must be unitary since its columns form an orthonormal basis, it follows that $A = QR$.
\end{proof}

\begin{rmk}
    Let $A$ be a full-rank $n \times m$ matrix with $n \geq m$, then the $QR$ decomposition still exists. In this case, the Gram-Schimdt process produces a basis for a \emph{linear subspace} of $\C^n$ with dimension $m$.
\end{rmk}

\begin{rmk}
    The $QR$ decomposition of $A$ produces the Cholesky factorization of $A^{*}A$, since $A^{*}A = (QR)^{*}QR = R^{*}Q^{*}QR = R^{*}R$.
\end{rmk}

\begin{thm}{Schur Normal Form}\proofbreak
    Let $A$ be an $n \times n$ matrix over $\C$. There exists a unitary $n \times n$ matrix $Q$ such that $Q^{*}AQ$ is upper triangular.
\end{thm}

\begin{proof}
    We proceed by induction on $n$. In the base case, $A$ is trivially upper triangular so we may take $Q = [1]$. Now assume this holds for some $n-1$, and consider $A$ an $n \times n$ matrix. Since $A$ is over $\C$, it must have an eigenvalue $\lambda_1$ with eigenvector $v_1$ such that $\norm{v_1}_2 = 1$. Complete $v_1$ to an orthonormal basis $\{v_1, \ldots, v_n\}$ for $\C^n$. Then take $U$ to be the matrix whose $i$th column is $v_i$, and so $U$ is a unitary matrix such that
    \begin{align*}
        U^{*}AU = \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{a}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$A_1$}}
        \end{array}\right].
    \end{align*}
    By assumption, there exists unitary $U_1$ such that $U_1^{*}A_1U_1$ is upper triangular. Let
    \begin{align*}
        Q = U\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right].
    \end{align*}
    Note that
    \begin{align*}
        Q^{*}Q = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right]^{*}U^{*}U\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1^{*}$}}
        \end{array}\right]\left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = I_n,
    \end{align*}
    and so $Q$ is unitary. Furthermore,
    \begin{align*}
        Q^{*}AQ = \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right]^{*}
        \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{a}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$A_1$}}
        \end{array}\right]
        \left[\begin{array}{c|c}
            1 & \transposeof{\vec{0}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$U_1$}}
        \end{array}\right] = \left[\begin{array}{c|c}
            \lambda_1 & \transposeof{\vec{b}} \\
            \hline
            \raisebox{-5pt}[0pt][0pt]{$\vec{0}$} & \raisebox{-5pt}[0pt][8pt]{\scalebox{1.5}{$R_1$}}
        \end{array}\right] = R,
    \end{align*}
    which must be upper triangular since $R_1$ is upper triangular by assumption.
\end{proof}

\begin{cor}
    Given a set $\mathscr{F}$ of pairwise commuting matrices, then there exists unitary $U$ which simultaneously upper triangularizes $\mathscr{F}$.
\end{cor}

\begin{cor}
    If $A, B$ commute, there there exists a permutation $\pi \in S_{n}$ such that
    \begin{align*}
        \sigma(A + B) &= \left\{\lambda_{i}(A) + \lambda_{\pi(i)}(B) : i \in 1, \ldots, n\right\} \\
        \sigma(AB) &= \left\{\lambda_{i}(A)\lambda_{\pi(i)}(B) : i \in 1, \ldots, n\right\}.
    \end{align*}
\end{cor}

\begin{thm}{Cayley-Hamilton}\label{thm:cayley-hamilton}\proofbreak
    For any matrix $A$ with characteristic polynomial $p_A(\cdot)$, we have $p_A(A) = 0$.
\end{thm}

\begin{proof}
    Consider the Schur decomposition $A = UTU^{*}$. Then
    \begin{align*}
        p_{A}(A) &= Up_{A}(T)U^{*} = U\left[\prod_{\lambda \in \sigma(A)}(T-\lambda I)\right]U^{*}.
    \end{align*}

    {\color{red}\Large TODO}
\end{proof}

\begin{cor}
    If $A$ is an invertible matrix, then there exists a polynomial $q(t)$ of degree less than $n$ such that $A^{-1} = q(A)$.
\end{cor}

\begin{proof}
    Consider the characteristic polynomial
    \begin{align*}
        p_A(t) = t^n + a_{n-1}t^{n-1} + \cdots + a_1t + a_0.
    \end{align*}
    By Cayley-Hamilton,
    \begin{align*}
        A\left(A^{n-1} + a_{n-1}A^{n-2} + \cdots + a_1I\right) = -a_0I.
    \end{align*}
    Since $a_0 = \pm\det(A)$ and $A$ is invertible we know $a_0 \neq 0$. Therefore,
    \begin{align*}
        q(t) &= \frac{1}{-a_0}\left(t^{n-1} + a_{n-1}t^{n-2} + \cdots + a_1\right).
    \end{align*}
\end{proof}

\begin{proof}
    Let $U$ be unitary such that $A = UT_{A}U^{*}$ and $B = UT_{B}U^{*}$. Then $A + B = U\left(T_A + T_B\right)U^{*}$ and $AB = UT_{A}U^{*}UT_BU^{*} = UT_AT_BU^{*}$.
\end{proof}

\begin{thm}{Power method}\proofbreak
    Consider a diagonalizable matrix $A$. Choose vector $b_0$, and define
    \begin{align*}
        b_{k+1} &= \frac{Ab_k}{\norm{Ab_k}} \\
        \mu_k &= \frac{\langle x_k, Ax_k \rangle}{\langle x_k, x_k\rangle}.
    \end{align*}
    Then $\mu_k$ tends to $\lambda_n$, the \emph{largest} eigenvalue of $A$ in absolute value, as long as $b_0$ is not orthogonal to $\lambda_n$ and $\lambda_n$ is \emph{strictly} greater in absolute value.
\end{thm}

\begin{thm}{Inverse power method}\proofbreak
    Consider a diagonalizable matrix $A$, and $q \in \C$. Note that $A - qI$ has eigenvalues equal to those of $A$ plus $q$, and $(A - qI)^{-1}$ has eigenvalues equal to $1/(\lambda - q)$ for all eigenvalues $\lambda$ of $A$. Applying the power method to $(A-qI)^{-1}$ then produces $\mu$ such that $1/\mu + q$ is the eigenvalue of $A$ closest to $q$.
\end{thm}

\begin{thm}\label{eigen-deflation}
    Let $A$ be a matrix with eigenvalue $\lambda_1$ with associated eigenvector $v_1$, and $x \in \C^n$ be any vector such that $\langle x, v_1 \rangle = 1$. The eigenvalues of the matrix
    \begin{align*}
        A - \lambda_1v_1x^{*}
    \end{align*}
    are $0, \lambda_2 - \lambda_1, \lambda_3 - \lambda_1, \ldots$, such that the associated eigenvectors satisfy
    \begin{align*}
        v_{i\geq 2} = (\lambda_i - \lambda_2)w_i + \lambda_1\langle x, w_i \rangle v_i.
    \end{align*}
\end{thm}

\begin{defn}{Wielandt Deflation}\proofbreak
    \begin{align*}
        x = \frac{1}{\lambda_1(v_1)_{i}}\transposeof{\begin{bmatrix}
            A_{i1} & A_{i2} & \cdots & A_{in}
        \end{bmatrix}}.
    \end{align*}
    Since $\langle x, v_1 \rangle = 1$, we can use this $x$ for Theorem \ref{eigen-deflation}. In particular, this $x$ makes the $i$th column of $B$ into $\vec{0}$, and so $B$ can be ``deflated'' into an $(n-1) \times (n-1)$ matrix that is equivalent for the purposees of finding the remaining eigenvalues.
\end{defn}

\begin{defn}
    Let $w \in \C^n$ such that $\norm{w}_2 = 1$. We define the \emph{Householder transformation} $H_w = I - 2ww^{*}$.
\end{defn}

\begin{lemma}\label{householder-transformation}\proofbreak
    For all $w \in \C^n$ with $\norm{w}_2 = 1$, we have $H_w$ is both Hermitian and unitary (a \emph{reflection} matrix), so $H_w^2 = I$.
\end{lemma}

\begin{proof}
    First, notice that $H_w^{*} = I^* - 2(ww^{*})^{*} = I - 2ww^{*}$. Furthermore, $H_wH_w^{*} = (I - 2ww^{*})(I - 2ww^{*}) = I - 4ww^{*} + 4ww^*ww^{*}$. Notice that $w(w^{*}w)w^{*} = ww^{*}$, so $4ww^{*}ww^{*} - 4ww^{*} = 0$, and so $H_w^{-1} = H_w^{*}$.
\end{proof}

\begin{rmk}
    Since $\norm{w}_2 = 1$, notice that $ww^{*}x$ gives the projection of $x$ onto $w$. Then $H_wx = x - 2ww^{*}x$ is the reflection of $x$ across the hyperplane $w^{\perp}$ (which has unit normal $w$).
\end{rmk}

\begin{lemma}\label{householder-elimination}
    Let $x \in \C^n$. There exists $v \in \C^n$ such that $\norm{v}_2 = 1$ and
    \begin{align*}
        H_vx = \begin{pmatrix}
            \alpha \\ 0 \\ \vdots \\ 0
        \end{pmatrix}
    \end{align*}
    for some $\alpha \in \C$.
\end{lemma}

\begin{proof}
    Since $H_v$ must be unitary, we necessarily have $\abs{\alpha} = \norm{x}_2$. Let $p = \langle v, x \rangle$, then it follows that
    \begin{align*}
        H_vx = x - 2pv = \alpha\transposeof{(1, 0, \ldots, 0)}.
    \end{align*}
    Since $\norm{v}_2 = 1$, taking the standard complex inner product by $v$ on the left we obtain
    \begin{align*}
        p - 2p = \alpha\overline{v_1},
    \end{align*}
    and so we necessarily have $p = -\alpha\overline{v_1}$.

    Since $x_1 - 2pv_1 = \alpha$, we have $v_1 = \frac{x_1 - \alpha}{2p}$. Then $x_1\alpha - 2pv_1\alpha$ is $x_1\alpha + 2\alpha\overline{v_1}v_1\alpha = x_1\alpha + 2\abs{p}^2$, which must equal $\alpha^2$, and so $\alpha^2 - \alpha x_1 = 2\abs{p}^2$. Therefore,
    \begin{align*}
        \abs{p} &= \sqrt{\frac{\alpha^2 - \alpha x_1}{2}}.
    \end{align*}

    Finally, $x_i - 2pv_i = 0$ implies that $v_i = \frac{x_i}{2p}$ for all $i > 1$.
\end{proof}

\begin{thm}{Householder Method}\proofbreak
    Define sequence of Householder transformations $H_i = I_n - 2w_iw_i^{*}$, where $w_i$ is given by the constructive proof in Lemma \ref{householder-elimination} to reduce the first column of $H_{i-1}\cdots H_1A$, then
    \begin{align*}
        H_nH_{n-1}\cdots H_1A = R.
    \end{align*}
    Since $H_i$ is unitary by Lemma \ref{householder-transformation}, it follows that $Q = H_nH_{n-1}\cdots H_1$ is unitary, and so $A = Q^{*}R$ is the $QR$ decomposition of $A$.
\end{thm}

\begin{rmk}
    The Householder method or Gram-Schimdt based method for finding the QR decomposition of a $n \times n$ matrix have time complexity $O(n^3)$. Solving a linear system once you have its QR decomposition has time complexity $O(n^2)$.
\end{rmk}

\begin{defn}
    A $n \times n$ matrix is a \emph{Hessenberg matrix} if $A_{ij} = 0$ whenever $i - j > 1$ --- that is, a Hessenberg matrix is almost upper triangular, but may have non-zero entries on the first subdiagonal below the major diagonal. This form may also be called \emph{upper Hessenberg}, in which case \emph{lower Hessenberg} analogously refers to having zeros \emph{above} the first subdiagonal above the diagonal.
\end{defn}

\begin{defn}
    A \emph{Given's rotation matrix} $G_{ij}(\theta)$ is almost an identity matrix, except the $i$ and $j$ entries form a clockwise $\theta$ 2D rotation matrix. That is,
    \begin{align*}
        \begin{pmatrix}
            G_{ii} & G_{ij} \\ G_{ji} & G_{jj}
        \end{pmatrix} &= \begin{pmatrix}
            \cos(\theta) & \sin(\theta) \\ -\sin(\theta) & \cos(\theta)
        \end{pmatrix}.
    \end{align*}
\end{defn}

\begin{rmk}
    Note that $\transposeof{G_{ij}(\theta)} = G_{ij}(-\theta)$. A Given's matrix $G_{ij}(\theta)$ performs a clockwise rotation by $\theta$ in the $i,j$ coordinate plane.
\end{rmk}

\begin{thm}{Hessenberg QR Method}\proofbreak
    Any Hessenberg matrix $A$ can be decomposed into $QR$ by Given's rotations. In particular, we take
    \begin{align*}
        \transposeof{G_{n-1}}\transposeof{G_{n-2}}\cdots \transposeof{G_{2}}\transposeof{G_{1}}A &= R,
    \end{align*}
    where $R$ is upper triangular, and
    \begin{align*}
        x^{(j)} &= \transposeof{G_{j-1}}\cdots\transposeof{G_{1}}A_{j} \\
        \theta_j &= \arcsin\left(\frac{-x{(j)}_{j+1}}{x{(j)}_{j}^2+x{(j)}_{j+1}^2}\right)\\
        G_{j} &= G_{j,j-1}(\theta_j).
    \end{align*}
\end{thm}

\begin{proof}
    Since $x$ is the first column of $\transposeof{G_{j-1}}\cdots\transposeof{G_{1}}A_{1}$, when we take $\theta_j$ as defined above, it follows that
    \begin{align*}
        \left(\transposeof{G_{j}}x\right)_k &= \begin{dcases}
            \cos(\theta)x_j - \sin(\theta)x_{j+1}, &k=j \\
            \sin(\theta)x_j + \cos(\theta)x_{j+1}, &k=j+1 \\
            x_k, &k\neq j,j+1.
        \end{dcases}
    \end{align*}
    By construction of $\theta_j$, $\sin(\theta)x_j + \cos(\theta)x_{j+1} = 0$, and so left-multiplying by $\transposeof{G_{j}}$ zeros out the $j+1$ entry of the $j$th column of $\transposeof{G_{j-1}}\cdots\transposeof{G_1}A$. Since only the $j$ and $j+1$ entries of any columns are changed, and these entries are already zeroed out for columns to the left, each Given's rotation matrix eliminates a single entry on the subdiagonal. In the end, the entire subdiagonal is zeroed out while leaving the rest of the lower triangle unchanged, and so $R$ is upper-triangular. Furthermore, since every Given's rotation matrix is Hermitian, we have unitary $Q = G_1G_2\cdots G_{n-1}$.
\end{proof}

\begin{thm}
    Any square matix $A$ can be reduced to a Hessenberg matrix $A_H$ using Householder transformations. Let $H_i$ be the Householder reflection of $w_i = \transposeof{(\underbrace{0, \ldots, 0}_{i}, \tilde{w}_i)}$, and then
    \begin{align*}
        A_H = \left(H_1 \cdots H_n\right)^{*}A H_1 \cdots H_n = H_n \cdots H_1 A H_1 \cdots H_n.
    \end{align*}
\end{thm}

\begin{defn}{The QR Method}\proofbreak
    A numerically stable but computational somewhat more expensive approach to obtaining the $QR$ decomposition of a matrix $A$ is to first reduce $A$ to an upper triangular matrix $H_0$ via Householder reduction, and then iteratively compute $H_i = R_{i-1}Q_{i-1}$ where $Q_{i-1}R_{i-1}$ is the Hessenberg $QR$ decomposition of $H_{i-1}$.

    Note that
    \begin{align*}
        H_k &= R_{k-1}Q_{k-1} = Q_{k-1}^{*}Q_{k-1}R_{k-1}Q_{k-1} = Q_{k-1}^{*}H_{k-1}Q_{k-1} \\
        &= Q_{k-1}^{*}Q_{k-2}^{*}\cdots Q_{1}^{*}H_0Q_1 \cdots Q_{k-2}Q_{k-1}.
    \end{align*}

    $H_k$ will converge to a Schur normal form of $A$, in particular $H_k$ will be an upper triangular matrix with the eigenvalues on the diagonal.
\end{defn}

\begin{prop}\label{hessenberg-qr-swap}
    Let $M = QR$, where $M$ is a Hessenberg matrix, $Q$ is unitary, and $R$ is upper triangular. Then $RQ$ si also Hessenberg.
\end{prop}

\begin{defn}
    The shifted QR method is a variant of the above QR method with faster convergence. It starts with choosing a scalar $\mu_k$ close to a known eigenvalue of $A$, and taking $A_{k} - \mu_kI = Q_{k}R_{k}$, $A_{k+1} = \mu_kI + R_{k}Q_{k}$
\end{defn}

\section{Krylov Subspaces}

\begin{defn}
    For a $n \times n$ matrix $A$, the $m$th \emph{Krylov subspace} of $A$ at $x \neq \vec{0}$ is
    \begin{align*}
        K_m(A, x) = \spn\left\{x, Ax, A^2x, \ldots, A^{m-1}x\right\}.
    \end{align*}
\end{defn}

\begin{rmk}
    Note that the dimension of $K_m(A, x)$ is at least $1$, and at most $\min(n, m, 1 + \rank A)$.
\end{rmk}

\begin{rmk}
    For sparse matrices $A$, $A^{k}x$ may be computed relatively efficiently.
\end{rmk}

\begin{lemma}
    Let $E_i$ for $i \in \N$ be a family of vector subspaces of $F^n$ where there exsts an $n \times n$ matrix $A$ such that $E_{i+1} = \spn\left(E_i \union \left(AE_i\right)\right)$. There exists $1 \leq p \neq n$ such that $E_{p'} = E_p$ for all $p' \geq p$.
\end{lemma}

\begin{proof}
    Note that $E_i$ is necessarily a subspace of $E_{i+1}$. If $E_{i+1} = E_i$ for any $i$, we are done. Since the dimension of $E_{i+1}$ is at least the dimension of $E_i$, it follows that $E_{n+1} = E_n$ since $E_i \subseteq \C^n$. Therefore, the set of $k \in \N$ such that $E_{k+1} = E_k$ is non-empty, and so by the well-ordering principle we can take $p = \min\left\{k \in \N : E_{k+1} = E_k\right\}$.
\end{proof}

\begin{defn}{Arnoldi method}\proofbreak
    The \emph{Arnoldi method} is an way to find eigenvalues that is particularly efficient for sparse matrices. We begin by finding an orthonormal basis $Q_n$ for the Krylov subspace $K_n(A, x)$ for unit vector $x$, perhaps via the Gram-Schimdt process. We then form the Hessenberg matrix $H_n = Q_n^{*}AQ_n$, the eigenvalues of which will generally approximate the largest eigenvalues of $A$.
\end{defn}

\section{Singular Value Decomposition}

\begin{defn}
    Let $A$ be an $m \times n$ matrix. A \emph{singular value decomposition} (SVD) of $A$ is a factorization $A = U\Sigma V^{*}$, where $U$ is an $m \times m$ unitary matrix, $\Sigma$ is an $m \times n$ diagonal matrix of non-negative real numbers, and $V$ is a $n \times n$ unitary matrix.
\end{defn}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \draw[thick] (0, 0) ellipse (1cm and 1cm);
        \draw[->, thick, -{Stealth[]}, red] (0, 0) -- (0, 1);
        \draw[->, thick, -{Stealth[]}, blue] (0, 0) -- (1, 0);

        \draw[thick, rotate around={35:(0,4)}] (0, 4) ellipse (1cm and 1cm);
        \draw[->, thick, -{Stealth[]}, red, rotate around={35:(0,4)}] (0, 4) -- +(0, 1);
        \draw[->, thick, -{Stealth[]}, blue, rotate around={35:(0,4)}] (0, 4) -- +(1, 0);

        \draw[thick, rotate around={35:(4,4)}] (4, 4) ellipse (2cm and 1cm);
        \draw[->, thick, -{Stealth[]}, red, rotate around={35:(4,4)}] (4, 4) -- +(0, 1);
        \draw[->, thick, -{Stealth[]}, blue, rotate around={35:(4,4)}] (4, 4) -- +(2, 0);

        \draw[thick, rotate around={290:(4,0)}] (4, 0) ellipse (2cm and 1cm);
        \draw[->, thick, -{Stealth[]}, red, rotate around={290:(4,0)}] (4, 0) -- +(0, 1);
        \draw[->, thick, -{Stealth[]}, blue, rotate around={290:(4,0)}] (4, 0) -- +(2, 0);

        \draw[->, thick, -{Stealth[]}] (-1, 1) to [out=120,in=240] node [left] {$V^{*}$} (-1, 3);
        \draw[->, thick, -{Stealth[]}] (1, 5) to [out=30,in=150] node [above] {$\Sigma$} (3, 5);
        \draw[->, thick, -{Stealth[]}] (5, 3) to [out=300,in=60] node [right] {$U$} (5, 1);
        \draw[->, thick, -{Stealth[]}] (1, -1) to [out=330,in=210] node [below] {$A$} (3, -1);
    \end{tikzpicture}
    \caption{Geometrical interpretation of the singular value decomposition}
    \label{fig:svd-geometry}
\end{figure}

\begin{prop}
    Let $A$ be an $m \times n$ matrix with singular value decomposition $U\Sigma V^{*}$, with non-zero singular values $\sigma_1, \ldots, \sigma_r$ where $A$ is the rank of $A$. Then $u_1, \ldots, u_r$ is an orthonormal basis for the image of $A$ and $v_{r+1}, \ldots, v_n$ is an orthonormal basis for the null space of $A$.
\end{prop}

\begin{proof}
    Notice that, if we order $\Sigma$ so the non-zero singular values come first, $\{e_1, \ldots, e_r\}$ form an orthonormal basis for the image of $\Sigma$ and $\{e_{r+1}, \ldots, e_n\}$ an orthonormal basis for the null space of $\Sigma$. Since $U$ and $V$ are unitary, and therefore non-singular, they preserve the dimension of linear subspaces. Therefore the image $A$ is simply the image $U\Sigma$ and the null space of $A$ is image $V\Sigma$ by Proposition \ref{adjoint-nullspace-image}, so the image of $A$ has orthonormal basis $\{Ue_1, \ldots, Ue_{r}\}$ and the null space $\{Ve_{r+1}, \ldots, Ve_{n}\}$.
\end{proof}

\begin{thm}
    All matrices possess a singular value decomposition.
\end{thm}

\begin{proof}
    Let $A$ be an $m \times n$ matrix. Notice that $A^{*}A$ is positive semi-definite, and so it has non-negative real eigenvalues. Let $v_1, \ldots, v_n$ be a set of orthonormal eigenvectors of $A^{*}A$ with eigenvalues $\lambda_1 = \sigma_1^2, \ldots, \lambda_n = \sigma_n^2$. Then $A^{*}Av_i = \sigma_i^2v_i$, and since $\langle v_i, v_i \rangle = 1$ we have
    \begin{align*}
        \sigma_i^2 = \langle v_i, A^{*}Av_i \rangle = \langle Av_i, Av_i \rangle,
    \end{align*}
    and so $\norm{Av_i}_2^2 = \sigma_i^2$. Let $r$ be the rank of $A^{*}A$, which is also the rank of $A$ by Lemma \ref{aastar-rank}, and order $\lambda_i$ such that $\lambda_1, \ldots, \lambda_r \neq 0$ and $\lambda_{r+1}, \ldots, \lambda_{n} = 0$.

    Next, for $1 \leq i \leq r$ we define
    \begin{align*}
        u_i = \frac{Av_i}{\sigma_i}.
    \end{align*}
    Note that
    \begin{align*}
        \langle u_i, u_j \rangle &= \left\langle \frac{Av_i}{\sigma_i}, \frac{Av_j}{\sigma_j} \right\rangle \\
        &= \frac{1}{\sigma_i\sigma_j}\left\langle v_i, A^{*}Av_j \right\rangle \\
        &= \frac{1}{\sigma_i\sigma_j}\left\langle v_i, \sigma_j^2v_j \right\rangle \\
        &= \frac{\sigma_j}{\sigma_i}\left\langle v_i, v_j \right\rangle,
    \end{align*}
    which is equal to $\sigma_j/\sigma_i$ if $i = j$ and zero otherwise. For $1 \leq i, j \leq r$,
    \begin{align*}
        u_i^{*}Av_j &= \sigma_ju_i^{*}v_j = \sigma_j\delta_{ij}.
    \end{align*}
    Construct $U_r$ such that $u_i$ is the $i$th column of $U_r$ and $v_j$ is the $j$th column of $V_r$, and then
    \begin{align*}
        (U_r^{*}AV_r)_{ij} &= \sigma_j\delta_{ij},
    \end{align*}
    so
    \begin{align*}
        U_r^{*}AV_r = \Sigma_r,
    \end{align*}
    where $\Sigma_r$ is a $r \times r$ diagonal matrix whose first $r$ entries are $\sigma_1, \ldots, \sigma_r$.

    Complete $u_i$ and $v_j$ to orthonormal bases for $\C^m$ and $\C^n$ respectively. Note that $u_i^{*}Av_j$ is necessarily zero for $j > r$, since the rank of $A$ is $r$. Let $U$ and $V$ be the respective completed versions of $U_r$ and $V_r$, and let $\Sigma$ be $U^{*}AV$. Note that $U$ and $V$ are both unitary since their columns are orthonormal bases. It follows that a (non-unique) singular value decomposition of $A$ is
    \begin{align*}
        U\Sigma V^{*}.
    \end{align*}
\end{proof}

\begin{prop}
    Consider an $m \times n$ matrix $A$ with singular value decomposition $U\Sigma V^{*}$. $\Sigma$ is unique up to ordering, and its diagonal elements are the singular values of $A$.
\end{prop}

\begin{proof}
    Notice that $A^{*}A = V(\Sigma^{*}\Sigma)V^{*}$, and so it is necessarily positive semi-definite and Hermitian. Therefore, $A^{*}A$ must have $n$ real eigenvalues. Let $\Lambda$ be a diagonal matrix of all the eigenvalues of $A^{*}A$. Since $V(\Sigma^{*}\Sigma)V^{*}$ is a Schur normal form, it follows that $\Lambda = \Sigma^{*}\Sigma$, and since the Cholesky decomposition is unique by Theorem \ref{cholesky-factorization}, it follows that $\Sigma$ is unique, up to the order of eigenvalues chosen in $\Lambda$.

    Furthermore, since the square roots of the eigenvalues of $A^{*}A$ are the \emph{singular values} of $A$ by definition, it follows that the diagonal entries of $\Sigma$ are precisely the singular values of $A$.
\end{proof}

\begin{exmp}
    The SVD of a matrix can be used to compute the Frobenius norm, since given $A = U\Sigma V^{*}$ has Frobenius norm
    \begin{align*}
        \norm{A}_F = \sqrt{\trace A^{*}A} = \sqrt{\trace V\Sigma^2V^{*}}.
    \end{align*}
    Since $\trace AB = \trace BA$, it follows that if $B$ is similar to $C$ where $B = SCS^{-1}$, we have
    \begin{align*}
        \trace B = \trace \left(SC\right)S^{-1} = \trace S^{-1}SC = \trace C,
    \end{align*}
    and so similar matrices have the same trace. Therefore,
    \begin{align*}
        \norm{A}_F = \sqrt{\trace \Sigma^2}.
    \end{align*}
\end{exmp}

\begin{rmk}
    A \emph{reduced} singular value decomposition of $A$ (as opposed to the full SVD decscribed above) is one where any columns of $U$, $\Sigma$, and $V$ corresponding to zero singular values have been removed, leaving $\hat{U}$ and $\hat{V}$ to be \emph{semi-unitary} and $\hat{\Sigma}$ square diagonal, such that $U^{*}U = V^{*}V = I_r$, where $r$ is the rank of $A$.
\end{rmk}

\section{Non-negative matrices}

For $A \in M_n$, let $\abs{A} \in M_n$ denote the matrix such that $\abs{A}_{ij} = \abs{A_{ij}}$. Note that $\abs{AB} \leq \abs{A}\abs{B}$, where the inequality is taken coordinate-wise. Therefore, $\abs{A^k} \leq \abs{A}^k$.

\begin{prop}\label{prop:inf-norm-identity}
    Let $A \in M_n$ such that $0 \leq A$, and $\sum_{j}A_{ij} = \alpha$ for all rows $i$, then
    \begin{align*}
        \alpha = \rho(A) = \norm{A}_{\infty}.
    \end{align*}
\end{prop}

\begin{proof}
    Notice that $A\vec{1} = \alpha\vec{1}$, so $\alpha \leq \rho(A)$. But $\norm{A}_{\infty} = \alpha$, and $\rho(A) \leq \norm{A}_{\infty}$, so the result follows.
\end{proof}

\begin{thm}\label{thm:positive-matrix-spectral-inequality}
    Let $A \in M_n$ and $B \in M_n(\R)$ such that $\abs{A} \leq B$. Then $\rho(A) \leq \rho(\abs{A}) \leq \rho(B)$.
\end{thm}

\begin{proof}
    Notice that $\abs{A^k} \leq \abs{A}^k \leq B^k$. Therefore, by coordinate-wise monotonicity of the Frobenius norm, we obtain
    \begin{align*}
        \norm{\abs{A^k}}_{F}^{1/k} \leq \norm{\abs{A}^k}_{F}^{1/k} \leq \norm{B^{k}}_F^{1/k}.
    \end{align*}
    Since $\norm{\abs{A}}_F = \norm{A}_F$, we have
    \begin{align*}
        \norm{A^k}_F^{1/k} \leq \norm{\abs{A}^k}_F^{1/k} \leq \norm{B^k}_F^{1/k}.
    \end{align*}
    As $k \to \infty$, we know $\rho(A) = \norm{A^k}_F^{1/k}$ by Theorem \ref{thm:spectral-radius-matrix-power-limit}. Therefore, $\rho(A) \leq \rho(\abs{A}) \leq \rho(B)$.
\end{proof}

\begin{cor}\label{cor:minimum-row-sum-spectral-bound}
    Let $A \in M_n$ such that $0 \leq A$. Then the minimum row sum is at most the spectral radius.
\end{cor}

\begin{proof}
    If the minimum row sum is zero, the result is trivial. Therefore, we consider $\sum_{i}A_{ij} > 0$. Let $\tilde{A} \in M_n$ such that the $k$th row of $\tilde{A}$ is the $k$th row of $A$, scaled by
    \begin{align*}
        \frac{\min_{i}\sum_{j}A_{ij}}{\sum_{j}A_{kj}},
    \end{align*}
    which is at most one. Therefore, $0 \leq \tilde{A} \leq A$, and the row sums of $\tilde{A}$ are all $\min_{i}\sum_{j}A_{ij}$. Therefore, by Proposition \ref{prop:inf-norm-identity} we have
    \begin{align*}
        \rho(\tilde{A}) = \norm{\tilde{A}}_{\infty} = \min_{i}\sum_{j}A_{ij},
    \end{align*}
    and so by Theorem \ref{thm:positive-matrix-spectral-inequality} we have $\rho(\tilde{A}) \leq \rho(A)$.
\end{proof}

\begin{cor}
    Let $A \in M_n$. If $A > 0$, then $\rho(A) > 0$. If $A \geq 0$ and $A$ is irreducible, we have $\rho(A) > 0$.
\end{cor}

\begin{proof}
    If $A > 0$, then the minimum row sum is positive, and so $\rho(A) > 0$ by Corollary \ref{cor:minimum-row-sum-spectral-bound}.

    If $A \geq 0$ and $A$ is irreducible, we cannot have a row of all zeros, so $\rho(A) > 0$ by Corollary \ref{cor:minimum-row-sum-spectral-bound}.
\end{proof}

\begin{thm}\label{thm:positive-matrix-spectral-inequalities}
    Let $A \in M_n$, and $x \in \C^n$ such that $A \geq 0$ and $x > 0$. If there exists $\alpha \geq 0$ such that $Ax > \alpha x$, then $\rho(A) > \alpha$. If $Ax \geq \alpha x$, then $\rho(A) \geq \alpha$.

    If there exists $\alpha \geq 0$ such that $Ax < \alpha x$, then $\rho(A) < \alpha$ (or with non-strict inequalities).
\end{thm}

\begin{proof}
    Let $X$ be the diagonal matrix whose diagonal is $x$. Note that $X\vec{1} = x$. It follows that $AX\vec{1} \geq \alpha X\vec{1}$, and so $X^{-1}AX\vec{1} \geq \alpha \vec{1}$. Therefore, every row sum of $X^{-1}AX$ is at least $\alpha$, and so $\rho(A) = \rho(X^{-1}AX) \geq \alpha$ by Corollary \ref{cor:minimum-row-sum-spectral-bound}.

    Similarly, if $Ax \leq \alpha x$, then $X^{-1}AX\vec{1} \leq \alpha\vec{1}$, and so the row sums are at most $\alpha$. It follows that $\rho(A) = \rho(X^{-1}AX) \leq \norm{X^{-1}AX}_{\infty} \leq \alpha$.

    If the inequalities are strict, there is some $\varepsilon$ gap such that $Ax \geq (\alpha + \varepsilon)x$ or $Ax \leq (\alpha - \varepsilon)x$, and so the above arguments imply $\rho(A) \geq \alpha+\varepsilon > \alpha$, or $\rho(A) \leq \alpha-\varepsilon < \alpha$ respectively.
\end{proof}

\begin{cor}
    Let $A \in M_n$ such that $A \geq 0$. If $A$ has a strictly positive eigenvector, then its associated eigenvalue is $\rho(A)$.
\end{cor}

\begin{proof}
    If $x > 0$ such that $Ax = \lambda x$, then we have $\rho(A) \leq \lambda$ and $\rho(A) \geq \lambda$, so $\rho(A) = \lambda$.
\end{proof}

\begin{lemma}\label{lemma:perron}
    Let $A \in M_n$ such that $A > 0$. If $\lambda, x$ is an eigenvalue, eigenvector pair such that $\abs{\lambda} = \rho(A)$, then $\abs{x} > 0$ and $A\abs{x} = \abs{\lambda}\abs{x}$.
\end{lemma}

\begin{proof}
    Since $A > 0$ and $\abs{x} \neq \vec{0}$, we know $A\abs{x}$ is a non-zero linear combination of strictly positive vectors, and so $A\abs{x} > 0$. We also know that
    \begin{align*}
        A\abs{x} = \abs{A}\abs{x} \geq \abs{Ax} = \abs{\lambda x} = \abs{\lambda}\abs{x},
    \end{align*}
    and so $A\abs{x} \geq \rho(A)\abs{x}$. Suppose, for the sake of contradiction, that $A\abs{x} \neq \rho(A)\abs{x}$, and so $w = A\abs{x} - \rho(A)\abs{x}$ satisfies $w \geq 0$. But then $Aw > 0$, so $A\left(A\abs{x}\right) > \rho(A)\left(A\abs{x}\right)$, and so by Theorem \ref{thm:positive-matrix-spectral-inequalities}, it follows that $\rho(A) > \rho(A)$. Therefore, we have a contradiction and so we must in fact have $A\abs{x} = \rho(A)\abs{x}$.

    Finally, $\abs{x} = (A\abs{x})/\abs{\lambda} > 0$, since $\abs{x} \geq 0$ implies $A\abs{x} > 0$.
\end{proof}

\begin{thm}{Perron}\proofbreak
    If $A \in M_n$ is a positive matrix, then
    \begin{itemize}
        \item $\rho(A) > 0$,
        \item $\rho(A) \in \sigma(A)$,
        \item there exists $x > 0$ such that $Ax = \rho(A)x$,
        \item the algebraic multiplicity of $\rho(A)$ is one,
        \item all other eigenvalues $\lambda$ satisfy $\abs{\lambda} < \rho(A)$.
    \end{itemize}
\end{thm}

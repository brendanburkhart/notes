\chapter{Probability}
\label{ch:probability}

\section{Axioms of Probability}

\begin{defn}\proofbreak
    \begin{itemize}
        \item Sample point: a possible outcome of a probabilistic experiment, often denoted by $\omega$.
        \item Sample space: the set of all sample points, often denoted by $\Omega$.
        \item Event: any subset of the sample space.
    \end{itemize}
\end{defn}

\begin{defn}
    If events $A_i \subseteq \Omega$ are disjoint, we say that these events are \emph{mutually exclusive}.
\end{defn}

\begin{defn}\label{kolmogorov-probability-axioms}
    A \emph{probability space} is a sample space $\Omega$ together with a function $P: \mathcal{P}(\Omega) \to \R$ that satisfies the following axioms:
    \begin{itemize}
        \item Non-negativity: for any event $A \subseteq \Omega$, $P(A) \geq 0$.
        \item Normalization: $P(\Omega) = 1$.
        \item Countable-additivity: if $A_i$ are a countable sequence of mutually exclusive events, then $P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i)$.
    \end{itemize}
\end{defn}

\begin{rmk}
    These axioms are due to Andrey Kolmogorov.
\end{rmk}

\begin{prop}
    Let $(\Omega, P)$ be a probability space, and $A \subseteq \Omega$ an event. Then $P(A) + P(A^{c}) = 1$.
\end{prop}

\begin{proof}
    Since $A$ and $A^{c}$ are mutually exclusive, $P(A) + P(A^{c}) = P(A \union A^{c}) = P(\Omega) = 1$.
\end{proof}

\begin{cor}
    $P(\emptyset) = 0$.
\end{cor}

\begin{prop}Monotonicity\label{probability-monotonicity}\proofbreak
    Let $A, B \subseteq \Omega$ be events such that $A \subseteq B$. Then $P(A) \leq P(B)$.
\end{prop}

\begin{proof}
    Let $C = B - A$. Then $A \union C = B$ and $A \intersection C = \emptyset$. Since $A$ and $C$ are therefore mutually exclusive, $P(A) + P(C) = P(A \union C) = P(B)$. Since $P(C) \geq 0$ by the axiom of non-negativity, it follows that $P(A) \leq P(B)$.
\end{proof}

\begin{thm}{Inclusion-exclusion Principle}\label{inclusion-exclusion}
    Let $A, B, C \subseteq \Omega$ be events. Then
    \[P(A \union B) = P(A) + P(B) - P(A \intersection B),\]
    and
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) \\
                               &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
                               &+ P(A \intersection B \intersection C).
    \end{align*}
\end{thm}

\begin{proof}
    Since $A \union B = (A - B) \union (B - A) \union (A \intersection B)$, which are necessarily mutually exclusive events, we have
    \[P(A \union B) = P(A - B) + P(B - A) + P(A \intersection B).\]
    Now note that $A = (A - B) \union (A \intersection B)$ and $B = (B - A) \union (A \intersection B)$, and so $P(A) = P(A - B) + P(A \intersection B)$ and $P(B) = P(B - A) + P(A \intersection B)$. Therefore, \begin{align*}
        P(A \union B) &= P(A - B) + P(B - A) + P(A \intersection B) \\
                      &= \big[P(A - B) + P(A \intersection B)\big] + \big[P(B - A) + P(A \intersection B)\big] - P(A \intersection B) \\
                      &= P(A) + P(B) - P(A \intersection B).
    \end{align*}

    To prove the three-way version of the principle, we can apply the two-way version to $A \union B$ and $C$. This gives us
    \begin{align*}\label{inclusion-exclusion-three-intermediate}\tag{$1$}
        P(A \union B \union C) &= P(A \union B) + P(C) - P((A \union B) \intersection C) \\
        &= \big[P(A) + P(B) - P(A \intersection B)\big] + P(C) - P((A \union B) \intersection C).
    \end{align*}
    Since $(A \union B) \intersection C = (A \intersection C) \union (B \intersection C)$, we can apply the principle again to find that
    \[P((A \union B) \intersection C) = P(A \intersection C) + P(B \intersection C) - P((A \intersection C) \intersection (B \intersection C)).\] Noting that $(A \intersection C) \intersection (B \intersection C) = A \intersection B \intersection C$ and substituting this back into \ref{inclusion-exclusion-three-intermediate}, we obtain
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) - P(A \intersection B) - \big[P(A \intersection C) + P(B \intersection C) - P(A \intersection B \intersection )\big] \\
        &= P(A) + P(B) + P(C) \\
        &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
        &+ P(A \intersection B \intersection C).
    \end{align*}
\end{proof}

\begin{prop}
    \[A \union B = A \disjointunion (B - A).\]
    \[A \union B \union C = A \disjointunion (B - A) \disjointunion (C - (A \union B)).\]
\end{prop}

\begin{exmp}{Finite, equally-likely probability law}\proofbreak
    Let $(\Omega, P)$ be a sample space together with a function $P: \mathcal{P}(\Omega) \to \R$, where $\abs{\Omega} \in \Z^{+}$ and for all $A \in \mathcal{\Omega}$, \[P(A) = \frac{\abs{A}}{\abs{\Omega}}.\]

    Since $\abs{A} \geq 0$ and $\abs{\Omega} > 0$, $P(A) \geq 0$ and so this model satifies the non-negativity axiom. Since $P(\Omega) = \frac{\abs{\Omega}}{\abs{\Omega}} = 1$, this model satisfies the normalization axiom.
    
    Let $A = A_1 \disjointunion A_2 \disjointunion \cdots$. Then
    \begin{align*}
        P(A) = \frac{\abs{A_1 \disjointunion A_2 \disjointunion \cdots}}{\abs{\Omega}} = \frac{\abs{A_1}}{\abs{\Omega}} + \frac{\abs{A_2}}{\abs{\Omega}} + \cdots = P(A_1) + P(A_2) + \cdots,
    \end{align*}
    and so this model also satisfies the countable-additivity axiom. Therefore, it is a probability space.
\end{exmp}

\begin{exmp}{Discrete probability law}\proofbreak
    Let $\Omega$ be a sample space such that $\abs{\Omega}$ is countable, and let $M: \Omega \to R$ be a mass-function: $M(\omega) \geq 0$ for all $\omega \in \Omega$, and $\sum_{\omega \in \Omega}M(\omega) = 1$.

    Now consider $P: \mathcal{P}(\Omega) \to \R$ defined by
    \[P(A) = \sum_{\omega \in A}M(\omega).\]

    Since $M(\omega) \geq 0$, $P(A) \geq$ and so $P$ satisfies the axiom of non-negativity. Since \[P(\Omega) = \sum_{\omega \in \Omega}M(\omega) = 1\] by the definition of $M$, $P$ satisfies the axiom of normalization. Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i) = \sum_{i}\sum_{\omega \in A_i}M(\omega) = \sum_{\omega \in A}M(\Omega) = P(A).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}


\begin{exmp}{Geometric probability law}\proofbreak
    Let $\Omega = \{\omega_1, \omega_2, \ldots\}$ be a sample space such that $\abs{\Omega} = \abs{\Z}$, and define $P: \mathcal{P}(\Omega) \to \R$ by
    \[P(A) = \sum_{\omega_i \in A}\left(\frac{1}{2}\right)^{i}.\]
    
    Since $\left(\frac{1}{2}\right)^i \geq 0$, $P(A) \geq 0$ and so $P$ satisfies the axiom of non-negativity. Since
    \begin{align*}
        P(\Omega) = \sum_{\omega_i \in \Omega}\left(\frac{1}{2}\right)^{i} = \sum_{i=1}^{\infty}\left(\frac{1}{2}\right)^{i} = \frac{1/2}{1 - 1/2} = 1,
    \end{align*}
    we know that $P$ satisfies the axiom of normalization.

    Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{\omega_j \in A}\left(\frac{1}{2}\right)^{j} = \sum_{i}\sum_{\omega_j \in A_i}\left(\frac{1}{2}\right)^{j} = \sum_{i}P(A_i).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}

\section{Conditional Probability}

\begin{exmp}
    Roll two six-sided dice, and only look at one of them. It is a six. Let $S$ denote the event that the sum of the values of the two dice is seven, and $A$ is the event that at least one of dice was a six. What is the probability of $P(S | A)?$ It is $\frac{2}{11}$, not $\frac{1}{6}$.
\end{exmp}

\begin{defn}
    Let $A, B \in Omega$. Then the probability that $A$ occurred, given that $B$ occured, is denoted by $P(A | B)$ and read as ``the probability of $A$ given $B$''.
\end{defn}

\begin{prop}
    If all sample points in $\Omega$ are equally likely, then $P(A | B) = \frac{P(A \intersection B)}{P(B)}$.
\end{prop}

\begin{cor}
    $P(A \intersection B) = P(A | B)P(B)$.
\end{cor}

\begin{thm}
    Let $(\Omega, P)$ be a probability space, and $B \subseteq \Omega$ an event such that $P(B) > 0$. Then $(B, P(\cdot | B))$ form a probability space.
\end{thm}

\begin{proof}
    To show that $(B, P(\cdot|B))$ is a probability space, we need to show non-negativity (that $P(A | B) \geq 0$ for all $A$), normalization (that $P(B|B) = 1$), and countable additivity (that $P(A_1 \disjointunion A_2 \disjointunion \cdots | B) = P(A_1 | B) + P(A_2 | B) + \cdots$ if $A_i$ is a countable sequence).

    For any $A \subseteq B$, we know that $P(A \intersection B), P(B) \geq 0$, and so \[P(A | B) = \frac{P(A \intersection B)}{P(B)} \geq 0.\]

    \[P(B | B) = \frac{P(B \intersection B)}{P(B)} = \frac{P(B)}{P(B)} = 1.\]

    \begin{align*}
        P(A_1 \disjointunion A_2 \disjointunion \cdots | B) &= \frac{P\left((A_1 \disjointunion A_2 \disjointunion \cdots) \intersection B\right)}{P(B)} = \frac{P(A_1 \intersection B) \disjointunion (A_2 \intersection B) \disjointunion \cdots)}{P(B)} \\
        &= \frac{P(A_1 \intersection B) + P(A_2 \intersection B) + \cdots)}{P(B)} = \frac{P(A_1 \intersection B)}{P(B)} + \frac{P(A_2 \intersection B)}{P(B)} + \cdots \\
        &= P(A_1 | B) + P(A_2 | B) + \cdots.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a sequence of coin tosses. Let $B$ be the event that the first head of the sequence occurs on an odd numbered toss (the first toss is \#1), and let $A$ be the event that the event first coin toss lands on heads. What is $P(A | B)$?

    Since $A \subseteq B$, we know that $A \intersection B = A$. Furthermore, $P(A) = \frac{1}{2}$, and $P(B) = \frac{1}{2} + \frac{1}{8} + \cdots = \frac{1/2}{1-1/4} = \frac{1}{2}\cdot\frac{4}{3} = \frac{2}{3}$. Therefore, \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1}{2}\cdot\frac{3}{2} = \frac{3}{4}.\]
\end{exmp}

\begin{thm}Law of Total Probability\label{total-probability}\proofbreak
    Let $B_1, \ldots, B_n \in Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \[P(A) = \sum_{i=1}^{n}P(A \intersection B_i) = \sum_{i=1}^{n}P(A | B_i)P(B_i).\]
\end{thm}

\begin{proof}
    Note that if $x \in (A \intersection B_i)$ and $x \in (A \intersection B_j)$, then $x \in B_i$ and $x \in B_j$. Since $B_1, \ldots, B_n$ are mutually exclusive they are disjoint, and so we must have $i = j$. Therefore, $A \intersection B_1, A \intersection B_2, \ldots, A \intersection B_n$ are disjoint.

    Also note that
    \begin{align*}
        \disjointunionbig_{i=1}^{n}A \intersection B_i = A \intersection \left(\disjointunionbig_{i=1}^{n}B_i\right) = A \intersection \Omega = A
    \end{align*}
    since $B_1, \ldots, B_n$ are exhaustive.

    Therefore, by the axiom of countable additivity,
    \begin{align*}
        P(A) = P\left(\disjointunionbig_{i=1}^{n}A \intersection B_i\right) = \sum_{i=1}^{n}P(A \intersection B_i).
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a box containing six coins. Three of these coins are fair, but one of them has two tails, and the remaining two have two heads each. If we randomly (uniformly) select one coin and flip it twice, what is the probability it comes up heads twice?

    Let $A$ be the event we observe two heads. Let $B_1$ be the event we selected one of the fair coins, $B_2$ the event we selected the $2$-tailed coin, and $B_3$ the event that we selected the $2$-headed coin. Then by the Law of Total Probability \ref{total-probability},
    \[P(A) = \sum_{i=1}^{n}P(A | B_i)P(B_i) = \left(\frac{1}{4}\cdot\frac{3}{6}\right) + \left(\frac{0}{1}\cdot\frac{1}{6}\right) + \left(\frac{1}{1}\cdot\frac{2}{6}\right) = \frac{1}{8} + \frac{1}{3} = \frac{11}{24}.\]
\end{exmp}

\begin{thm}{Bayes' Rule}\proofbreak
    Let $B_1, \ldots, B_n \in \Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \begin{align*}
        P(B_k | A) = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A \intersection B_k)} = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A | B_k)P(B_k)}.
    \end{align*}
\end{thm}

\section{Independence}

\begin{defn}
    Events $A, B \in \Omega$ are \emph{independent} if $P(A \intersection B) = P(A)P(B)$.
\end{defn}

\begin{prop}
    If $A$ and $B$ are independent events, then $P(A | B) = P(A)$ and $P(B | A) = P(B)$.
\end{prop}

\begin{proof}
    \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)\]
\end{proof}

\begin{prop}
    If $A$ and $B$ are independent events, then $A$ and $B^{c}$ are independent.
\end{prop}

\begin{proof} Since $B$ and $B^{c}$ are necessarily mutually exclusive and exhaust $A$, we have $A = (A \intersection B) \disjointunion (A \intersection B^{c})$. Furthermore, $P(B^{c}) = 1- P(B)$. Then
    \begin{align}
        P(A \intersection B^{c}) &+ P(A \intersection B) = P(A) \\
        P(A \intersection B^{c}) &+ P(A)P(B) = P(A) \\
        P(A \intersection B^{c}) &= P(A)\left(1 - P(B)\right).
    \end{align}
\end{proof}

\begin{defn}
    We say events $A_1, A_2, \ldots$ are \emph{independent} if every for every finite subcollection $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ we have
    \[P(A_{i_1} \intersection A_{i_2} \intersection \cdots \intersection A_{i_k}) = P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k}).\]
\end{defn}

\section{Random Variables}

\begin{defn}
    A \emph{random variable} $X$ on a sample space $\Omega$ is a function $X: \Omega \to \R$.
\end{defn}

\begin{exmp}
    Let $A \subseteq \Omega$ be an event, and let
    \[X: \omega \mapsto \begin{dcases}
        1, & \omega \in A \\
        0, & \omega \not\in A
    \end{dcases}.\]
    Then $X$ is a random variable.
\end{exmp}

\begin{defn}
    A random variable $X$ is said to be a \emph{discrete} random variable when its image $X(\Omega)$ is countable.
\end{defn}

\begin{defn}
    The \emph{Bernoulli distribution} distribution is the probability distribution of a random variable that takes value $1$ with probability $0 \leq p \leq 1$, and value $0$ with probability $1 - p$. For a random variable $X$ with this distribution and $k \in \{0, 1\}$:
    \[P(X = k) = \begin{dcases}
        p, & k = 1  \\
        1 - p, & k = 0
    \end{dcases} = p^k(1-p)^{1-k}.\]
\end{defn}

\begin{defn}
    The \emph{binomial} distribution with parameters $n$ and $p$ is the distribution of the number of successes in a sequence of $n$ independent trials which can take values $1, 0$ with probability $p$ and $1 - p$. The probability of $ 0 \leq k \leq n$ successes is
    \[P(X = k) = \binom{n}{k}p^k(1 - p)^{n-k}.\]
\end{defn}

\begin{defn}
    The \emph{Poission} distribution with parameter $\lambda > 0$, is the distribution of the number of independent events that occur in a fixed period when the events occur at a constant rate with $\lambda$ events per period on average. The probability of $k$ events during this period is
    \[P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}.\]
    Alternatively, if the independent events occur at a rate of $r$ per unit period, in a period of length $t$, we can use $\lambda = rt$.
\end{defn}

\begin{defn}
    The \emph{geometric} distribution with parameter $0 \leq p \leq 1$ is the distribution of the number of independent Bernoulli $p$ trials needed to get the first success. It is given by
    \[P(X = k) = (1-p)^{k-1}p.\]
\end{defn}

\begin{defn}
    The \emph{negative binomial distribution} is a generalization of the geometric distribution. Rather than the distribution of the first success in a series of independent Bernoulli $p$ trials, it is the distribution of the $r$th success in such a series. For $k \geq r$, it is given by
    \[P(X = k) = \binom{k-1}{r-1}p^r(1-p)^{k-r}.\]
\end{defn}

\begin{defn}
    The \emph{expected value} of a discrete random variable $X$ with probability mass function $P(X)$ is
    \[E(X) = \sum_{x \in \Omega}xP(X = x).\]
\end{defn}

\begin{exmp}
    Consider a discrete random variable $X$ with a Bernoulli $p$ distribution. The expected value of $X$ is
    \begin{align*}
        E(X) = \sum_{x \in {0, 1}}xP(X = x) = 0(1-p) + 1(p) = p.
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a discrete random variable $X$ with a binomial $n, p$ distribution. The expected value of $X$ is then
    \begin{align*}
        E(X) &= \sum_{x \in {0, 1, \ldots, n}}xP(X = x) \\
             &= \sum_{x=1}^{n}x\binom{n}{x}p^x(1 - p)^{n-x} \\
             &= \sum_{x=1}^{n}x\frac{n!}{x!(n-x)!}p^x(1 - p)^{n-x} \\
             &= p\sum_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^{x-1}(1 - p)^{n-x} \\
             &= np\sum_{x=0}^{n-1}\frac{(n-1)!}{x!(n-x-1)!}p^x(1 - p)^{n-x-1} \\
             &= np\sum_{x=0}^{n-1}\binom{n-1}{x}p^x(1 - p)^{(n-1)-x} \\
             &= np\left(p+(1-p)\right)^{n-1} = np.
    \end{align*}
\end{exmp}

\begin{prop}
    Expectation is linear, that is $E(kX_1+X_2) = kE(X_1) + E(X_2)$.
\end{prop}

\begin{proof}
    Follows from linearity of sums.
\end{proof}

\begin{thm}{Law of the Unconscious Statistician ($\mathcal{LOTUS}$)}{\label{lotus}}\proofbreak
    Let $X$ be a discrete random variable with expected value $E(X)$. For any function $f$, \[E[f(X)] = \sum_{x}f(x)P(X=x).\]
\end{thm}

\begin{defn}
    Let $X$ be a discrete random variable. For $k \geq 1$, the $k$th \emph{moment} of $X$ is the expected value of $X^k$.
\end{defn}

\begin{rmk}
    The mean of a random variable $X$ (denoted by $\mu$) is the first moment of $X$.
\end{rmk}

\begin{exmp}
    All moments of a Bernoulli $p$ distribution are simply $p$.
\end{exmp}

\begin{defn}
    For a random variable $X$, the \emph{variance} of $X$ is the expected squared distance from the mean: \[\variance(x) = E\left[\left(X-E(X)\right)^2\right] = E\left[\left(x - \mu\right)^2\right].\]
\end{defn}

\begin{rmk}
    While the mean has the same units as $X$, the variance has measure of those units squared.
\end{rmk}

\begin{defn}
    The \emph{standard deviation} $\sigma$ of a random variable $X$ is the positive square root of the variance of $X$.
\end{defn}

\begin{prop}
    When the first and second moments of a random variable exist and are finite, the variance can be computed by $\sigma^2 = E(X^2) - E(X)^2$.
\end{prop}

\begin{proof}
    By $\mathcal{LOTUS}$ \ref{lotus}, the variance is
    \begin{align*}
        E\left[(X-E(X))^2\right] &= E\left[X^2 - 2XE(X) + E(X)^2\right] \\
        &= E(X^2) - 2E(X)E\left[X\right] + E\left[E(X)^2\right] \\
        &= E(X^2) - 2E(X)^2 + E(X)^2 = E(X^2) - E(X)^2.
    \end{align*}
\end{proof}

\begin{exmp}
    Let $X$ be a binomial $n, p$ distribution. Recall that the mean of $X$ is $np$. Note that $E(X^2) = E[X(X-1)+X]$, and by $\mathcal{LOTUS}$ \ref{lotus}
    \begin{align*}
        E(X(X-1)) &= \sum_{x=0}^{n}x(x-1)\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
        &= \sum_{x=2}^{n}\frac{n!}{(x-2)!(n-x)!}p^x(1-p)^{n-x} \\
        &= n(n-1)p^2\sum_{x=2}^{n}\frac{(n-2)!}{(x-2)!(n-x)!}p^{(x-2)}(1-p)^{n-x} \\
        &= n(n-1)\sum_{x=0}^{n-2}\frac{(n-2)!}{x!((n-2)-x)!}p^x(1-p)^{(n-2)-x} \\
        &= n(n-1)p^2\left(p+(1-p)\right)^{n-2} = n(n-1)p^2.
    \end{align*}
    Therefore, $E(X^2) = E[X(X-1)]+E[X]=n(n-1)p^2+np = n^2p^2+np-np^2$, and so the variance is $E(X^2)-E(X)^2 = n^2p^2+np-np^2-n^2p^2 = np-np^2 = n(p-p^2)$.
\end{exmp}

\begin{defn}
    Given a random variable $X$, the \emph{moment generating function} for $\theta \in \R$ is
    \begin{align*}
        M(\theta) = E(e^{\theta X}).
    \end{align*}
\end{defn}

\begin{prop}
    If the moment generating function exists for a particular random variable,
    \begin{enumerate}
        \item it uniquely identifies that random variablem i.e. it is injective,
        \item all moments of the random variable exist and are finite,
        \item the $n$th derivative of the moment generating function at $\theta = 0$ is the $n$th moment of the random variable.
    \end{enumerate}
\end{prop}

\begin{prop}\proofbreak
    3) Consider the Maclaurin series of $e^{\theta X}$, which is
    \[e^{\theta X} = \sum_{n=0}^{\infty}\frac{(\theta X)^n}{n!} = 1 + X\theta + X^2\frac{\theta^2}{2} + \cdots.\]
    Then by linearity of the expected value and by linearity of the derivative, $M^{(k)}(0) = E\left[X^k\right]$.
\end{prop}

\begin{exmp}
    Consider the Poission $\mu$ distribution, where $P(X = x) = \frac{e^{-\mu}\mu^x}{x!}$. The moment generating function, if it exists, by $\mathcal{LOTUS}$ \ref{lotus} would be given by
    \begin{align*}
       E(e^{\theta X})] &= \sum_{x=0}^{\infty}e^{\theta x}P(X = x) \\
       &= \sum_{x=0}^{\infty}e^{\theta x}\frac{e^{-\mu}\mu^x}{x!} \\
       &= e^{-\mu}\sum_{x=0}^{\infty}\frac{\left(\mu e^{\theta}\right)^{x}}{x!} \\
       &= e^{-\mu}e^{e^{\theta}\mu} = e^{\mu(e^{\theta-1})}.
    \end{align*}
\end{exmp}

\section{Cumulative Distribution Functions}

\begin{defn}
    Given a random variable $X$, the function $p(x) = P(X = x)$ is the \emph{probability mass function} (or PMF) of $X$.
\end{defn}

\begin{defn}
    Given a random variable $X$, the function $F: \R \to [0, 1]$ defined by
    \[x \mapsto P(X \leq x)\]
    is the \emph{cumulative distribution function} (or CDF) of $X$.
\end{defn}

\begin{prop}
    Given a random variable $X$ with CDF $F$, for any $x < y$, then $F(X) \leq F(Y)$.
\end{prop}

\begin{proof}
    If $x < y$ then the event $X \leq x$ is a subset of the event $X \leq y$, so $P(X \leq x) \leq P(X \leq y)$ by Proposition \ref{probability-monotonicity}.
\end{proof}

\begin{prop}
    Given a random variable $X$ with CDF $F$, for all  $F$ is right-continuous:
    \[\forall x \in \R,\;\;\lim_{n \to\infty}F(x + \frac{1}{n}) = F(x),\]
    and left-limits always exist:
    \[\forall x \in \R,\;\;\lim_{n \to \infty}F(x - \frac{1}{n}).\]
\end{prop}

\begin{defn}
    A random variable with a continuous CDF is a \emph{continuous} random variable.
\end{defn}

\begin{prop}
    Given a continuous random variable $X$, $P(X = x) = 0$ for all $x$.
\end{prop}

\begin{proof}
    Let $F_X(x) = P(X \leq x)$ be the (continuous) CDF of $X$. Fix some $x$. Then for any $h > 0$,
    \begin{align*}
        P(x-h < X \leq x) = F(x) - F(x - h).
    \end{align*}
    Therefore, as $h \to 0$, $P(x - h < X \leq x) \to 0$. Therefore, $P(x < X \leq x) = 0$, and so $P(X = x) = 0$.
\end{proof}

\begin{defn}
    The \emph{probability density function} (PDF) of a continuous random variable $X$ is
    \begin{align*}
        f(x) = \lim_{h \to 0}\frac{P(x - h < X \leq x)}{h}.
    \end{align*}
\end{defn}

\begin{rmk}
    Not all continuous random variables possess a PDF, since the limit may not exist. Every continuous CDF can be decomposed into a linear combination of two functions: a \emph{absolutely continuous} part and a \emph{singlular continuous} part. The Cantor function is an example of a CDF whose absolutely continuous part is zero.
\end{rmk}

\begin{thm}
    Let $F(x)$ be the CDF and $f(x)$ the PDF of an absolutely continuous random variable $X$. Then:
    \begin{itemize}
        \item $F'(x) = f(x)$,
        \item $F(x) = \int_{-\infty}^{x}f(x)dx$,
        \item Since $F(x)$ exists for all $x \in \R$, so does $f(x)$,
        \item $\int_{-\infty}^{\infty}f(x)dx = 1$.
    \end{itemize}
\end{thm}

\begin{cor}
    \begin{align*}
        P(a < X \leq b) &= P(X \leq b) - P(X \leq a) \\
        &= F(b) - F(a) \\
        &= \int_{-\infty}^{b}f(x)dx - \int_{-\infty}^{a}f(x)dx \\
        &= \int_{a}^{b}f(x)dx.
    \end{align*}
\end{cor}

\begin{defn}
    The expected value of a continuous random variable $X$ is
    \begin{align*}
        E(X) = \int_{-\infty}^{\infty}xf_X(x)dx.
    \end{align*}
\end{defn}

\chapter{Probability}
\label{ch:probability}

\section{Axioms of Probability}

\begin{defn}\proofbreak
    \begin{itemize}
        \item Sample point: a possible outcome of a probabilistic experiment, often denoted by $\omega$.
        \item Sample space: the set of all sample points, often denoted by $\Omega$.
        \item Event: any subset of the sample space.
    \end{itemize}
\end{defn}

\begin{defn}
    If events $A_i \subseteq \Omega$ are disjoint, we say that these events are \emph{mutually exclusive}.
\end{defn}

\begin{defn}\label{kolmogorov-probability-axioms}
    A \emph{probability space} is a sample space $\Omega$ together with a function $P: \mathcal{P}(\Omega) \to \R$ that satisfies the following axioms:
    \begin{itemize}
        \item Non-negativity: for any event $A \subseteq \Omega$, $P(A) \geq 0$.
        \item Normalization: $P(\Omega) = 1$.
        \item Countable-additivity: if $A_i$ are a countable sequence of mutually exclusive events, then $P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i)$.
    \end{itemize}
\end{defn}

\begin{rmk}
    These axioms are due to Andrey Kolmogorov.
\end{rmk}

\begin{prop}
    Let $(\Omega, P)$ be a probability space, and $A \subseteq \Omega$ an event. Then $P(A) + P(A^{c}) = 1$.
\end{prop}

\begin{proof}
    Since $A$ and $A^{c}$ are mutually exclusive, $P(A) + P(A^{c}) = P(A \union A^{c}) = P(\Omega) = 1$.
\end{proof}

\begin{cor}
    $P(\emptyset) = 0$.
\end{cor}

\begin{prop}Monotonicity\label{probability-monotonicity}\proofbreak
    Let $A, B \subseteq \Omega$ be events such that $A \subseteq B$. Then $P(A) \leq P(B)$.
\end{prop}

\begin{proof}
    Let $C = B - A$. Then $A \union C = B$ and $A \intersection C = \emptyset$. Since $A$ and $C$ are therefore mutually exclusive, $P(A) + P(C) = P(A \union C) = P(B)$. Since $P(C) \geq 0$ by the axiom of non-negativity, it follows that $P(A) \leq P(B)$.
\end{proof}

\begin{thm}{Inclusion-exclusion Principle}\label{inclusion-exclusion}
    Let $A, B, C \subseteq \Omega$ be events. Then
    \[P(A \union B) = P(A) + P(B) - P(A \intersection B),\]
    and
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) \\
                               &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
                               &+ P(A \intersection B \intersection C).
    \end{align*}
\end{thm}

\begin{proof}
    Since $A \union B = (A - B) \union (B - A) \union (A \intersection B)$, which are necessarily mutually exclusive events, we have
    \[P(A \union B) = P(A - B) + P(B - A) + P(A \intersection B).\]
    Now note that $A = (A - B) \union (A \intersection B)$ and $B = (B - A) \union (A \intersection B)$, and so $P(A) = P(A - B) + P(A \intersection B)$ and $P(B) = P(B - A) + P(A \intersection B)$. Therefore, \begin{align*}
        P(A \union B) &= P(A - B) + P(B - A) + P(A \intersection B) \\
                      &= \big[P(A - B) + P(A \intersection B)\big] + \big[P(B - A) + P(A \intersection B)\big] - P(A \intersection B) \\
                      &= P(A) + P(B) - P(A \intersection B).
    \end{align*}

    To prove the three-way version of the principle, we can apply the two-way version to $A \union B$ and $C$. This gives us
    \begin{align*}\label{inclusion-exclusion-three-intermediate}\tag{$1$}
        P(A \union B \union C) &= P(A \union B) + P(C) - P((A \union B) \intersection C) \\
        &= \big[P(A) + P(B) - P(A \intersection B)\big] + P(C) - P((A \union B) \intersection C).
    \end{align*}
    Since $(A \union B) \intersection C = (A \intersection C) \union (B \intersection C)$, we can apply the principle again to find that
    \[P((A \union B) \intersection C) = P(A \intersection C) + P(B \intersection C) - P((A \intersection C) \intersection (B \intersection C)).\] Noting that $(A \intersection C) \intersection (B \intersection C) = A \intersection B \intersection C$ and substituting this back into \ref{inclusion-exclusion-three-intermediate}, we obtain
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) - P(A \intersection B) - \big[P(A \intersection C) + P(B \intersection C) - P(A \intersection B \intersection )\big] \\
        &= P(A) + P(B) + P(C) \\
        &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
        &+ P(A \intersection B \intersection C).
    \end{align*}
\end{proof}

\begin{prop}
    \[A \union B = A \disjointunion (B - A).\]
    \[A \union B \union C = A \disjointunion (B - A) \disjointunion (C - (A \union B)).\]
\end{prop}

\begin{exmp}{Finite, equally-likely probability law}\proofbreak
    Let $(\Omega, P)$ be a sample space together with a function $P: \mathcal{P}(\Omega) \to \R$, where $\abs{\Omega} \in \Z^{+}$ and for all $A \in \mathcal{\Omega}$, \[P(A) = \frac{\abs{A}}{\abs{\Omega}}.\]

    Since $\abs{A} \geq 0$ and $\abs{\Omega} > 0$, $P(A) \geq 0$ and so this model satifies the non-negativity axiom. Since $P(\Omega) = \frac{\abs{\Omega}}{\abs{\Omega}} = 1$, this model satisfies the normalization axiom.
    
    Let $A = A_1 \disjointunion A_2 \disjointunion \cdots$. Then
    \begin{align*}
        P(A) = \frac{\abs{A_1 \disjointunion A_2 \disjointunion \cdots}}{\abs{\Omega}} = \frac{\abs{A_1}}{\abs{\Omega}} + \frac{\abs{A_2}}{\abs{\Omega}} + \cdots = P(A_1) + P(A_2) + \cdots,
    \end{align*}
    and so this model also satisfies the countable-additivity axiom. Therefore, it is a probability space.
\end{exmp}

\begin{exmp}{Discrete probability law}\proofbreak
    Let $\Omega$ be a sample space such that $\abs{\Omega}$ is countable, and let $M: \Omega \to R$ be a mass-function: $M(\omega) \geq 0$ for all $\omega \in \Omega$, and $\sum_{\omega \in \Omega}M(\omega) = 1$.

    Now consider $P: \mathcal{P}(\Omega) \to \R$ defined by
    \[P(A) = \sum_{\omega \in A}M(\omega).\]

    Since $M(\omega) \geq 0$, $P(A) \geq$ and so $P$ satisfies the axiom of non-negativity. Since \[P(\Omega) = \sum_{\omega \in \Omega}M(\omega) = 1\] by the definition of $M$, $P$ satisfies the axiom of normalization. Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i) = \sum_{i}\sum_{\omega \in A_i}M(\omega) = \sum_{\omega \in A}M(\Omega) = P(A).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}


\begin{exmp}{Geometric probability law}\proofbreak
    Let $\Omega = \{\omega_1, \omega_2, \ldots\}$ be a sample space such that $\abs{\Omega} = \abs{\Z}$, and define $P: \mathcal{P}(\Omega) \to \R$ by
    \[P(A) = \sum_{\omega_i \in A}\left(\frac{1}{2}\right)^{i}.\]
    
    Since $\left(\frac{1}{2}\right)^i \geq 0$, $P(A) \geq 0$ and so $P$ satisfies the axiom of non-negativity. Since
    \begin{align*}
        P(\Omega) = \sum_{\omega_i \in \Omega}\left(\frac{1}{2}\right)^{i} = \sum_{i=1}^{\infty}\left(\frac{1}{2}\right)^{i} = \frac{1/2}{1 - 1/2} = 1,
    \end{align*}
    we know that $P$ satisfies the axiom of normalization.

    Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{\omega_j \in A}\left(\frac{1}{2}\right)^{j} = \sum_{i}\sum_{\omega_j \in A_i}\left(\frac{1}{2}\right)^{j} = \sum_{i}P(A_i).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}

\section{Conditional Probability}

\begin{exmp}
    Roll two six-sided dice, and only look at one of them. It is a six. Let $S$ denote the event that the sum of the values of the two dice is seven, and $A$ is the event that at least one of dice was a six. What is the probability of $P(S | A)?$ It is $\frac{2}{11}$, not $\frac{1}{6}$.
\end{exmp}

\begin{defn}
    Let $A, B \in \Omega$. Then the probability that $A$ occurred, given that $B$ occured, is denoted by $P(A | B)$ and read as ``the probability of $A$ given $B$''.
\end{defn}

\begin{prop}
    If all sample points in $\Omega$ are equally likely, then $P(A | B) = \frac{P(A \intersection B)}{P(B)}$.
\end{prop}

\begin{cor}
    $P(A \intersection B) = P(A | B)P(B)$.
\end{cor}

\begin{thm}
    Let $(\Omega, P)$ be a probability space, and $B \subseteq \Omega$ an event such that $P(B) > 0$. Then $(B, P(\cdot | B))$ form a probability space.
\end{thm}

\begin{proof}
    To show that $(B, P(\cdot|B))$ is a probability space, we need to show non-negativity (that $P(A | B) \geq 0$ for all $A$), normalization (that $P(B|B) = 1$), and countable additivity (that $P(A_1 \disjointunion A_2 \disjointunion \cdots | B) = P(A_1 | B) + P(A_2 | B) + \cdots$ if $A_i$ is a countable sequence).

    For any $A \subseteq B$, we know that $P(A \intersection B), P(B) \geq 0$, and so \[P(A | B) = \frac{P(A \intersection B)}{P(B)} \geq 0.\]

    \[P(B | B) = \frac{P(B \intersection B)}{P(B)} = \frac{P(B)}{P(B)} = 1.\]

    \begin{align*}
        P(A_1 \disjointunion A_2 \disjointunion \cdots | B) &= \frac{P\left((A_1 \disjointunion A_2 \disjointunion \cdots) \intersection B\right)}{P(B)} = \frac{P(A_1 \intersection B) \disjointunion (A_2 \intersection B) \disjointunion \cdots)}{P(B)} \\
        &= \frac{P(A_1 \intersection B) + P(A_2 \intersection B) + \cdots)}{P(B)} = \frac{P(A_1 \intersection B)}{P(B)} + \frac{P(A_2 \intersection B)}{P(B)} + \cdots \\
        &= P(A_1 | B) + P(A_2 | B) + \cdots.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a sequence of coin tosses. Let $B$ be the event that the first head of the sequence occurs on an odd numbered toss (the first toss is \#1), and let $A$ be the event that the event first coin toss lands on heads. What is $P(A | B)$?

    Since $A \subseteq B$, we know that $A \intersection B = A$. Furthermore, $P(A) = \frac{1}{2}$, and $P(B) = \frac{1}{2} + \frac{1}{8} + \cdots = \frac{1/2}{1-1/4} = \frac{1}{2}\cdot\frac{4}{3} = \frac{2}{3}$. Therefore, \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1}{2}\cdot\frac{3}{2} = \frac{3}{4}.\]
\end{exmp}

\begin{thm}Law of Total Probability\label{total-probability}\proofbreak
    Let $B_1, \ldots, B_n \in \Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \[P(A) = \sum_{i=1}^{n}P(A \intersection B_i) = \sum_{i=1}^{n}P(A | B_i)P(B_i).\]
\end{thm}

\begin{proof}
    Note that if $x \in (A \intersection B_i)$ and $x \in (A \intersection B_j)$, then $x \in B_i$ and $x \in B_j$. Since $B_1, \ldots, B_n$ are mutually exclusive they are disjoint, and so we must have $i = j$. Therefore, $A \intersection B_1, A \intersection B_2, \ldots, A \intersection B_n$ are disjoint.

    Also note that
    \begin{align*}
        \disjointunionbig_{i=1}^{n}A \intersection B_i = A \intersection \left(\disjointunionbig_{i=1}^{n}B_i\right) = A \intersection \Omega = A
    \end{align*}
    since $B_1, \ldots, B_n$ are exhaustive.

    Therefore, by the axiom of countable additivity,
    \begin{align*}
        P(A) = P\left(\disjointunionbig_{i=1}^{n}A \intersection B_i\right) = \sum_{i=1}^{n}P(A \intersection B_i).
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a box containing six coins. Three of these coins are fair, but one of them has two tails, and the remaining two have two heads each. If we randomly (uniformly) select one coin and flip it twice, what is the probability it comes up heads twice?

    Let $A$ be the event we observe two heads. Let $B_1$ be the event we selected one of the fair coins, $B_2$ the event we selected the $2$-tailed coin, and $B_3$ the event that we selected the $2$-headed coin. Then by the Law of Total Probability \ref{total-probability},
    \[P(A) = \sum_{i=1}^{n}P(A | B_i)P(B_i) = \left(\frac{1}{4}\cdot\frac{3}{6}\right) + \left(\frac{0}{1}\cdot\frac{1}{6}\right) + \left(\frac{1}{1}\cdot\frac{2}{6}\right) = \frac{1}{8} + \frac{1}{3} = \frac{11}{24}.\]
\end{exmp}

\begin{thm}{Bayes' Rule}\proofbreak
    Let $B_1, \ldots, B_n \in \Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \begin{align*}
        P(B_k | A) = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A \intersection B_k)} = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A | B_k)P(B_k)}.
    \end{align*}
\end{thm}

\section{Independence}

\begin{defn}
    Events $A, B \in \Omega$ are \emph{independent} if $P(A \intersection B) = P(A)P(B)$.
\end{defn}

\begin{prop}
    If $A$ and $B$ are independent events, then $P(A | B) = P(A)$ and $P(B | A) = P(B)$.
\end{prop}

\begin{proof}
    \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)\]
\end{proof}

\begin{prop}
    If $A$ and $B$ are independent events, then $A$ and $B^{c}$ are independent.
\end{prop}

\begin{proof} Since $B$ and $B^{c}$ are necessarily mutually exclusive and exhaust $A$, we have $A = (A \intersection B) \disjointunion (A \intersection B^{c})$. Furthermore, $P(B^{c}) = 1- P(B)$. Then
    \begin{align}
        P(A \intersection B^{c}) &+ P(A \intersection B) = P(A) \\
        P(A \intersection B^{c}) &+ P(A)P(B) = P(A) \\
        P(A \intersection B^{c}) &= P(A)\left(1 - P(B)\right).
    \end{align}
\end{proof}

\begin{defn}
    We say events $A_1, A_2, \ldots$ are \emph{independent} if every for every finite subcollection $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ we have
    \[P(A_{i_1} \intersection A_{i_2} \intersection \cdots \intersection A_{i_k}) = P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k}).\]
\end{defn}

\section{Random Variables}

\begin{defn}
    A \emph{random variable} $X$ on a sample space $\Omega$ is a function $X: \Omega \to \R$.
\end{defn}

\begin{exmp}
    Let $A \subseteq \Omega$ be an event, and let
    \[X: \omega \mapsto \begin{dcases}
        1, & \omega \in A \\
        0, & \omega \not\in A
    \end{dcases}.\]
    Then $X$ is a random variable.
\end{exmp}

\begin{defn}
    A random variable $X$ is said to be a \emph{discrete} random variable when its image $X(\Omega)$ is countable.
\end{defn}

\begin{defn}
    The \emph{Bernoulli distribution} distribution is the probability distribution of a random variable that takes value $1$ with probability $0 \leq p \leq 1$, and value $0$ with probability $1 - p$. For a random variable $X$ with this distribution and $k \in \{0, 1\}$:
    \[P(X = k) = \begin{dcases}
        p, & k = 1  \\
        1 - p, & k = 0
    \end{dcases} = p^k(1-p)^{1-k}.\]
\end{defn}

\begin{defn}
    The \emph{binomial} distribution with parameters $n$ and $p$ is the distribution of the number of successes in a sequence of $n$ independent trials which can take values $1, 0$ with probability $p$ and $1 - p$. The probability of $ 0 \leq k \leq n$ successes is
    \[P(X = k) = \binom{n}{k}p^k(1 - p)^{n-k}.\]
\end{defn}

\begin{defn}
    The \emph{Poission} distribution with parameter $\lambda > 0$, is the distribution of the number of independent events that occur in a fixed period when the events occur at a constant rate with $\lambda$ events per period on average. The probability of $k$ events during this period is
    \[P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}.\]
    Alternatively, if the independent events occur at a rate of $r$ per unit period, in a period of length $t$, we can use $\lambda = rt$.
\end{defn}

\begin{defn}
    The \emph{geometric} distribution with parameter $0 \leq p \leq 1$ is the distribution of the number of independent Bernoulli $p$ trials needed to get the first success. It is given by
    \[P(X = k) = (1-p)^{k-1}p.\]
\end{defn}

\begin{defn}
    The \emph{negative binomial distribution} is a generalization of the geometric distribution. Rather than the distribution of the first success in a series of independent Bernoulli $p$ trials, it is the distribution of the $r$th success in such a series. For $k \geq r$, it is given by
    \[P(X = k) = \binom{k-1}{r-1}p^r(1-p)^{k-r}.\]
\end{defn}

\begin{defn}
    The \emph{expected value} of a discrete random variable $X$ with probability mass function $P(X)$ is
    \[E(X) = \sum_{x \in \Omega}xP(X = x).\]
\end{defn}

\begin{exmp}
    Consider a discrete random variable $X$ with a Bernoulli $p$ distribution. The expected value of $X$ is
    \begin{align*}
        E(X) = \sum_{x \in {0, 1}}xP(X = x) = 0(1-p) + 1(p) = p.
    \end{align*}
\end{exmp}

\begin{exmp}
    Consider a discrete random variable $X$ with a binomial $n, p$ distribution. The expected value of $X$ is then
    \begin{align*}
        E(X) &= \sum_{x \in {0, 1, \ldots, n}}xP(X = x) \\
             &= \sum_{x=1}^{n}x\binom{n}{x}p^x(1 - p)^{n-x} \\
             &= \sum_{x=1}^{n}x\frac{n!}{x!(n-x)!}p^x(1 - p)^{n-x} \\
             &= p\sum_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^{x-1}(1 - p)^{n-x} \\
             &= np\sum_{x=0}^{n-1}\frac{(n-1)!}{x!(n-x-1)!}p^x(1 - p)^{n-x-1} \\
             &= np\sum_{x=0}^{n-1}\binom{n-1}{x}p^x(1 - p)^{(n-1)-x} \\
             &= np\left(p+(1-p)\right)^{n-1} = np.
    \end{align*}
\end{exmp}

\begin{prop}
    Expectation is linear, that is $E(kX_1+X_2) = kE(X_1) + E(X_2)$.
\end{prop}

\begin{proof}
    Follows from linearity of sums.
\end{proof}

\begin{thm}{Law of the Unconscious Statistician ($\mathcal{LOTUS}$)}{\label{lotus}}\proofbreak
    Let $X$ be a discrete random variable with expected value $E(X)$. For any function $f$, \[E[f(X)] = \sum_{x}f(x)P(X=x).\]
\end{thm}

\begin{defn}
    Let $X$ be a discrete random variable. For $k \geq 1$, the $k$th \emph{moment} of $X$ is the expected value of $X^k$.
\end{defn}

\begin{rmk}
    The mean of a random variable $X$ (denoted by $\mu$) is the first moment of $X$.
\end{rmk}

\begin{exmp}
    All moments of a Bernoulli $p$ distribution are simply $p$.
\end{exmp}

\begin{defn}
    For a random variable $X$, the \emph{variance} of $X$ is the expected squared distance from the mean: \[\variance(x) = E\left[\left(X-E(X)\right)^2\right] = E\left[\left(x - \mu\right)^2\right].\]
\end{defn}

\begin{rmk}
    While the mean has the same units as $X$, the variance has measure of those units squared.
\end{rmk}

\begin{defn}
    The \emph{standard deviation} $\sigma$ of a random variable $X$ is the positive square root of the variance of $X$.
\end{defn}

\begin{prop}
    When the first and second moments of a random variable exist and are finite, the variance can be computed by $\sigma^2 = E(X^2) - E(X)^2$.
\end{prop}

\begin{proof}
    By $\mathcal{LOTUS}$ \ref{lotus}, the variance is
    \begin{align*}
        E\left[(X-E(X))^2\right] &= E\left[X^2 - 2XE(X) + E(X)^2\right] \\
        &= E(X^2) - 2E(X)E\left[X\right] + E\left[E(X)^2\right] \\
        &= E(X^2) - 2E(X)^2 + E(X)^2 = E(X^2) - E(X)^2.
    \end{align*}
\end{proof}

\begin{exmp}
    Let $X$ be a binomial $n, p$ distribution. Recall that the mean of $X$ is $np$. Note that $E(X^2) = E[X(X-1)+X]$, and by $\mathcal{LOTUS}$ \ref{lotus}
    \begin{align*}
        E(X(X-1)) &= \sum_{x=0}^{n}x(x-1)\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
        &= \sum_{x=2}^{n}\frac{n!}{(x-2)!(n-x)!}p^x(1-p)^{n-x} \\
        &= n(n-1)p^2\sum_{x=2}^{n}\frac{(n-2)!}{(x-2)!(n-x)!}p^{(x-2)}(1-p)^{n-x} \\
        &= n(n-1)\sum_{x=0}^{n-2}\frac{(n-2)!}{x!((n-2)-x)!}p^x(1-p)^{(n-2)-x} \\
        &= n(n-1)p^2\left(p+(1-p)\right)^{n-2} = n(n-1)p^2.
    \end{align*}
    Therefore, $E(X^2) = E[X(X-1)]+E[X]=n(n-1)p^2+np = n^2p^2+np-np^2$, and so the variance is $E(X^2)-E(X)^2 = n^2p^2+np-np^2-n^2p^2 = np-np^2 = n(p-p^2)$.
\end{exmp}

\begin{defn}
    Given a random variable $X$, the \emph{moment generating function} for $\theta \in \R$ is
    \begin{align*}
        M(\theta) = E(e^{\theta X}).
    \end{align*}
\end{defn}

\begin{prop}
    If the moment generating function exists for a particular random variable,
    \begin{enumerate}
        \item it uniquely identifies that random variablem i.e. it is injective,
        \item all moments of the random variable exist and are finite,
        \item the $n$th derivative of the moment generating function at $\theta = 0$ is the $n$th moment of the random variable.
    \end{enumerate}
\end{prop}

\begin{prop}\proofbreak
    3) Consider the Maclaurin series of $e^{\theta X}$, which is
    \[e^{\theta X} = \sum_{n=0}^{\infty}\frac{(\theta X)^n}{n!} = 1 + X\theta + X^2\frac{\theta^2}{2} + \cdots.\]
    Then by linearity of the expected value and by linearity of the derivative, $M^{(k)}(0) = E\left[X^k\right]$.
\end{prop}

\begin{exmp}
    Consider the Poission $\mu$ distribution, where $P(X = x) = \frac{e^{-\mu}\mu^x}{x!}$. The moment generating function, if it exists, by $\mathcal{LOTUS}$ \ref{lotus} would be given by
    \begin{align*}
       E(e^{\theta X})] &= \sum_{x=0}^{\infty}e^{\theta x}P(X = x) \\
       &= \sum_{x=0}^{\infty}e^{\theta x}\frac{e^{-\mu}\mu^x}{x!} \\
       &= e^{-\mu}\sum_{x=0}^{\infty}\frac{\left(\mu e^{\theta}\right)^{x}}{x!} \\
       &= e^{-\mu}e^{e^{\theta}\mu} = e^{\mu(e^{\theta-1})}.
    \end{align*}
\end{exmp}

\section{Cumulative Distribution Functions}

\begin{defn}
    Given a random variable $X$, the function $p(x) = P(X = x)$ is the \emph{probability mass function} (or PMF) of $X$.
\end{defn}

\begin{defn}
    Given a random variable $X$, the function $F: \R \to [0, 1]$ defined by
    \[x \mapsto P(X \leq x)\]
    is the \emph{cumulative distribution function} (or CDF) of $X$.
\end{defn}

\begin{prop}
    Given a random variable $X$ with CDF $F_X$, for any $x < y$, then $F_X(x) \leq F_X(y)$.
\end{prop}

\begin{proof}
    If $x < y$ then the event $X \leq x$ is a subset of the event $X \leq y$, so $P(X \leq x) \leq P(X \leq y)$ by Proposition \ref{probability-monotonicity}.
\end{proof}

\begin{thm}\label{event-sequence-probability}
    Consider sequences of events $E_i \in \Omega$ such that
    \begin{align*}
        E_1 \subseteq E_2 \subseteq E_3 \subseteq \cdots,
    \end{align*}
    or
    \begin{align*}
        E_1 \supseteq E_2 \supseteq E_3 \supseteq \cdots.
    \end{align*}

    Then
    \begin{align*}
        \lim_{n \to \infty}P(E_n) = P\left(\lim_{n \to \infty}E_n\right).
    \end{align*}
\end{thm}

\begin{proof}
    Consider the case of increasing events, so $E_1 \subseteq E_2 \cdots$. Let $F_n = E_n - \union_{i < n}E_i$. Then
    \begin{align*}
        \bigcup_{i=1}^{n}E_i = \bigsqcup_{i=1}^{n}F_i,\;\;\;\bigcup_{i=1}^{\infty}E_i = \bigsqcup_{i=1}^{\infty}F_i.
    \end{align*}
    Therefore,
    \begin{align*}
        P\left(\lim_{n \to \infty} E_n\right) &= P\left(\bigcup_{i=1}^{\infty}E_i\right) = P\left(\bigcup_{i=1}^{\infty}F_i\right) = \sum_{i =1}^{\infty}P(F_i) \\
        &= \lim_{n \to \infty}\sum_{i}^{n}P(F_i) = \lim_{n \to \infty} P\left(\bigsqcup_{i}^{n}F_i\right) = \lim_{n \to \infty} P\left(\bigcup_{i}^{n}E_i\right) = \lim_{n \to \infty}P(E_n).
    \end{align*}

    Now consider the case where $E_1 \supseteq E_2 \cdots$. Notice that $E_1^{c} \subseteq E_2^{c} \cdots$, so $E_i^{c}$ forms an increasing sequence of events. Therefore,
    \begin{align*}
        \lim_{n \to \infty}P(E_n^{c}) = P\left(\lim_{n \to \infty}E_n^{c}\right) = P\left(\bigcup_{i}^{\infty}E_i^{c}\right).
    \end{align*}
    Since
    \begin{align*}
        \bigcup_{i}^{\infty}E_i^{c} = \left(\bigcap_{i}^{\infty}E_i\right)^{c},
    \end{align*}
    it follows that
    \begin{align*}
        \lim_{n \to \infty}P(E_n^{c}) = P\left(\left(\bigcap_{i}^{\infty}E_i\right)^{c}\right) = 1 - P\left(\bigcap_{i}^{\infty}E_i\right).
    \end{align*}
    Since $1 - P(E_i) = P(E_i^{c})$, we then have
    \begin{align*}
        \lim_{n \to \infty}P(E_n^{c}) = \lim_{n \to \infty}\left(1 - P(E_n)\right) = 1 - \left(\lim_{n \to \infty}P(E_n)\right).
    \end{align*}
    It follows that
    \begin{align*}
        \lim_{n \to \infty}P(E_n) = P\left(\bigcap_{i}^{\infty}E_i\right) = P\left(\lim_{n \to \infty}E_n\right).
    \end{align*}
\end{proof}

\begin{prop}
    Given a random variable $X$ with CDF $F$, $F$ is right-continuous for all $x \in \R$:
    \[\forall x \in \R,\;\;\lim_{n \to\infty}F(x + \frac{1}{n}) = F(x),\]
    and left-limits exist for all $x \in \R$:
    \[\forall x \in \R,\;\;\lim_{n \to \infty}F(x - \frac{1}{n}).\]
\end{prop}

\begin{proof}
    The event that $X \leq x$ is
    \begin{align*}
        \bigcap_{n = 1}^{\infty}(X \leq x + \frac{1}{n}),
    \end{align*}
    and so by Theorem \ref{event-sequence-probability} right-continuity follows:
    \begin{align*}
        F(x) = P(X \leq x) = \lim_{n \to \infty}P(X \leq x + \frac{1}{n}) = \lim_{n \to \infty}F(x + \frac{1}{n}).
    \end{align*}

    Similarly,
    \begin{align*}
        \lim_{n \to \infty}F(x - \frac{1}{n}) = \lim_{n \to \infty}P(X \leq x - \frac{1}{n}) = P(X < x),
    \end{align*}
    so the left limits always exist. Note that $P(X < x)$ will not equal $P(X \leq x)$ if $P(X = x) > 0$, so $F$ may not be left-continuous.
\end{proof}

\begin{defn}
    A random variable with a continuous CDF is a \emph{continuous} random variable.
\end{defn}

\begin{prop}
    Given a continuous random variable $X$, $P(X = x) = 0$ for all $x$.
\end{prop}

\begin{proof}
    Let $F_X(x) = P(X \leq x)$ be the (continuous) CDF of $X$. Fix some $x$. Then for any $h > 0$,
    \begin{align*}
        P(x-h < X \leq x) = F(x) - F(x - h).
    \end{align*}
    Therefore, as $h \to 0$, $P(x - h < X \leq x) \to 0$. Therefore, $P(x < X \leq x) = 0$, and so $P(X = x) = 0$.
\end{proof}

\begin{defn}
    The \emph{probability density function} (PDF) of a continuous random variable $X$ is
    \begin{align*}
        f(x) = \lim_{h \to 0}\frac{P(x - h < X \leq x)}{h} = \lim_{h \to 0}\frac{F(x) - F(x-h)}{h}.
    \end{align*}
\end{defn}

\begin{rmk}
    Not all continuous random variables possess a PDF, since the limit may not exist. Every continuous CDF can be decomposed into a linear combination of two functions: a \emph{absolutely continuous} part and a \emph{singlular continuous} part. The Cantor function is an example of a CDF whose absolutely continuous part is zero.
\end{rmk}

\begin{thm}
    Let $F(x)$ be the CDF and $f(x)$ the PDF of an absolutely continuous random variable $X$. Then:
    \begin{enumerate}
        \item $F'(x) = f(x)$,
        \item $F(x) = \int_{-\infty}^{x}f(x)dx$,
        \item $f(x)$ exists almost everywhere,
        \item $\int_{-\infty}^{\infty}f(x)dx = 1$.
    \end{enumerate}
\end{thm}

\begin{proof}\proofbreak
    \begin{enumerate}
        \item Definition of $f(x)$,
        \item Follows from above,
        \item Follows from absolute continuity of $F(x)$,
        \item Follows from absolute continuity of $F(x)$.
    \end{enumerate}
\end{proof}

\begin{cor}
    \begin{align*}
        P(a < X \leq b) &= P(X \leq b) - P(X \leq a) \\
        &= F(b) - F(a) \\
        &= \int_{-\infty}^{b}f(x)dx - \int_{-\infty}^{a}f(x)dx \\
        &= \int_{a}^{b}f(x)dx.
    \end{align*}
\end{cor}

\begin{prop}
    A function $f: \R \to \R$ is a probability density function when $f(x) \geq 0$, $f(x)$ is defined for all $\R$, and $\int_{-\infty}^{\infty}f(x)dx = 1$.
\end{prop}

\begin{defn}
    A random variable $X$ has the \emph{exponential} distribution with parameter $\lambda > 0$ when its probability density function is
    \begin{align}
        f(x) = \begin{dcases}
            \lambda e^{-\lambda x}, & x > 0 \\
            0, & x \leq 0
        \end{dcases}.
    \end{align}
\end{defn}

\begin{exmp}
    This is a probability density function since $f(x)$ is clearly non-negative, defined for all $x \in \R$, and
    \begin{align*}
        \int_{-\infty}^{\infty}f(x)dx &= \int_{0}^{\infty}f(x)dx = \int_{0}^{\infty}\lambda e^{-\lambda x}dx \\
        &= \lambda \frac{1}{-\lambda}e^{-\lambda x}\big\rvert_{0}^{\infty} = 0 - (-e^{0}) = 1.
    \end{align*}
\end{exmp}

\begin{defn}
    The expected value of a continuous random variable $X$ with probability density function $f_X(x)$ is
    \begin{align*}
        E(X) = \int_{-\infty}^{\infty}xf_X(x)dx.
    \end{align*}
\end{defn}

\begin{exmp}
    The expected value of a random variable $X \distribution \mathrm{exp}(\lambda)$ is
    \begin{align*}
        E(X) = \int_{-\infty}^{\infty}xf_X(x)dx = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx.
    \end{align*}
    Integrating by parts, we let $u = x$ and $dv = \lambda e^{-\lambda x}dx$. Therefore, $du = dx$ and $v = -e^{-\lambda x}$. It follows that
    \begin{align*}
        \int x\lambda e^{-\lambda x}dx &= \int udv = uv - \int vdu = -xe^{-\lambda x} + \int e^{-\lambda x}dx \\
        &= -xe^{-\lambda x} - \frac{1}{\lambda}e^{-\lambda x} = -e^{-\lambda x}\left(x + \frac{1}{\lambda}\right).
    \end{align*}

    Note that since $\lambda > 0$, by L'Hopital's rule we have
    \begin{align*}
        \lim_{x \to \infty}xe^{-\lambda x} = \lim_{x \to \infty}\frac{x}{e^{\lambda x}} = \lim_{x \to \infty}\frac{1}{\lambda e^{\lambda x}} = 0.
    \end{align*}

    Therefore,
    \begin{align*}
        E(X) = \left(-e^{-\lambda x}\left(x + \frac{1}{\lambda}\right)\right)\big\rvert_{0}^{\infty} = 0 - \left(-\left(0 + \frac{1}{\lambda}\right)\right) = \frac{1}{\lambda}.
    \end{align*}

    The second moment of $X$ is
    \begin{align*}
        E(X^2) = \int_{-\infty}^{\infty}x^2f_X(x)dx = \int_{0}^{\infty}x\lambda e^{-\lambda x}dx.
    \end{align*}
    We can apply integration by parts twice to obtain $E(X^2)$
\end{exmp}

\begin{defn}
    A random variable $X$ has the \emph{uniform} distribution with parameters $a, b$ ($a < b$) when its probability density function is
    \begin{align}
        f(x) = \begin{dcases}
            0, & x \leq a \\
            \frac{1}{b - a}, & a < x < b \\
            0, & x \geq b
        \end{dcases}.
    \end{align}
\end{defn}

\begin{defn}
    The first moment (expected value) of a random variable $X \distribution \mathrm{uniform}(a, b)$ is
    \begin{align*}
        E(X) = \int_{-\infty}^{\infty}xf_X(x)dx = \int_{a}^{b}\frac{x}{b - a}dx = \frac{1}{b - a}\int_{a}^{b}xdx = \frac{b^2 - a^2}{2(b - a)} = \frac{(b - a)(b + a)}{2(b - a)} = \frac{b+a}{2}.
    \end{align*}

    The second moment is
    \begin{align*}
        E(X^2) = \int_{-\infty}^{\infty}x^2f_X(x)dx = \int_{a}^{b}\frac{x^2}{b - a}dx = \frac{b^3 - a^3}{3(b - a)} = \frac{(b-a)(b^2 + ab + a^2)}{3(b - a)} = \frac{b^2+ab+a^2}{3}.
    \end{align*}

    Therefore, the variance is
    \begin{align*}
        \sigma^2 = E(X^2) - E(X)^2 = \frac{b^2+ab+a^2}{3} - \left(\frac{b+a}{2}\right)^2 = \frac{b^2-2ab+a^2}{12}.
    \end{align*}
\end{defn}

\begin{rmk}
    When higher moments of random variables exist, there are additional quantities we can consider to understand distributions.
\end{rmk}

\begin{defn}
    The \emph{skewness} of a random variable $X$ is
    \begin{align*}
        E\left[\left(\frac{X-\mu}{\sigma}\right)^3\right]
    \end{align*}
\end{defn}

\begin{defn}
    The \emph{kurtosis} of a random variable $X$ is
    \begin{align*}
        E\left[\left(\frac{X-\mu}{\sigma}\right)^4\right]
    \end{align*}
\end{defn}

\begin{rmk}
    Skewness quantifies how skewed the distribution is --- positive skewness means it is right-tailed, and negative skewness means it is left tailed. When the skewness is zero, the distribution is perfectly symmetrical around its mean.

    Kurtosis measure how heavy the tails of the distribution are.
\end{rmk}

\begin{rmk}
    From the moment generating function, we can show that the $k$th moment of an exponentially distributed random variable is $k!\lambda^{-k}$. From there, we can calculate that the skewness of the exponential distribution is $2$ and the kurtosis is $9$, regardless of the specific value of $\lambda$.
\end{rmk}

\section{Gamma Distribution}

\begin{defn}
    The Euler gamma function is
    \begin{align*}
        \Gamma(\alpha) = \int_{0}^{\infty}x^{\alpha-1}e^{-x}dx.
    \end{align*}
    for $\alpha \in \C$ where $\alpha \neq 0$ and $\alpha$ is not a negative even integer.
\end{defn}

\begin{exmp}
    \begin{align*}
        \Gamma(1) = \int_{0}^{\infty}x^{1 - 1}e^{-x}dx = \int_{0}^{\infty}e^{-x}dx \\
        &= -e^{-x}\big\rvert_{0}^{\infty} = 1.
    \end{align*}
\end{exmp}

\begin{exmp}
    \begin{align*}
        \Gamma\left(\frac{1}{2}\right) = \int_{0}^{\infty}x^{-1/2}e^{-x}dx
    \end{align*}
    Let $x = u^2$, $dx = 2udu$. Then
    \begin{align*}
        \int_{0}^{\infty}\frac{1}{u}e^{-u^2}2udu = 2\int_{0}^{\infty}e^{-u^2}du.
    \end{align*}
    Then
    \begin{align*}
        \left(\Gamma\left(\frac{1}{2}\right)^2\right) = 4\int_{0}^{\infty}\int_{0}^{\infty}e^{-u^2}e^{-v^2}dudv.
    \end{align*}
    With a change to polar coordinates we obtain
    \begin{align*}
        \left(\Gamma\left(\frac{1}{2}\right)^2\right) = 4\int_{0}^{\pi/2}\int_{0}^{\infty}e^{-r^2}rdrd\theta = 4\int_{0}^{\pi/2}\frac{1}{2}d\theta = \pi.
    \end{align*}
    Therefore, $\Gamma(\frac{1}{2}) = \sqrt{\pi}$ since it is clear that $\Gamma(\frac{1}{2}) \geq 0$.
\end{exmp}

\begin{prop}
    For real $\alpha > 1$,
    \begin{align*}
        \Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1).
    \end{align*}
\end{prop}

\begin{proof}
    \begin{align*}
        \Gamma(\alpha) = \int_{0}^{\infty}x^{\alpha - 1}e^{-x}du.
    \end{align*}
    We can integrate this by parts to obtain
    \begin{align*}
        \Gamma(\alpha) = (\alpha - 1)\int_{0}^{\infty}x^{\alpha - 1 - 1}e^{-x}du.
    \end{align*}
    Therefore,
    \begin{align*}
        \Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1).
    \end{align*}
\end{proof}

\begin{rmk}
    $\Gamma(n+1) = n!$ for $n \in \Z$, $n \geq 0$, since $\Gamma(1) = 1$ and $\Gamma(n+1) = (n)(n-1)\cdots(3)(2)\Gamma(1)$.

    Additionally, if we know $\Gamma(x)$ for some $x \in (0, 1]$, then $\Gamma(x + n) = (x + n)(x + n - 1)\cdots(x + 1)\Gamma(x)$ for any $n \geq 0$.
\end{rmk}

\begin{prop}
    Let $f: \R \to \R_{\geq 0}$ have a finite integral. Then
    \begin{align*}
        g(x) = \frac{f(x)}{\int_{-\infty}^{\infty}f(x)dx}
    \end{align*}
    is a probability density function. This method of converting functions into a probability density function is called \emph{normalization}.
\end{prop}

\begin{exmp}
    Consider
    \begin{align*}
        g(x) = x^{a-1}e^{-x/\beta}
    \end{align*}
    for $x > 0$, and $g(x) = 0$ elsewhere for any $\alpha, \beta > 0$.

    With the substitution $u = x/\beta$,
    \begin{align*}
        \int_{-\infty}^{\infty}g(x)dx = \int_{0}^{\infty}(\beta u)^{\alpha -1}e^{-u}\beta du = \beta^{\alpha}\Gamma(\alpha).
    \end{align*}

    We can then normalize $g(x)$ to obtain the probability density function
    \begin{align*}
        f(x) = \begin{dcases}
            \frac{x^{a-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}, & x > 0 \\
            0, & x \leq 0
        \end{dcases}.
    \end{align*}

    This defines the Gamma$(\alpha, \beta)$ distribution, where $\alpha$ is the \emph{shape} parameter and $\beta$ is the \emph{scale} parameter.
\end{exmp}

\begin{rmk}
    The fact that the Gamma distribution's PDF integrates to $1$ can be used to solve other integrals. For example, consider.
    \begin{align*}
        \int_{0}^{\infty}x^{1/2}e^{-2x}dx.
    \end{align*}
    Let $f(x)$ be the PDF of the Gamma$(3/2, 1/2)$ distribution. Then
    \begin{align*}
        \int_{0}^{\infty}x^{1/2}e^{-2x}dx &= (1/2)^{3/2}\Gamma(3/2)\int_{0}^{\infty}\frac{x^{1/2}e^{-2x}}{(1/2)^{3/2}\Gamma(3/2)}dx = (1/2)^{3/2}\Gamma(3/2)\int_{0}^{\infty}f(x)dx \\
        &= \sqrt{\frac{1}{8}} \cdot \frac{1}{2}\Gamma\left(\frac{1}{2}\right) \cdot 1 = \frac{\sqrt{\pi}}{4\sqrt{2}}.
    \end{align*}
\end{rmk}

\begin{rmk}
    When $\alpha = 1$, the Gamma$(\alpha, \beta)$ distribution becomes the exp$\left(\frac{1}{\beta}\right)$ distribution.
\end{rmk}

\begin{prop}
    The mean of $X \distribution \mathrm{Gamma}(\alpha, \beta)$ is $\alpha\beta$.
\end{prop}

\begin{proof}
    \begin{align*}
        E(X) &= \int_{-\infty}^{\infty}xf_X(x)dx = \int_{0}^{\infty}x\left(\frac{x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\right)dx = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha}e^{-x/\beta}dx \\
        &= \frac{1}{\beta^{\alpha}\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha}e^{-x/\beta}dx = \frac{\beta^{\alpha+1}\Gamma(\alpha+1)}{\beta^{\alpha}\Gamma(\alpha)} = \beta\alpha.
    \end{align*}
\end{proof}

\begin{prop}
    The variance of $X \distribution \mathrm{Gamma}(\alpha, \beta)$ is $\alpha \beta^2$.
\end{prop}

\begin{proof}
    \begin{align*}
        E(X^2) &= \int_{-\infty}^{\infty}x^2f_X(x)dx = \int_{0}^{\infty}x^2\left(\frac{x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\right)dx = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha+1}e^{-x/\beta}dx \\
        &= \frac{1}{\beta^{\alpha}\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha+1}e^{-x/\beta}dx = \frac{\beta^{\alpha+2}\Gamma(\alpha+2)}{\beta^{\alpha}\Gamma(\alpha)} = \beta^2(\alpha+1)\alpha.
    \end{align*}

    The variance of $X \distribution \mathrm{Gamma}(\alpha, \beta)$ is therefore
    \begin{align*}
        E(X^2) - E(X)^2 = \beta^2(\alpha+1)\alpha - \alpha^2\beta^2 = \alpha\beta^2.
    \end{align*}
\end{proof}

\begin{prop}
    The moment generating function of $X \distribution \mathrm{Gamma}(\alpha, \beta)$ is
    \begin{align*}
        \left(1 - \beta\theta\right)^{-\alpha}.
    \end{align*}
\end{prop}

\begin{proof}
    \begin{align*}
        M(\theta) &= E(e^{\theta X}) = \int_{0}^{\infty}e^{\theta x}\frac{x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}dx \\
        &= \frac{1}{\beta^{\alpha}\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha-1}e^{-x(1/\beta-\theta)}dx = \frac{\left[\left(\frac{1}{\beta}-\theta\right)^{-1}\right]^{\alpha}}{\beta^{\alpha}} = (1 - \beta\theta)^{-\alpha}.
    \end{align*}
\end{proof}

\section{Normal Distribution}

\begin{defn}
    A continuous random variable $X$ with the \emph{Normal} (or \emph{Gaussian}) distribution with parameters $\mu, \sigma$ has the probability density function
    \begin{align*}
        f(x) = \frac{e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}}{\sigma\sqrt{2\pi}} = \frac{e^{-\frac{\left(x - \mu\right)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}.
    \end{align*}
    We write $X \distribution \mathrm{Normal}(\mu, \sigma^2)$.
\end{defn}

\begin{defn}
    The \emph{standard normal distribution} is the normal distribution with $\mu = 0$ and $\sigma = 1$, and has probability density function
    \begin{align*}
        \varphi(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}.
    \end{align*}
\end{defn}

\begin{rmk}\proofbreak
    \begin{itemize}
        \item For large $n$ and fixed $p$, the binomial$(n, p)$ distribution approximates the Normal distribution.
        \item For large $\lambda$, the Poisson distribution approximates the Normal distribution.
        \item For large $\alpha$ and fixed $\beta$, the Gamma distribution approximates the Normal distribution.
    \end{itemize}
\end{rmk}

\begin{thm}
    If $X \distribution \mathrm{Normal}(\mu, \sigma^2)$ and $Y = \alpha X + \beta$ where $\alpha \neq 0$, then
    \begin{align*}
        Y \distribution \mathrm{Normal}(\alpha\mu + \beta, \alpha^2\sigma^2).
    \end{align*}
\end{thm}

\begin{proof}
    Assume $\alpha > 0$. Then the CDF of $Y$ is $F_Y(y) = P(Y \leq y)$, which is $P(\alpha X + \beta \leq y)$ and so
    \begin{align*}
        F_Y(y) = P\left(X \leq \frac{y - \beta}{\alpha}\right) = \int_{-\infty}^{\frac{y-\beta}{\alpha}}\frac{e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}}{\sqrt{2\pi\sigma^2}}dx.
    \end{align*}
    Therefore,
    \begin{align*}
        f_Y(y) &= \frac{d}{dy}F_Y(y) = \frac{d}{dy}\int_{-\infty}^{\frac{y-\beta}{\alpha}}\frac{e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}}{\sqrt{2\pi\sigma^2}}dx \\
        &= \frac{e^{-\frac{1}{2}\left(\frac{\frac{y-\beta}{\alpha} - \mu}{\sigma}\right)^2}}{\sqrt{2\pi\sigma^2}}\frac{d}{dy}\frac{y - \beta}{\alpha} = \frac{e^{-\frac{1}{2}\left(\frac{y - \beta - \alpha\mu}{\alpha\sigma}\right)^2}}{\alpha\sqrt{2\pi\sigma^2}} = \frac{e^{-\frac{1}{2}\left(\frac{y - \left(\alpha\mu + \beta\right)}{\sigma}\right)^2}}{\sqrt{2\pi\alpha^2\sigma^2}}.
    \end{align*}
    The proof for the case $\alpha < 0$ is nearly identical.
\end{proof}

\begin{cor}\proofbreak
    \begin{itemize}
        \item If $X \distribution \mathrm{Normal}(\mu, \sigma^2)$, then $Z = \frac{X - \mu}{\sigma} \distribution \mathrm{Normal}(0, 1)$.
        \item If $Z \distribution \mathrm{Normal}(0, 1)$, then $X = \mu + \sigma Z \distribution \mathrm{Normal}(\mu, \sigma^2)$.
    \end{itemize}
\end{cor}

\begin{lemma}\label{gaussian-integral}
    \begin{align*}
        \int_{-\infty}^{\infty}e^{-x^2}dx = \sqrt{\pi}.
    \end{align*}
\end{lemma}

\begin{proof}
    Notice that $x^2 = (-x)^2$, and so $e^{-x^2}$ is even. Therefore,
    \begin{align*}
        \int_{-\infty}^{\infty}e^{-x^2}dx = 2\int_{0}^{\infty}e^{-x^2}dx.
    \end{align*}
    Let $t = x^2$, so $dt = 2xdx = 2\sqrt{t}dx$. Then
    \begin{align*}
        2\int_{0}^{\infty}e^{-x^2}dx &= \int_{0}^{\infty}x^{-1/2}e^{-x^2}2x^{1/2}dx = \int_{0}^{\infty}t^{-1/2}e^{-t}dt \\
        &= 1^{1/2}\Gamma(1/2)\int_{0}^{\infty}\frac{t^{-1/2}e^{-t}}{1^{1/2}\Gamma(1/2)}dt = \Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}.
    \end{align*}
\end{proof}

\begin{thm}
    The moment generating function of $Z \distribution \mathrm{Normal}(0, 1)$ is
    \begin{align*}
        E\left(e^{\theta Z}\right) = e^{\frac{\theta^2}{2}}.
    \end{align*}
\end{thm}

\begin{proof}
    \begin{align*}
        E\left(e^{\theta Z}\right) &= \int_{-\infty}^{\infty}e^{\theta z}\frac{e^{-\frac{z^2}{2}}}{\sqrt{2\pi}}dz = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^2}{2}+\theta}dz.
    \end{align*}
    Note that from $\theta z - \frac{z^2}{2}$, we can take $2\theta z - z^2$ and completing the square into $(z - \theta)^2$, obtaining
    \begin{align*}
        \theta z - \frac{z^2}{2} = -\frac{1}{2}\left(z - \theta\right)^2 + \frac{\theta^2}{2}.
    \end{align*}
    Therefore,
    \begin{align*}
        E\left(e^{\theta Z}\right) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^2}{2}+\theta}dz = \frac{1}{\sqrt{2\pi}}e^{\theta^2/2}\int_{-\infty}^{\infty}e^{-\frac{1}{2}\left(z - \theta\right)^2}dz.
    \end{align*}
    Now we make the substitution $u = \frac{z - \theta}{\sqrt{2}}$, so $du = \frac{1}{\sqrt{2}}dz$. By Lemma \ref{gaussian-integral}, we know that
    \begin{align*}
        \int_{-\infty}^{\infty}e^{-u^2}du = \sqrt{\pi},
    \end{align*}
    and so
    \begin{align*}
        \int_{-\infty}^{\infty}e^{-\frac{1}{2}\left(z - \theta\right)^2}dz = \sqrt{2\pi}.
    \end{align*}
    Therefore,
    \begin{align*}
        E\left(e^{\theta Z}\right) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^2}{2}+\theta}dz = \frac{1}{\sqrt{2\pi}}e^{\theta^2/2}\sqrt{2\pi} = e^{\frac{\theta^2}{2}}.
    \end{align*}
\end{proof}

\begin{cor}
    Let $X \distribution \mathrm{Normal}(\mu, \sigma^2)$, then
    \begin{itemize}
        \item $E(Z) = 0$,
        \item $E(Z^2) = 1$,
        \item $E(X) = \mu$, and
        \item $\variance{X} = \sigma^2$.
    \end{itemize}
\end{cor}

\begin{proof}
    \begin{align*}
        E(Z) &= M_{Z}'(0) = \frac{d}{d\theta}\big\rvert_{\theta=0}e^{\theta^2/2} = \left[\theta e^{\theta^2/2}\right]_{\theta = 0} = 0 \\
        E(Z^2) &= M_{Z}''(0) = \frac{d^2}{d\theta^2}\big\rvert_{\theta=0}e^{\theta^2/2} = \left[\theta^2 e^{\theta^2/2} + e^{\theta^2/2}\right]_{\theta = 0} = 1.
    \end{align*}
    Then,
    \begin{align*}
        E(X) &= E(\sigma Z + \mu) = \sigma E(Z) + \mu = \mu \\
        E(X^2) &= E(\sigma^2Z^2 + \sigma\mu Z + \mu^2) = \sigma^2E(Z^2) + \sigma\mu E(Z) + \mu^2 = \sigma^2 + \mu^2 \\
        \variance{X} &= E(X^2) - E(X)^2 = \sigma^2 + \mu^2 - \mu^2 = \sigma^2.
    \end{align*}
\end{proof}

\section{Transformations of Continuous Random Variables}

\begin{defn}{CDF Method}\label{cdf-method}\proofbreak
    Given random variables $X$ and $Y$ such that $Y = g(X)$ and the CDF of $X$ is known, we can find the CDF and PDF of $Y$ as follows:
    \begin{itemize}
        \item Determine the support of $Y$.
        \item Compute $F_Y(y) = P(Y \leq y) = P(g(X) \leq y)$.
        \item Finally, the PDF of $Y$ is $f_Y' = \frac{d}{dy}F_Y(y)$.
    \end{itemize}
\end{defn}

\begin{exmp}
    Let $Z \distribution \mathrm{Normal}(0, 1)$, and $X = Z^2$. Then the support of $X$ is the non-negative real numbers, and so
    \begin{align*}
        F_X(x) &= P(X \leq x) = P(Z^2 \leq x) = P(\abs{Z} \leq \sqrt{x}) \\
        &= P(-\sqrt{x} \leq Z \leq \sqrt{x}) = P(Z \leq \sqrt{x}) - P(Z \leq -\sqrt{x}) = \varPhi(\sqrt{x}) - \varPhi(-\sqrt{x}).
    \end{align*}
    for $x \geq 0$ and $F_X(x) = 0$ otherwise. Finally, for $x > 0$,
    \begin{align*}
        f_X(x) &= F_X'(x) = \frac{d}{dx}\varPhi(\sqrt{x}) - \frac{d}{dx}\varPhi(-\sqrt{x}) = \frac{1}{2\sqrt{x}}\left[\varphi(\sqrt{x}) + \varphi(-\sqrt{x})\right] \\
        &= \frac{1}{2\sqrt{x}}\left[\frac{e^{-\frac{x}{2}}}{\sqrt{2\pi}} + \frac{e^{-\frac{x}{2}}}{\sqrt{2\pi}}\right] = \frac{e^{-\frac{x}{2}}}{\sqrt{2\pi x}}.
    \end{align*}
    Note that this is the PDF of a continuous random variable with the Gamma$(1/2, 2)$ distribution.
\end{exmp}

\begin{rmk}
    The $\chi^2$ (\emph{chi-square}) distribution with $k$ degrees of freedom is the Gamma$\left(\frac{k}{2}, 2\right)$ distribution. 
\end{rmk}

\begin{thm}\label{single-variable-jacobian-method}
    Let $X, Y$ be continuous random variables such that $Y = g(X)$, where $g$ is \emph{strictly} monotonic. Then,
    \begin{align*}
        f_Y(y) = f_X(g^{-1}(y))\abs{\frac{d}{dy}g^{-1}(y)}.
    \end{align*} 
\end{thm}

\begin{proof}
    Recall that if $g$ is a strictly increasing (or decreasing) function that is smooth, then $g^{-1}$ exists and is also strictly increasing (or decreasing respectively).

    We first consider only the case where $g$ is strictly increasing, so $\frac{d}{dy}g^{-1} > 0$.
    \begin{align*}
        F_Y(y) &= P(Y \leq y) = P(g(X) \leq y) \\
        &= P(X \leq g^{-1}(y)) = F_X(g^{-1}(y)),
    \end{align*}
    and so
    \begin{align*}
        f_Y(y) &= \frac{d}{dx}F_X(g^{-1}(y)) = f_X\left(g^{-1}(y)\right)\frac{d}{dy}g^{-1}(y).
    \end{align*}

    Now we consider when $g$ is strictly decreasing, so $\frac{d}{dy}g^{-1} < 0$.
    \begin{align*}
        F_Y(y) &= P(Y \leq y) = P(g(X) \leq y) = P(g(X) \leq y) \\
        &= P(X \geq g^{-1}(y)) = 1 - P(X \leq g^{-1}(y)) = 1 - F_X(g^{-1}(y)),
    \end{align*}
    and so
    \begin{align*}
        f_Y(y) &= \frac{d}{dx}\left[1-F_X(g^{-1}(y))\right] = -f_X\left(g^{-1}(y)\right)\frac{d}{dy}g^{-1}(y) = f_X(g^{-1}(y))\abs{\frac{d}{dy}g^{-1}(y)}.
    \end{align*}
\end{proof}

\begin{exmp}
    Let $X \distribution \mathrm{Normal}(\mu, \sigma^2)$, and $Z = \frac{X - \mu}{\sigma}$ for some $\sigma > 0 0$. Note that $g(x) = \frac{x - \mu}{\sigma}$ is strictly increasing (or decreasing if $\sigma < 0$), and $g^{-1}(z) = \sigma z + \mu$. Then by Theorem \ref{single-variable-jacobian-method},
    \begin{align*}
        f_Z(z) = f_X(\sigma z + \mu)\frac{1}{\sigma} = \frac{e^{-\frac{1}{2}\left(\frac{\sigma z + \mu - \mu}{\sigma}\right)^2}}{\sigma\sqrt{2\pi}}\frac{1}{\sigma} = \frac{e^{-\frac{1}{2}z^2}}{\sqrt{2\pi}}.
    \end{align*}
\end{exmp}

\begin{defn}
    Discrete random variables $X_1, X_2, \ldots, X_n$ are \emph{independent} when
    \begin{align*}
        P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = P(X_1 = x_1)P(X_2 = x_2)\cdots P(X_n = x_n).
    \end{align*}
\end{defn}

\begin{defn}
    Continuous random variables $X_1, X_2, \ldots, X_n$ are \emph{independent} when
    \begin{align*}
        f_{X_1,X_2,\ldots,X_n}(x_1, x_2, \ldots, x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\cdots f_{X_n}(x_n).
    \end{align*}
\end{defn}

\begin{prop}
    Let $X, Y$ be continuous random variables, and consider $U = X + Y$. If $X$ and $Y$ are independent,
    $f_U(u) = (f_X * f_Y)(u)$. Similarly, if $X$ and $Y$ are independent discrete random variables,
    $P_U(u) = (P_X * P_Y)(u)$.
\end{prop}

\begin{proof}
    \begin{align*}
        F_U(u) &= P(U \leq u) = P(X + Y \leq u) \\
        &= \int_{-\infty}^{\infty}\int_{-\infty}^{u-x}f_{X,Y}(x,y)dydx,
    \end{align*}
    and so
    \begin{align*}
        f_U(u) &= \frac{d}{du}\int_{-\infty}^{\infty}\int_{-\infty}^{u-x}f_{X,Y}(x,y)dydx = \int_{-\infty}^{\infty}\frac{\partial}{\partial u}\int_{-\infty}^{u-x}f_{X,Y}(x,y)dydx \\
        &= \int_{-\infty}^{\infty}f_{X,Y}(x,u-x)dx.
    \end{align*}
    In the case that $X,Y$ are independent, we know that $f_{X,Y}(x,u-x) = f_X(x)f_Y(u-x)$, so
    \begin{align*}
        f_U(u) = \int_{-\infty}^{\infty}f_X(x)f_Y(u-x)dx = (f_X * f_Y)(u).
    \end{align*}
\end{proof}

\begin{exmp}
    Let $X,Y \distribution \mathrm{exp}(1)$ be independent continuous random variables. Then the PDF of $U = X+Y$ is
    \begin{align*}
        \int_{-\infty}^{\infty}f_X(x)f_Y(u-x)dx = \int_{0}^{u}e^{-x}e^{-(u-x)}dx = e^{-u}\int_{0}^{u}dx = ue^{-u}.
    \end{align*}
    Therefore, $U \distribution \Gamma(2, 1)$.
\end{exmp}

\begin{exmp}
    Let $X \distribution \mathrm{binomial}(n,p)$ and $Y \distribution \mathrm{binomial}(m, p)$ be independent discrete random variables. Then the PMF of $U = X+Y$ is
    \begin{align*}
        \sum_{x=0}^{u}P_X(x)P_Y(u-x) &= \sum_{x=0}^{u}\binom{n}{x}p^{x}(1-p)^{n-x}\binom{m}{u-x}p^{u-x}(1-p)^{m+x-u} \\
        &= \sum_{x=0}^{u}\binom{n}{x}\binom{m}{u-x}p^{u}(1-p)^{n+m-u} = p^{u}(1-p)^{n+m-u}\sum_{x=0}^{u}\binom{n}{x}\binom{m}{u-x} \\
        &= \binom{n+m}{u}p^{u}(1-p)^{n+m-u}.
    \end{align*}
    Therefore, $U \distribution \mathrm{binomial}(n+m, p)$.
\end{exmp}

\begin{thm}
    Let $X_1, X_2, \ldots, X_n, Y_1, Y_2, \ldots, Y_k$ be independent random variables. Then for any functions $h_1, h_2$,
    \begin{align*}
        H_1 &= h_1(X_1, \ldots, X_n) \\
        H_2 &= h_2(Y_1, \ldots, Y_k)
    \end{align*}
    are independent.
\end{thm}

\begin{thm}
    Let $X$ and $Y$ be independent random variables with moment generating functions $M_X(\theta), M_Y(\theta)$. Then
    \begin{align*}
        M_{X+Y}(\theta) = M_X(\theta)M_Y(\theta).
    \end{align*}
\end{thm}

\begin{proof}
    \begin{align}
        M_{X+Y}(\theta) = E\left(e^{\theta(X+Y)}\right) = E\left(e^{\theta X}e^{\theta Y)}\right) = E\left(e^{\theta X}\right)E\left(e^{\theta Y)}\right) = M_X(\theta)M_Y(\theta).
    \end{align}
\end{proof}

\begin{rmk}
    If $U$ and $V$ are random variables with moment generating functions that agree in an open neighborhood of $\theta = 0$, then $U$ and $V$ have the same distribution.
\end{rmk}

\section{Ordered Statistics}

Let $X_1, X_2, \ldots, X_n$ be independent and identically distributed random variables with PDF $f_X(x)$ and CDF $F_X(x)$.

Let $Y_n = \mathrm{max}(X_1, \ldots, X_n)$ and $Y_1 = \mathrm{min}(X_1, \ldots, X_n)$.

Note that the event $Y_n \leq u$ is equivalent to $X_i \leq u$, so $F_{Y_n}(u) = F_X(u)^{n}$, and so $f_{Y_n}(u) = nF_X(n)^{n-1}f_X(u)$.

Similarly, $Y_1 \leq u = 1 - Y_1 \geq u = 1 - X_i \geq u$, so $F_{Y_1}(u) = 1 - \left(1 - F_X(u)\right)^{n}$. Therefore, $f_{Y_1}(u) = n\left(1-F_X(u)\right)^{n-1}f_x(u)$.

Let $Y_j$ be the $j$th smallest value of $X_1, X_2, \ldots, X_n$. Recall that, by definition,
\begin{align*}
    f_{Y_j}(y) = \lim_{h \to 0^{+}}\frac{P(y-h < Y_j \leq y)}{h}.
\end{align*}
Fix some $0 < h << 1$. Then $y-h < Y_j \leq y$ is equivalent to having $j-1$ of $X_i$ strictly less than $y-h$, one of $X_i$ must be between $y-h$ and $y$, and $n-j$ of $X_i$ strictly greater than $y$. This probability is
\begin{align*}
    \binom{n}{j-1}F(y-h)^{j-1}\binom{n-(j-1)}{1}\left(F(y)-F(h)\right)\binom{n-j}{n-j}\left(1-F(y)\right)^{n-j} \\
    = \frac{n!}{(j-1)!(n-j)!}F(y-h)^{j-1}\left(1-F(y)\right)^{n-j}\left(F(y)-F(h)\right),
\end{align*}
and so
\begin{align*}
    f_{Y_j}(y) &= \lim_{h \to 0^{+}}\frac{P(y-h<Y_j\leq y)}{h} \\
    &= \lim_{h \to 0^{+}}\frac{n!}{(j-1)!(n-j)!}F(y-h)^{j-1}\left(1-F(y)\right)^{n-j}\frac{\left(F(y)-F(h)\right)}{h} \\
    &= \frac{n!}{(j-1)!(n-j)!}F(y-h)^{j-1}\left(1-F(y)\right)^{n-j}f_X(y).
\end{align*}

\chapter{Probability}
\label{ch:probability}

\section{Axioms of Probability}

\begin{defn}\proofbreak
    \begin{itemize}
        \item Sample point: a possible outcome of a probabilistic experiment, often denoted by $\omega$.
        \item Sample space: the set of all sample points, often denoted by $\Omega$.
        \item Event: any subset of the sample space.
    \end{itemize}
\end{defn}

\begin{defn}
    If events $A_i \subseteq \Omega$ are disjoint, we say that these events are \emph{mutually exclusive}.
\end{defn}

\begin{defn}\label{kolmogorov-probability-axioms}
    A \emph{probability space} is a sample space $\Omega$ together with a function $P: \mathcal{P}(\Omega) \to \R$ that satisfies the following axioms:
    \begin{itemize}
        \item Non-negativity: for any event $A \subseteq \Omega$, $P(A) \geq 0$.
        \item Normalization: $P(\Omega) = 1$.
        \item Countable-additivity: if $A_i$ are a countable sequence of mutually exclusive events, then $P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i)$.
    \end{itemize}
\end{defn}

\begin{rmk}
    These axioms are due to Andrey Kolmogorov.
\end{rmk}

\begin{prop}
    Let $(\Omega, P)$ be a probability space, and $A \subseteq \Omega$ an event. Then $P(A) + P(A^{c}) = 1$.
\end{prop}

\begin{proof}
    Since $A$ and $A^{c}$ are mutually exclusive, $P(A) + P(A^{c}) = P(A \union A^{c}) = P(\Omega) = 1$.
\end{proof}

\begin{cor}
    $P(\emptyset) = 0$.
\end{cor}

\begin{prop}Monotonicity\label{probability-monotonicity}\proofbreak
    Let $A, B \subseteq \Omega$ be events such that $A \subseteq B$. Then $P(A) \leq P(B)$.
\end{prop}

\begin{proof}
    Let $C = B - A$. Then $A \union C = B$ and $A \intersection C = \emptyset$. Since $A$ and $C$ are therefore mutually exclusive, $P(A) + P(C) = P(A \union C) = P(B)$. Since $P(C) \geq 0$ by the axiom of non-negativity, it follows that $P(A) \leq P(B)$.
\end{proof}

\begin{thm}{Inclusion-exclusion Principle}\label{inclusion-exclusion}
    Let $A, B, C \subseteq \Omega$ be events. Then
    \[P(A \union B) = P(A) + P(B) - P(A \intersection B),\]
    and
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) \\
                               &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
                               &+ P(A \intersection B \intersection C).
    \end{align*}
\end{thm}

\begin{proof}
    Since $A \union B = (A - B) \union (B - A) \union (A \intersection B)$, which are necessarily mutually exclusive events, we have
    \[P(A \union B) = P(A - B) + P(B - A) + P(A \intersection B).\]
    Now note that $A = (A - B) \union (A \intersection B)$ and $B = (B - A) \union (A \intersection B)$, and so $P(A) = P(A - B) + P(A \intersection B)$ and $P(B) = P(B - A) + P(A \intersection B)$. Therefore, \begin{align*}
        P(A \union B) &= P(A - B) + P(B - A) + P(A \intersection B) \\
                      &= \big[P(A - B) + P(A \intersection B)\big] + \big[P(B - A) + P(A \intersection B)\big] - P(A \intersection B) \\
                      &= P(A) + P(B) - P(A \intersection B).
    \end{align*}

    To prove the three-way version of the principle, we can apply the two-way version to $A \union B$ and $C$. This gives us
    \begin{align*}\label{inclusion-exclusion-three-intermediate}\tag{$1$}
        P(A \union B \union C) &= P(A \union B) + P(C) - P((A \union B) \intersection C) \\
        &= \big[P(A) + P(B) - P(A \intersection B)\big] + P(C) - P((A \union B) \intersection C).
    \end{align*}
    Since $(A \union B) \intersection C = (A \intersection C) \union (B \intersection C)$, we can apply the principle again to find that
    \[P((A \union B) \intersection C) = P(A \intersection C) + P(B \intersection C) - P((A \intersection C) \intersection (B \intersection C)).\] Noting that $(A \intersection C) \intersection (B \intersection C) = A \intersection B \intersection C$ and substituting this back into \ref{inclusion-exclusion-three-intermediate}, we obtain
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) - P(A \intersection B) - \big[P(A \intersection C) + P(B \intersection C) - P(A \intersection B \intersection )\big] \\
        &= P(A) + P(B) + P(C) \\
        &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
        &+ P(A \intersection B \intersection C).
    \end{align*}
\end{proof}

\begin{prop}
    \[A \union B = A \disjointunion (B - A).\]
    \[A \union B \union C = A \disjointunion (B - A) \disjointunion (C - (A \union B)).\]
\end{prop}

\begin{exmp}{Finite, equally-likely probability law}\proofbreak
    Let $(\Omega, P)$ be a sample space together with a function $P: \mathcal{P}(\Omega) \to \R$, where $\abs{\Omega} \in \Z^{+}$ and for all $A \in \mathcal{\Omega}$, \[P(A) = \frac{\abs{A}}{\abs{\Omega}}.\]

    Since $\abs{A} \geq 0$ and $\abs{\Omega} > 0$, $P(A) \geq 0$ and so this model satifies the non-negativity axiom. Since $P(\Omega) = \frac{\abs{\Omega}}{\abs{\Omega}} = 1$, this model satisfies the normalization axiom.
    
    Let $A = A_1 \disjointunion A_2 \disjointunion \cdots$. Then
    \begin{align*}
        P(A) = \frac{\abs{A_1 \disjointunion A_2 \disjointunion \cdots}}{\abs{\Omega}} = \frac{\abs{A_1}}{\abs{\Omega}} + \frac{\abs{A_2}}{\abs{\Omega}} + \cdots = P(A_1) + P(A_2) + \cdots,
    \end{align*}
    and so this model also satisfies the countable-additivity axiom. Therefore, it is a probability space.
\end{exmp}

\begin{exmp}{Discrete probability law}\proofbreak
    Let $\Omega$ be a sample space such that $\abs{\Omega}$ is countable, and let $M: \Omega \to R$ be a mass-function: $M(\omega) \geq 0$ for all $\omega \in \Omega$, and $\sum_{\omega \in \Omega}M(\omega) = 1$.

    Now consider $P: \mathcal{P}(\Omega) \to \R$ defined by
    \[P(A) = \sum_{\omega \in A}M(\omega).\]

    Since $M(\omega) \geq 0$, $P(A) \geq$ and so $P$ satisfies the axiom of non-negativity. Since \[P(\Omega) = \sum_{\omega \in \Omega}M(\omega) = 1\] by the definition of $M$, $P$ satisfies the axiom of normalization. Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i) = \sum_{i}\sum_{\omega \in A_i}M(\omega) = \sum_{\omega \in A}M(\Omega) = P(A).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}


\begin{exmp}{Geometric probability law}\proofbreak
    Let $\Omega = \{\omega_1, \omega_2, \ldots\}$ be a sample space such that $\abs{\Omega} = \abs{\Z}$, and define $P: \mathcal{P}(\Omega) \to \R$ by
    \[P(A) = \sum_{\omega_i \in A}\left(\frac{1}{2}\right)^{i}.\]
    
    Since $\left(\frac{1}{2}\right)^i \geq 0$, $P(A) \geq 0$ and so $P$ satisfies the axiom of non-negativity. Since
    \begin{align*}
        P(\Omega) = \sum_{\omega_i \in \Omega}\left(\frac{1}{2}\right)^{i} = \sum_{i=1}^{\infty}\left(\frac{1}{2}\right)^{i} = \frac{1/2}{1 - 1/2} = 1,
    \end{align*}
    we know that $P$ satisfies the axiom of normalization.

    Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{\omega_j \in A}\left(\frac{1}{2}\right)^{j} = \sum_{i}\sum_{\omega_j \in A_i}\left(\frac{1}{2}\right)^{j} = \sum_{i}P(A_i).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}

\section{Conditional Probability}

\begin{exmp}
    Roll two six-sided dice, and only look at one of them. It is a six. Let $S$ denote the event that the sum of the values of the two dice is seven, and $A$ is the event that at least one of dice was a six. What is the probability of $P(S | A)?$ It is $\frac{2}{11}$, not $\frac{1}{6}$.
\end{exmp}

\begin{defn}
    Let $A, B \in Omega$. Then the probability that $A$ occurred, given that $B$ occured, is denoted by $P(A | B)$ and read as ``the probability of $A$ given $B$''.
\end{defn}

\begin{prop}
    If all sample points in $\Omega$ are equally likely, then $P(A | B) = \frac{P(A \intersection B)}{P(B)}$.
\end{prop}

\begin{cor}
    $P(A \intersection B) = P(A | B)P(B)$.
\end{cor}

\begin{thm}
    Let $(\Omega, P)$ be a probability space, and $B \subseteq \Omega$ an event such that $P(B) > 0$. Then $(B, P(\cdot | B))$ form a probability space.
\end{thm}

\begin{proof}
    To show that $(B, P(\cdot|B))$ is a probability space, we need to show non-negativity (that $P(A | B) \geq 0$ for all $A$), normalization (that $P(B|B) = 1$), and countable additivity (that $P(A_1 \disjointunion A_2 \disjointunion \cdots | B) = P(A_1 | B) + P(A_2 | B) + \cdots$ if $A_i$ is a countable sequence).

    For any $A \subseteq B$, we know that $P(A \intersection B), P(B) \geq 0$, and so \[P(A | B) = \frac{P(A \intersection B)}{P(B)} \geq 0.\]

    \[P(B | B) = \frac{P(B \intersection B)}{P(B)} = \frac{P(B)}{P(B)} = 1.\]

    \begin{align*}
        P(A_1 \disjointunion A_2 \disjointunion \cdots | B) &= \frac{P\left((A_1 \disjointunion A_2 \disjointunion \cdots) \intersection B\right)}{P(B)} = \frac{P(A_1 \intersection B) \disjointunion (A_2 \intersection B) \disjointunion \cdots)}{P(B)} \\
        &= \frac{P(A_1 \intersection B) + P(A_2 \intersection B) + \cdots)}{P(B)} = \frac{P(A_1 \intersection B)}{P(B)} + \frac{P(A_2 \intersection B)}{P(B)} + \cdots \\
        &= P(A_1 | B) + P(A_2 | B) + \cdots.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a sequence of coin tosses. Let $B$ be the event that the first head of the sequence occurs on an odd numbered toss (the first toss is \#1), and let $A$ be the event that the event first coin toss lands on heads. What is $P(A | B)$?

    Since $A \subseteq B$, we know that $A \intersection B = A$. Furthermore, $P(A) = \frac{1}{2}$, and $P(B) = \frac{1}{2} + \frac{1}{8} + \cdots = \frac{1/2}{1-1/4} = \frac{1}{2}\cdot\frac{4}{3} = \frac{2}{3}$. Therefore, \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1}{2}\cdot\frac{3}{2} = \frac{3}{4}.\]
\end{exmp}

\begin{thm}Law of Total Probability\label{total-probability}\proofbreak
    Let $B_1, \ldots, B_n \in Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \[P(A) = \sum_{i=1}^{n}P(A \intersection B_i) = \sum_{i=1}^{n}P(A | B_i)P(B_i).\]
\end{thm}

\begin{proof}
    Note that if $x \in (A \intersection B_i)$ and $x \in (A \intersection B_j)$, then $x \in B_i$ and $x \in B_j$. Since $B_1, \ldots, B_n$ are mutually exclusive they are disjoint, and so we must have $i = j$. Therefore, $A \intersection B_1, A \intersection B_2, \ldots, A \intersection B_n$ are disjoint.

    Also note that
    \begin{align*}
        \disjointunionbig_{i=1}^{n}A \intersection B_i = A \intersection \left(\disjointunionbig_{i=1}^{n}B_i\right) = A \intersection \Omega = A
    \end{align*}
    since $B_1, \ldots, B_n$ are exhaustive.

    Therefore, by the axiom of countable additivity,
    \begin{align*}
        P(A) = P\left(\disjointunionbig_{i=1}^{n}A \intersection B_i\right) = \sum_{i=1}^{n}P(A \intersection B_i).
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a box containing six coins. Three of these coins are fair, but one of them has two tails, and the remaining two have two heads each. If we randomly (uniformly) select one coin and flip it twice, what is the probability it comes up heads twice?

    Let $A$ be the event we observe two heads. Let $B_1$ be the event we selected one of the fair coins, $B_2$ the event we selected the $2$-tailed coin, and $B_3$ the event that we selected the $2$-headed coin. Then by the Law of Total Probability \ref{total-probability},
    \[P(A) = \sum_{i=1}^{n}P(A | B_i)P(B_i) = \left(\frac{1}{4}\cdot\frac{3}{6}\right) + \left(\frac{0}{1}\cdot\frac{1}{6}\right) + \left(\frac{1}{1}\cdot\frac{2}{6}\right) = \frac{1}{8} + \frac{1}{3} = \frac{11}{24}.\]
\end{exmp}

\begin{thm}{Bayes' Rule}\proofbreak
    Let $B_1, \ldots, B_n \in \Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \begin{align*}
        P(B_k | A) = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A \intersection B_k)} = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A | B_k)P(B_k)}.
    \end{align*}
\end{thm}

\section{Independence}

\begin{defn}
    Events $A, B \in \Omega$ are \emph{independent} if $P(A \intersection B) = P(A)P(B)$.
\end{defn}

\begin{prop}
    If $A$ and $B$ are independent events, then $P(A | B) = P(A)$ and $P(B | A) = P(B)$.
\end{prop}

\begin{proof}
    \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)\]
\end{proof}

\begin{prop}
    If $A$ and $B$ are independent events, then $A$ and $B^{c}$ are independent.
\end{prop}

\begin{proof} Since $B$ and $B^{c}$ are necessarily mutually exclusive and exhaust $A$, we have $A = (A \intersection B) \disjointunion (A \intersection B^{c})$. Furthermore, $P(B^{c}) = 1- P(B)$. Then
    \begin{align}
        P(A \intersection B^{c}) &+ P(A \intersection B) = P(A) \\
        P(A \intersection B^{c}) &+ P(A)P(B) = P(A) \\
        P(A \intersection B^{c}) &= P(A)\left(1 - P(B)\right).
    \end{align}
\end{proof}

\begin{defn}
    We say events $A_1, A_2, \ldots$ are \emph{independent} if every for every finite subcollection $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ we have
    \[P(A_{i_1} \intersection A_{i_2} \intersection \cdots \intersection A_{i_k}) = P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k}).\]
\end{defn}

\begin{defn}
    A \emph{random variable} $X$ on a sample space $\Omega$ is a function $X: \Omega \to \R$.
\end{defn}

\begin{exmp}
    Let $A \subseteq \Omega$ be an event, and let
    \[X: \omega \mapsto \begin{dcases}
        1, & \omega \in A \\
        0, & \omega \not\in A
    \end{dcases}.\]
    Then $X$ is a random variable.
\end{exmp}

\begin{defn}
    A random variable $X$ is said to be a \emph{discrete} random variable when its image $X(\Omega)$ is countable.
\end{defn}

\begin{defn}
    The \emph{Bernoulli distribution} distribution is the probability distribution of a random variable that takes value $1$ with probability $0 \leq p \leq 1$, and value $0$ with probability $1 - p$. For a random variable $X$ with this distribution and $k \in \{0, 1\}$:
    \[P(X = k) = \begin{dcases}
        p, & k = 1  \\
        1 - p, & k = 0
    \end{dcases} = p^k(1-p)^{1-k}.\]
\end{defn}

\begin{defn}
    The \emph{binomial} distribution with parameters $n$ and $p$ is the distribution of the number of successes in a sequence of $n$ independent trials which can take values $1, 0$ with probability $p$ and $1 - p$. The probability of $ 0 \leq k \leq n$ successes is
    \[P(X = k) = \binom{n}{k}p^k(1 - p)^{n-k}.\]
\end{defn}

\begin{defn}
    The \emph{Poission} distribution with parameter $\lambda > 0$, is the distribution of the number of independent events that occur in a fixed period when the events occur at a rate of $\lambda$ per unit period.
\end{defn}

\begin{defn}
    The \emph{expected value} of a discrete random variable $X$ with probability mass function $P(X)$ is
    \[E(X) = \sum_{x \in \Omega}xP(X = x).\]
\end{defn}

\begin{exmp}
    Show expected value of binomial is $np$.
\end{exmp}

\begin{defn}
    Poisson distribution
\end{defn}

\begin{defn}
    Geometric
\end{defn}

\begin{defn}
    Hyper-geometric
\end{defn}

\begin{defn}
    Negative binomial distribution
\end{defn}

\begin{defn}
    Zeta distribution
\end{defn}

\begin{prop}
    Expectation is linear, that is $E(kX_1+X_2) = kE(X_1) + E(X_2)$.
\end{prop}

\begin{thm}{Law of the Unconscious Statistician ($\mathcal{LOTUS}$)}{\label{lotus}}\proofbreak
    Let $X$ be a discrete random variable with expected value $E(X)$. For any function $f$, \[E(f(X)) = \sum_{x}f(x)P(X=x).\]
\end{thm}

\begin{defn}
    Let $X$ be a discrete random variable. For $k \geq 1$, the $k$th \emph{moment} of $X$ is the expected variable of $X^k$.
\end{defn}

\begin{rmk}
    The mean of a random variable $X$ (denoted by $\mu$) is the first moment of $X$.
\end{rmk}

\begin{exmp}
    All moments of a Bernoulli $p$ distribution are simply $p$.
\end{exmp}

\begin{defn}
    For a random variable $X$, the \emph{variance} of $X$ is the expected squared distance from the mean: \[\mathrm{Var}(x) = E\left[\left(X-E(X)\right)^2\right] = E\left[\left(x - \mu\right)^2\right].\]
\end{defn}

\begin{rmk}
    While the mean has the same units as $X$, the variance has measure of those units squared.
\end{rmk}

\begin{defn}
    The \emph{standard deviation} $\sigma$ of a random variable $X$ is the positive square root of the variance of $X$.
\end{defn}

\begin{prop}
    When the first and second moments of a random variable exist and are finite, the variance can be computed by $\sigma^2 = E(X^2) - E(X)^2$.
\end{prop}

\begin{proof}
    By $\mathcal{LOTUS}$ \ref{lotus}, the variance is
    \begin{align*}
        E\left[(X-E(X))^2\right] &= E\left[X^2 - 2XE(X) + E(X)^2\right] \\
        &= E(X^2) - 2E(X)E\left[X\right] + E\left[E(X)^2\right] \\
        &= E(X^2) - 2E(X)^2 + E(X)^2 = E(X^2) - E(X)^2.
    \end{align*}
\end{proof}

\begin{exmp}
    Let $X$ be a binomial $n, p$ distribution. Recall that the mean of $X$ is $np$. Note that $E(X^2) = E(X(X-1)+X)$, and by $\mathcal{LOTUS}$ \ref{lotus}
    \begin{align*}
        E(X(X-1)) &= \sum_{n=0}^{n}x(x-1)\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
        &= \cdots \\
        &= n(n-1)p^2.
    \end{align*}
    Therefore, $E(X^2) = n(n-1)p^2+np$, and so the variance is $n(n-1)p^2+np-n^2p^2 - n^2p^2-n^2o^2-np^2+np = n(p-p^2)$.
\end{exmp}

\begin{defn}
    Given a random variable $X$, the \emph{moment generating function} for $\theta \in \R$ is
    \begin{align*}
        M(\theta) = E(e^{\theta X}).
    \end{align*}
\end{defn}

\begin{prop}
    If the moment generating function exists for a random variable,
    \begin{itemize}
        \item it uniquely identifies that random variablem i.e. it is injective,
        \item all moments of the random variable exist and are finite,
        \item the $n$th derivative of the moment generating function at $\theta = 0$ is the $n$th moment of the random variable.
    \end{itemize}
\end{prop}

\begin{prop}
    Can show the last one by MacLaurin expansion around $\theta = 0$.
\end{prop}

\chapter{Probability}
\label{ch:probability}

\section{Axioms of Probability}

\begin{defn}\proofbreak
    \begin{itemize}
        \item Sample point: a possible outcome of a probabilistic experiment, often denoted by $\omega$.
        \item Sample space: the set of all sample points, often denoted by $\Omega$.
        \item Event: any subset of the sample space.
    \end{itemize}
\end{defn}

\begin{defn}
    If events $A_i \subseteq \Omega$ are disjoint, we say that these events are \emph{mutually exclusive}.
\end{defn}

\begin{defn}\label{kolmogorov-probability-axioms}
    A \emph{probability space} is a sample space $\Omega$ together with a function $P: \mathcal{P}(\Omega) \to \R$ that satisfies the following axioms:
    \begin{itemize}
        \item Non-negativity: for any event $A \subseteq \Omega$, $P(A) \geq 0$.
        \item Normalization: $P(\Omega) = 1$.
        \item Countable-additivity: if $A_i$ are a countable sequence of mutually exclusive events, then $P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i)$.
    \end{itemize}
\end{defn}

\begin{rmk}
    These axioms are due to Andrey Kolmogorov.
\end{rmk}

\begin{prop}
    Let $(\Omega, P)$ be a probability space, and $A \subseteq \Omega$ an event. Then $P(A) + P(A^{c}) = 1$.
\end{prop}

\begin{proof}
    Since $A$ and $A^{c}$ are mutually exclusive, $P(A) + P(A^{c}) = P(A \union A^{c}) = P(\Omega) = 1$.
\end{proof}

\begin{cor}
    $P(\emptyset) = 0$.
\end{cor}

\begin{prop}Monotonicity\label{probability-monotonicity}\proofbreak
    Let $A, B \subseteq \Omega$ be events such that $A \subseteq B$. Then $P(A) \leq P(B)$.
\end{prop}

\begin{proof}
    Let $C = B - A$. Then $A \union C = B$ and $A \intersection C = \emptyset$. Since $A$ and $C$ are therefore mutually exclusive, $P(A) + P(C) = P(A \union C) = P(B)$. Since $P(C) \geq 0$ by the axiom of non-negativity, it follows that $P(A) \leq P(B)$.
\end{proof}

\begin{thm}{Inclusion-exclusion Principle}\label{inclusion-exclusion}
    Let $A, B, C \subseteq \Omega$ be events. Then
    \[P(A \union B) = P(A) + P(B) - P(A \intersection B),\]
    and
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) \\
                               &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
                               &+ P(A \intersection B \intersection C).
    \end{align*}
\end{thm}

\begin{proof}
    Since $A \union B = (A - B) \union (B - A) \union (A \intersection B)$, which are necessarily mutually exclusive events, we have
    \[P(A \union B) = P(A - B) + P(B - A) + P(A \intersection B).\]
    Now note that $A = (A - B) \union (A \intersection B)$ and $B = (B - A) \union (A \intersection B)$, and so $P(A) = P(A - B) + P(A \intersection B)$ and $P(B) = P(B - A) + P(A \intersection B)$. Therefore, \begin{align*}
        P(A \union B) &= P(A - B) + P(B - A) + P(A \intersection B) \\
                      &= \big[P(A - B) + P(A \intersection B)\big] + \big[P(B - A) + P(A \intersection B)\big] - P(A \intersection B) \\
                      &= P(A) + P(B) - P(A \intersection B).
    \end{align*}

    To prove the three-way version of the principle, we can apply the two-way version to $A \union B$ and $C$. This gives us
    \begin{align*}\label{inclusion-exclusion-three-intermediate}\tag{$1$}
        P(A \union B \union C) &= P(A \union B) + P(C) - P((A \union B) \intersection C) \\
        &= \big[P(A) + P(B) - P(A \intersection B)\big] + P(C) - P((A \union B) \intersection C).
    \end{align*}
    Since $(A \union B) \intersection C = (A \intersection C) \union (B \intersection C)$, we can apply the principle again to find that
    \[P((A \union B) \intersection C) = P(A \intersection C) + P(B \intersection C) - P((A \intersection C) \intersection (B \intersection C)).\] Noting that $(A \intersection C) \intersection (B \intersection C) = A \intersection B \intersection C$ and substituting this back into \ref{inclusion-exclusion-three-intermediate}, we obtain
    \begin{align*}
        P(A \union B \union C) &= P(A) + P(B) + P(C) - P(A \intersection B) - \big[P(A \intersection C) + P(B \intersection C) - P(A \intersection B \intersection )\big] \\
        &= P(A) + P(B) + P(C) \\
        &- P(A \intersection B) - P((A \intersection C) - P(B \intersection C) \\
        &+ P(A \intersection B \intersection C).
    \end{align*}
\end{proof}

\begin{prop}
    \[A \union B = A \disjointunion (B - A).\]
    \[A \union B \union C = A \disjointunion (B - A) \disjointunion (C - (A \union B)).\]
\end{prop}

\begin{exmp}{Finite, equally-likely probability law}\proofbreak
    Let $(\Omega, P)$ be a sample space together with a function $P: \mathcal{P}(\Omega) \to \R$, where $\abs{\Omega} \in \Z^{+}$ and for all $A \in \mathcal{\Omega}$, \[P(A) = \frac{\abs{A}}{\abs{\Omega}}.\]

    Since $\abs{A} \geq 0$ and $\abs{\Omega} > 0$, $P(A) \geq 0$ and so this model satifies the non-negativity axiom. Since $P(\Omega) = \frac{\abs{\Omega}}{\abs{\Omega}} = 1$, this model satisfies the normalization axiom.
    
    Let $A = A_1 \disjointunion A_2 \disjointunion \cdots$. Then
    \begin{align*}
        P(A) = \frac{\abs{A_1 \disjointunion A_2 \disjointunion \cdots}}{\abs{\Omega}} = \frac{\abs{A_1}}{\abs{\Omega}} + \frac{\abs{A_2}}{\abs{\Omega}} + \cdots = P(A_1) + P(A_2) + \cdots,
    \end{align*}
    and so this model also satisfies the countable-additivity axiom. Therefore, it is a probability space.
\end{exmp}

\begin{exmp}{Discrete probability law}\proofbreak
    Let $\Omega$ be a sample space such that $\abs{\Omega}$ is countable, and let $M: \Omega \to R$ be a mass-function: $M(\omega) \geq 0$ for all $\omega \in \Omega$, and $\sum_{\omega \in \Omega}M(\omega) = 1$.

    Now consider $P: \mathcal{P}(\Omega) \to \R$ defined by
    \[P(A) = \sum_{\omega \in A}M(\omega).\]

    Since $M(\omega) \geq 0$, $P(A) \geq$ and so $P$ satisfies the axiom of non-negativity. Since \[P(\Omega) = \sum_{\omega \in \Omega}M(\omega) = 1\] by the definition of $M$, $P$ satisfies the axiom of normalization. Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{i}P(A_i) = \sum_{i}\sum_{\omega \in A_i}M(\omega) = \sum_{\omega \in A}M(\Omega) = P(A).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}


\begin{exmp}{Geometric probability law}\proofbreak
    Let $\Omega = \{\omega_1, \omega_2, \ldots\}$ be a sample space such that $\abs{\Omega} = \abs{\Z}$, and define $P: \mathcal{P}(\Omega) \to \R$ by
    \[P(A) = \sum_{\omega_i \in A}\left(\frac{1}{2}\right)^{i}.\]
    
    Since $\left(\frac{1}{2}\right)^i \geq 0$, $P(A) \geq 0$ and so $P$ satisfies the axiom of non-negativity. Since
    \begin{align*}
        P(\Omega) = \sum_{\omega_i \in \Omega}\left(\frac{1}{2}\right)^{i} = \sum_{i=1}^{\infty}\left(\frac{1}{2}\right)^{i} = \frac{1/2}{1 - 1/2} = 1,
    \end{align*}
    we know that $P$ satisfies the axiom of normalization.

    Finally, for $A = A_1 \disjointunion A_2 \disjointunion \cdots$,
    \begin{align*}
        P(\disjointunionbig_{i}A_i) = \sum_{\omega_j \in A}\left(\frac{1}{2}\right)^{j} = \sum_{i}\sum_{\omega_j \in A_i}\left(\frac{1}{2}\right)^{j} = \sum_{i}P(A_i).
    \end{align*}
    Therefore, $P$ satisfies the axiom of countable additivity and so $(\Omega, P)$ is a probability space.
\end{exmp}

\section{Conditional Probability}

\begin{exmp}
    Roll two six-sided dice, and only look at one of them. It is a six. Let $S$ denote the event that the sum of the values of the two dice is seven, and $A$ is the event that at least one of dice was a six. What is the probability of $P(S | A)?$ It is $\frac{2}{11}$, not $\frac{1}{6}$.
\end{exmp}

\begin{defn}
    Let $A, B \in Omega$. Then the probability that $A$ occurred, given that $B$ occured, is denoted by $P(A | B)$ and read as ``the probability of $A$ given $B$''.
\end{defn}

\begin{prop}
    If all sample points in $\Omega$ are equally likely, then $P(A | B) = \frac{P(A \intersection B)}{P(B)}$.
\end{prop}

\begin{cor}
    $P(A \intersection B) = P(A | B)P(B)$.
\end{cor}

\begin{thm}
    Let $(\Omega, P)$ be a probability space, and $B \subseteq \Omega$ an event such that $P(B) > 0$. Then $(B, P(\cdot | B))$ form a probability space.
\end{thm}

\begin{proof}
    To show that $(B, P(\cdot|B))$ is a probability space, we need to show non-negativity (that $P(A | B) \geq 0$ for all $A$), normalization (that $P(B|B) = 1$), and countable additivity (that $P(A_1 \disjointunion A_2 \disjointunion \cdots | B) = P(A_1 | B) + P(A_2 | B) + \cdots$ if $A_i$ is a countable sequence).

    For any $A \subseteq B$, we know that $P(A \intersection B), P(B) \geq 0$, and so \[P(A | B) = \frac{P(A \intersection B)}{P(B)} \geq 0.\]

    \[P(B | B) = \frac{P(B \intersection B)}{P(B)} = \frac{P(B)}{P(B)} = 1.\]

    \begin{align*}
        P(A_1 \disjointunion A_2 \disjointunion \cdots | B) &= \frac{P\left((A_1 \disjointunion A_2 \disjointunion \cdots) \intersection B\right)}{P(B)} = \frac{P(A_1 \intersection B) \disjointunion (A_2 \intersection B) \disjointunion \cdots)}{P(B)} \\
        &= \frac{P(A_1 \intersection B) + P(A_2 \intersection B) + \cdots)}{P(B)} = \frac{P(A_1 \intersection B)}{P(B)} + \frac{P(A_2 \intersection B)}{P(B)} + \cdots \\
        &= P(A_1 | B) + P(A_2 | B) + \cdots.
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a sequence of coin tosses. Let $B$ be the event that the first head of the sequence occurs on an odd numbered toss (the first toss is \#1), and let $A$ be the event that the event first coin toss lands on heads. What is $P(A | B)$?

    Since $A \subseteq B$, we know that $A \intersection B = A$. Furthermore, $P(A) = \frac{1}{2}$, and $P(B) = \frac{1}{2} + \frac{1}{8} + \cdots = \frac{1/2}{1-1/4} = \frac{1}{2}\cdot\frac{4}{3} = \frac{2}{3}$. Therefore, \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1}{2}\cdot\frac{3}{2} = \frac{3}{4}.\]
\end{exmp}

\begin{thm}Law of Total Probability\label{total-probability}\proofbreak
    Let $B_1, \ldots, B_n \in Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \[P(A) = \sum_{i=1}^{n}P(A \intersection B_i) = \sum_{i=1}^{n}P(A | B_i)P(B_i).\]
\end{thm}

\begin{proof}
    Note that if $x \in (A \intersection B_i)$ and $x \in (A \intersection B_j)$, then $x \in B_i$ and $x \in B_j$. Since $B_1, \ldots, B_n$ are mutually exclusive they are disjoint, and so we must have $i = j$. Therefore, $A \intersection B_1, A \intersection B_2, \ldots, A \intersection B_n$ are disjoint.

    Also note that
    \begin{align*}
        \disjointunionbig_{i=1}^{n}A \intersection B_i = A \intersection \left(\disjointunionbig_{i=1}^{n}B_i\right) = A \intersection \Omega = A
    \end{align*}
    since $B_1, \ldots, B_n$ are exhaustive.

    Therefore, by the axiom of countable additivity,
    \begin{align*}
        P(A) = P\left(\disjointunionbig_{i=1}^{n}A \intersection B_i\right) = \sum_{i=1}^{n}P(A \intersection B_i).
    \end{align*}
\end{proof}

\begin{exmp}
    Consider a box containing six coins. Three of these coins are fair, but one of them has two tails, and the remaining two have two heads each. If we randomly (uniformly) select one coin and flip it twice, what is the probability it comes up heads twice?

    Let $A$ be the event we observe two heads. Let $B_1$ be the event we selected one of the fair coins, $B_2$ the event we selected the $2$-tailed coin, and $B_3$ the event that we selected the $2$-headed coin. Then by the Law of Total Probability \ref{total-probability},
    \[P(A) = \sum_{i=1}^{n}P(A | B_i)P(B_i) = \left(\frac{1}{4}\cdot\frac{3}{6}\right) + \left(\frac{0}{1}\cdot\frac{1}{6}\right) + \left(\frac{1}{1}\cdot\frac{2}{6}\right) = \frac{1}{8} + \frac{1}{3} = \frac{11}{24}.\]
\end{exmp}

\begin{thm}{Bayes' Rule}\proofbreak
    Let $B_1, \ldots, B_n \in Omega$ be mutually exclusive and exhaustive events. Then for any event $A \in \Omega$,
    \begin{align*}
        P(B_k | A) = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A \intersection B_k)} = \frac{P(A|B_k))P(B_k)}{\sum_{k}P(A | B_k)P(B_k)}.
    \end{align*}
\end{thm}

\section{Independence}

\begin{defn}
    Events $A, B \in \Omega$ are \emph{independent} if $P(A \intersection B) = P(A)P(B)$.
\end{defn}

\begin{prop}
    If $A$ and $B$ are independent events, then $P(A | B) = P(A)$ and $P(B | A) = P(B)$.
\end{prop}

\begin{proof}
    \[P(A | B) = \frac{P(A \intersection B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)\]
\end{proof}

\begin{prop}
    If $A$ and $B$ are independent events, then $A$ and $B^{c}$ are independent.
\end{prop}

\begin{proof} Since $B$ and $B^{c}$ are necessarily mutually exclusive and exhaust $A$, we have $A = (A \intersection B) \disjointunion (A \intersection B^{c})$. Furthermore, $P(B^{c}) = 1- P(B)$. Then
    \begin{align}
        P(A \intersection B^{c}) &+ P(A \intersection B) = P(A) \\
        P(A \intersection B^{c}) &+ P(A)P(B) = P(A) \\
        P(A \intersection B^{c}) &= P(A)\left(1 - P(B)\right).
    \end{align}
\end{proof}

\begin{defn}
    We say events $A_1, A_2, \ldots$ are \emph{independent} if every for every finite subcollection $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ we have
    \[P(A_{i_1} \intersection A_{i_2} \intersection \cdots \intersection A_{i_k}) = P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k}).\]
\end{defn}

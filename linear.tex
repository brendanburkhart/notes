\chapter{Linear Algebra}
\label{ch:linear}

\section{Row operations and echelon form}

\begin{defn}\label{linear-eq}
    A \emph{linear equation} is an equation of the form $a_1x_1 + a_2x_2 + \cdots + a_nx_n = b$
\end{defn}

\begin{defn}\label{linear-sys}
    A \emph{system of linear equations} (or \emph{linear system}) is a set of linear equations of the same variables (e.g. $x_1, x_2, \ldots, x_n$).
\end{defn}

\begin{defn}\label{linear-sys-solutions}
    The \emph{solution set} of a system of linear equations is the intersection of the solution set of each individual linear equation.
\end{defn}

Systems of linear equations can be represented via matrices, where each column is a specific variable, each row is a linear equation, and the entries are the coefficients. Augmentations represent the constant term (denoted $b$ in definition \ref{linear-eq}).

\begin{exmp}
    The linear system \[\begin{linsys}{3}
            x &+ &2y &- &z &= &-1 \\
            2x &+ &2y &+ &z &= &1 \\
            3x &+ &5y &- &2z &= &-1
        \end{linsys}\] can be represented by the augmented matrix below.
    \[\begin{amatrix}{3}
            1 &2 &-1 &-1 \\
            2 &2 &1 &1 \\
            3 &5 &-2 &-1
        \end{amatrix}\]
\end{exmp}

Row operations are certain operations on matrices or systems of linear equations which can be used to simply or otherwise transform the system, while leaving the solution set unchanged.

\begin{defn}\label{row-op}
    Row operations: \begin{enumerate}
        \item Swap two rows/equations
        \item Multiply a single row by a non-zero scalar
        \item Add a scalar multiple of one row to another
    \end{enumerate}
\end{defn}

\begin{thm}\label{solutions-unchanged-by-row-ops}
    If a linear system is obtained from another by one of the row operations, then the two systems have the same set of solutions.
\end{thm}

\begin{proof}\proofbreak
    \begin{enumerate}
        \item As the intersection of sets is commutative, by Definition \ref{linear-sys-solutions}, the solution set of a linear system is unaffected by the order of the individual linear systems, therefore swapping rows/equations does not change the solution set.
        \item Since multiplying a single row by a non-zero scalar is equivalent to multiplying both sides of the represented equation by that scalar, the solution set is clearly left unchanged.
        \item Since adding a multiple of one row to another row is equivalent to adding equal quantities to both sides of the equation, no solutions are removed from the solution set. If the row $(sb_1 + a_1)x_1 + (sb_2 + a_2) + \cdots + (sb_n + a_n)x_n = sk_b + k_a$ is the result of adding $s$ times row $b_1x_1 + \cdots + b_nx_n = k_b$ to row $a_1x_1 + \cdots + a_nx_n = k_a$, then $(sb_1 + a_1)x_1 + (sb_2 + a_2) + \cdots + (sb_n + a_n)x_n = s(b_1x_1 + \cdots + b_nx_n) + k_a$  which implies that $a_1x_1 + \cdots + a_nx_n = k_a$, so no information was lost and the solution set remains the same.
    \end{enumerate}
\end{proof}

\begin{defn}
    A matrix in (row) \emph{echelon form} has all rows of zeroes at the bottom, and the leading term of each row is to the right of the row leading term above it.
\end{defn}

\begin{defn}
    In \emph{reduced row echelon form}, a matrix in row echelon form additionally has leading terms which are all $1$, and the entries above and below each leading term are zero.
\end{defn}

\begin{rmk}
    The reduced row echelon form of a particular matrix is always unique, however the row echelon form is not. Additionally, it is always possible to transform a given matrix into either echelon form via the row operations.
\end{rmk}

\begin{exmp}
    \begin{align*}
        \begin{amatrix}{3}
            1 &0 &1 &4 \\
            1 &-1 &2 &5 \\
            4 &-1 &5 &17
        \end{amatrix}
         & \begin{aligned}
             & \ro{r_2 \rightarrow r_2 - r_1}  \\
             & \ro{r_3 \rightarrow r_3 - 4r_1} \\
        \end{aligned}
         & \begin{amatrix}{3}
            1 &0 &1 &4 \\
            0 &-1 &1 &1 \\
            0 &-1 &1 &1
        \end{amatrix}
         & \begin{aligned}
             & \ro{r_2 \rightarrow -r_2} \\
        \end{aligned}
         & \begin{amatrix}{3}
            1 &0 &1 &4 \\
            0 &1 &-1 &-1 \\
            0 &-1 &1 &1
        \end{amatrix} \\
         & \begin{aligned}
             & \ro{r_3 \rightarrow r_3 + r_2} \\
        \end{aligned}
         & \begin{amatrix}{3}
            1 &0 &1 &4 \\
            0 &1 &-1 &-1 \\
            0 &0 &0 &0
        \end{amatrix}
    \end{align*}

    Since there is no leading term corresponding to $z$, $z$ is a free variable. If we let $z$ be zero, then the augmentation yields a particular solution to this linear system.

    Solution set in vector form:
    \[\begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} = \begin{pmatrix}
            4 \\ -1 \\ 0
        \end{pmatrix} + z\begin{pmatrix}
            -1 \\ 1 \\ 1
        \end{pmatrix}\]

    Solution set in parametric form:
    \[\left\{\left(4 - z, -1 + z, z\right) \compbar z \in \R \right\}\]

\end{exmp}

\section{Vector Spaces}

\begin{defn}
    Let $F$ be a field. A \emph{vector space} over $F$ consists of a set $V$ together with two operations:

    \begin{itemize}
        \item[] $+: V \times V \to V$ (vector addition)
        \item[] $\cdot: F \times V \to V$ (scalar multiplication)
    \end{itemize}
    which satisfy the following axioms for all $v, w, e \in V$ and $r, s \in F$:
    \begin{enumerate}
        \item $v + w = w + v$ (additive commutativity)
        \item $(v + w) + u = v + (w + u)$ (additive associativity)
        \item $\exists \vec{0} \in V \textrm{ s.t. } v + \vec{0} = v$ (additive identity)
        \item $\exists z \in V \textrm{ s.t. } v + z = \vec{0}$ (additive inverse)
        \item $(r + s) \cdot v = r \cdot v + s \cdot v$ (distributivity over scalar addition)
        \item $r \cdot (v + w) = r \cdot v + r \cdot w$ (distributivity over vector addition)
        \item $(rs)\cdot v = r \cdot (s \cdot v)$ (multiplicative associativity)
        \item $1 \cdot v = v$ (multiplicative identity)
    \end{enumerate}
\end{defn}

\begin{rmk}
    You can prove the following for all $v \in V$ and $r \in F$ from those axioms (see Lemma \ref{vector-space-properties}):
    \begin{itemize}
        \item The additive identity ($\vec{0}$) is unique.
        \item Additive inverses are unique.
        \item $0 \cdot v = \vec{0}$.
        \item $-1 \cdot v = -v$.
        \item $r \cdot \vec{0} = \vec{0}$.
    \end{itemize}
\end{rmk}

\begin{exmp}
    $\R^n$ is a vector space over $\R$ with $+$ and $\cdot$ defined as usual.
\end{exmp}

\begin{exmp}
    Let $F$ be any field. Then \[F^n = \left\{\begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix} \compbar x_i \in F \textrm{ for } i = 1, \ldots, n \right\}\] is a vector space over $F$ with $x + y$ defined as \[\begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix} + \begin{pmatrix}
            y_1 \\ \vdots \\ y_n
        \end{pmatrix} = \begin{pmatrix}
            x_1 + y_1 \\ \vdots \\ x_n + y_n
        \end{pmatrix}\] and $r \cdot x$ defined as \[r\begin{pmatrix}
            x_1 \\ \vdots \\ x_n
        \end{pmatrix} = \begin{pmatrix}
            rx_1 \\ \vdots \\ rx_n
        \end{pmatrix}.\]
\end{exmp}

Consider the field $\mathbb{F}_2 = \{0, 1\}$, and the vector space $\mathbb{F}_2^2$. This vector space has four elements: $\left\{\begin{pmatrix}
        0 \\ 0
    \end{pmatrix}, \begin{pmatrix}
        0 \\ 1
    \end{pmatrix}, \begin{pmatrix}
        1 \\ 0
    \end{pmatrix}, \begin{pmatrix}
        1 \\ 1
    \end{pmatrix}\right\}$. In general, $\mathbb{F}_2^n$ has $2^n$ elements. Note that since $0 + 0 = 0$ and $1 + 1 = 0$, every element of $\mathbb{F}_2^n$ is its own inverse. Furthermore, every element of \textit{any} vector space $V$ over $\mathbb{F}_2$ is its own inverse. This is because $v + v = 1 \cdot v + 1 \cdot v = (1 + 1) \cdot v = 0 \cdot v = \vec{0}$.

Consider the empty set. It cannot be a vector space, since a vector space requires the existence of an additive identity. However, the set $V = \left\{\star\right\}$ with $\star \cdot \star = \star$ and $r \cdot \star = \star$ is a vector space, called the \textbf{trivial} vector space. Note that $\star = \vec{0}$.

\begin{exmp}
    $\C$ forms a vector space over $\R$, with element-wise addition and $r \cdot (a + bi) = (ra) + (rb)i$.
\end{exmp}

\begin{rmk}
    $\C =\C^1$ forms a vector space over $\C$, as does $\R = \R^1$ over $\R$.
\end{rmk}

\begin{exmp}
    Let $V$ be the set of $2 \times 2$ matrices over $\R$, denoted $M_{2 \times 2}(R)$:
    \[\left\{\begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} \compbar a,b,c,d \in \R \right\}.\] Define vector addition and scalar multiplication as follows. \[\begin{pmatrix}
            a & b \\ c &d
        \end{pmatrix} + \begin{pmatrix}
            a' & b' \\ c' &d'
        \end{pmatrix} = \begin{pmatrix}
            a + a' & b + b' \\ c + c' &d + d'
        \end{pmatrix}\] \[r \cdot \begin{pmatrix}
            a & b \\ c &d
        \end{pmatrix} = \begin{pmatrix}
            ra & rb \\ rc &rd
        \end{pmatrix}\] Note that $V$ is essentially equivalent to $\R^4$. $V$ is a vector space over $\R$, and in general $M_{m\times n}(F)$ with entry-wise vector addition and scalar multiplication forms a vector space over $F$.
\end{exmp}

\begin{exmp}
    Let $F$ be any field, and $n \in \N$. Then define the set $P_n(F)$, the set of all polynomials with coefficients in $F$ of degree at most $n$, as follows: \[P_n(F) = \left\{a_0 + a_1x_1 + \cdots + a_nx^n \compbar a_i \in F \textrm{ for }i=0,\ldots,n\right\},\] \[(a_0 + \cdots + a_nx^n) + (b_0 + \cdots + b_nx^n) = (a_0 + b_0) + \cdots + (a_n + b_n)x^n,\] \[r(a_0 + \cdots + a_nx^n) = ra_0 + \cdots + ra_nx^n.\]

    $P_n(F)$ is a vector space over $F$. Similarly, $P(F)$ (the set of polynomials with coefficients in $F$ of \textit{any} degree) forms a vector space over $F$.
\end{exmp}

\begin{exmp}
    Let $S$ be any set, $F$ any field, and $V = \{f: S \to F\}$. $V$ forms a vector space over $F$ with $(f + g)(s) = f(s) + g(s)$ and $(rf)(s) = r(f(s))$ for all $r \in F, s \in S$.

    Notice that if $S = \{1, \ldots, n\}$, we get $F^n$, since $f \in V$ is a function from a coordinate index $i$ to the value of that coordinate $x_i$. If $S = \N$ and $V = \left\{(a_1, a_2, \ldots) \compbar a_i \in F \textrm{ for } i = 1, 2, \ldots\right\}$, we similarly get the set of all sequences in $F$.
\end{exmp}

\begin{lemma}\label{vector-space-properties}
    Let $V$ be a vector space over a field $F$. Then for all $v \in V$ and $r \in F$: \begin{enumerate}
        \item The additive identity is unique.
        \item The additive inverse of $v$ is unique.
        \item $0 \cdot v = \vec{0}$.
        \item $(-1) \cdot v + v = \vec{0}$.
        \item $r \cdot \vec{0} = \vec{0}$.
    \end{enumerate}
\end{lemma}

\begin{proof}\proofbreak
    \begin{enumerate}
        \item Let $i_1$ and $i_2$ be additive identities of $V$. Then $i_1 = i_1 + i_2 = i_2$, and by transitivity $i_1 = i_2$. The additive identity is therefore unique.
        \item Let $w_1$ and $w_2$ be additive inverses of $v$. Then $v + w_1 = \vec{0}$ and $v + w_2 = \vec{0}$, so $v + w_1 = v + w_2$. Then $w_1 + (v + w_1) = w_1 + (v + w_2)$, which implies that $(w_1 + v) + w_1 = (w_1 + v) + w_2$. Therefore, $w_1 = w_2$, so the additive inverse of $v$ is unique and so can be denoted $-v$.
        \item $0 \cdot v + 0 \cdot v = (0 + 0) \cdot v = 0 \cdot v$. Since the additive identity is unique, this implies that $0 \cdot v = \vec{0}$.
        \item $(-1) \cdot v + v = (-1) \cdot v + 1 \cdot v = (-1 + 1) \cdot v = 0 \cdot v = \vec{0}$.
        \item $r \cdot \vec{0} = r \cdot (v + -v) = r \cdot (1 + -1) \cdot v = (r \cdot 0)\cdot v = 0 \cdot v = \vec{0}$.
    \end{enumerate}
\end{proof}

\begin{defn}\label{subspace-defn}
    Let $V$ be a vector space over $F$, and $W \subseteq V$. We say $W$ is a \emph{subspace} of $V$ if it is itself a vector space over $F$ with the operations defined as for $V$.
\end{defn}

\begin{exmp}
    Let $W = \left\{\begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} \in \R^3 \compbar x + y + z = 0\right\}$. $W$ is a vector subspace over $\R$ with the operations inherited from $\R^3$. $W$ is a plane within $\R^3$, with normal $(1, 1, 1)$.
\end{exmp}

\begin{prop}\label{subspace-proof}
    A subset $W$ of $V$ is a subspace if and only if
    \begin{enumerate}
        \item $W$ is non-empty.
        \item $W$ is closed under vector addition.
        \item $W$ is closed under scalar multiplication.
    \end{enumerate}
\end{prop}

\begin{proof}\proofbreak
    ($\Longrightarrow$) If $W$ is a subspace, then by definition it is itself a vector space, so it must contain $\vec{0}$ and so it is non-empty. Additionally, (2) and (3) follow from the definition of binary operations.

    ($\Longleftarrow$) Since $W$ is a subset of $V$ with the same operations, all axioms of vector spaces follow except for the existence of the additive identity and additive inverses, and $W$ being closed under vector addition and scalar multiplication (as these are defined as binary operations for $V$ rather than $W$).

    (2) and (3) then guarantee that $W$ is closed under vector addition and scalar multiplication. Since $W$ is closed under scalar multiplication, we know that $(0 \cdot w) \in W$ for any $w \in W$. Since $0 \cdot w = \vec{0}$, we know that $\vec{0} \in W$ so $W$ has an additive identity. Similarly, since $(-1 \cdot w) \in W$ and $-1 \cdot w = -w$, every element in $W$ must have an additive inverse.
\end{proof}

\begin{exmp}
    Let $V = \R^3$, $W = \left\{\begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} \in \R^3 \compbar x + y + z = 1\right\}$. Then $W$ cannot be a subspace of $V$, as $\vec{0} \notin W$.
\end{exmp}

\begin{exmp}
    Let $S \subseteq F^n$ be the solution set to a homogeneous system of linear equations over a field $F$. Then clearly $\vec{0} \in S$, and since every solution is equal to zero, $S$ is closed under addition and multiplication. Therefore, by Proposition \ref{subspace-proof}, $S$ is a vector subspace of $F^n$.
\end{exmp}

\begin{exmp}
    If $V$ is any vector space over $F$, then both $\{\vec{0}\}$ and $V$ are subspaces of $V$.
\end{exmp}

\begin{defn}
    $\left\{\vec{0}\right\}$ is the \emph{trivial} subspace.
\end{defn}

\section{Linear Combinations and Spans}

\begin{defn}
    Let $V$ be a vector space over $F$, and $S \subseteq V$ with $S \neq \emptyset$. We say $v \in V$ is a \emph{linear combination} of vectors in $S$ if \[v = c_1s_1 + \cdots + c_nc_n \textrm{ for some } c_i \in F, s_i \in S.\]
\end{defn}

\begin{exmp}
    In $\R^3$, $\begin{pmatrix}
            2 \\ 3 \\ 0
        \end{pmatrix} = 2\begin{pmatrix}
            1 \\ 0 \\ 0
        \end{pmatrix} + 3\begin{pmatrix}
            0 \\ 1 \\ 0
        \end{pmatrix}$, so $\begin{pmatrix}
            2 \\ 3 \\ 0
        \end{pmatrix}$ is a linear combination of vectors in $\left\{\begin{pmatrix}
            1 \\ 0 \\ 0
        \end{pmatrix},\begin{pmatrix}
            0 \\ 1 \\ 0
        \end{pmatrix},\begin{pmatrix}
            0 \\ 0 \\ 1
        \end{pmatrix}\right\}$.
\end{exmp}

\begin{defn}
    Let $S \subseteq V$. The \emph{span} (also \emph{linear span} or \emph{linear hull}) of $S$ is the set of all linear combinations of vectors in $S$. \[\spanset{S} = \left\{c_1s_1 + \cdots + c_ns_n \compbar n \in \N, c_i \in F, s_i \in S\right\}\]

    By convention, if $S = \emptyset$, we define $\spanset{S} = \left\{\vec{0}\right\}$.
\end{defn}

\begin{rmk}
    If $V = \spanset{S}$, we may say that $S$ spans $V$ or that $S$ generates $V$, as well as variations such as that $V$ is spanned/generated by $V$, or that $S$ is a spanning/generating set of $V$.
\end{rmk}

\begin{rmk}
    If $S \subseteq T$, then $\spanset{S} \subseteq \spanset{T}$.
\end{rmk}

\begin{lemma}
    Let $V$ be a vector space over $F$, and $S \subseteq V$. Then $\spanset{S}$ is a subspace of $V$.
\end{lemma}

\begin{proof}
    If $S = \emptyset$, then $\spanset{S} = \left\{\vec{0}\right\}$, so $\spanset{S}$ is the trivial subspace.

    If $S \neq \emptyset$, then there exists some $s \in S$. Since $(1 \cdot s) \in \spanset{S}$, we know that $\spanset{S} \neq \emptyset$. Let $v, w \in \spanset{S}$, $r \in F$. We need to prove that $v + r \cdot w \in \spanset{S}$. Since $v \in \spanset{S}$, we know that $v = c_1s_1 + \cdots + c_ns_n$ for some $n \geq 1$, $c_i \in F$, and $s_i \in S$. Similarly, $w = d_1t_1 + \cdots + d_mt_m$ for some $m \geq 1$, $d_i \in F$, and $t_i \in S$. Then $v + rw = c_1s_1 + \cdots + c_ns_n + rd_1t_1 + \cdots + rd_mt_m$, so $v + r \cdot w \in \spanset{S}$. Therefore, by Lemma \ref{subspace-proof}, $\spanset{S}$ is a subspace of $V$.
\end{proof}

\begin{rmk}
    We also proved that $S \subseteq \spanset{S}$, and that $\spanset{S}$ is the smallest subspace of $V$ that contains $S$, in the sense that if $W$ is a subspace of $V$ such that $S \subseteq W$, then $\spanset{S} \subseteq W$.
\end{rmk}

\begin{exmp}
    Let $V$ be a vector space over $F$. Given $S \subseteq V$, and $v \in V$, how do we know if $v \in \spanset S$?

    Let $V = P_2(\R)$, $S = \left\{x^2 + 3x - 2, 2x^2 + 5x -3\right\}$, and $v = -x^2 - 4x + 4$. We are trying to determine $a, b \in F$ such that $-x^2 - 4x + 4 = a(x^2 + 3x - 2) + b(2x^2 + 5x - 3)$. This would imply that $a + 2b = -1$, $3a + 5b = -4$, and $-2a - 3b = 4$. This system of equations can be represented as a matrix, and solved for $a$ and $b$.
    \begin{align*}
        \begin{amatrix}{2}
            1 &2 &-1 \\
            3 &5 &-4 \\
            -2 &-3 &4
        \end{amatrix}
        &\begin{aligned}
             & \ro{r_2 \rightarrow r_2 - 3r_1}  \\
             & \ro{r_3 \rightarrow r_3 + 2r_1} \\
        \end{aligned}
        &\begin{amatrix}{2}
            1 &2 &-1 \\
            0 &-1 &-1 \\
            0 &1 &2
        \end{amatrix}
        &\begin{aligned}
            & \ro{r_2 \rightarrow r_2 + r_3}  \\
       \end{aligned}
       &\begin{amatrix}{2}
           1 &2 &-1 \\
           0 &0 &1 \\
           0 &1 &2
       \end{amatrix}
    \end{align*}
    This would imply that $0 = 1$, which is a contradiction, so no such $a, b$ exist.
\end{exmp}

\begin{lemma}\label{equal-spans-condition}
    Let $V$ be a vector space over $F$, $S \subseteq V$, and $v \in V$. Then $\spanset{S \cup \left\{v\right\}} = \spanset{S}$ if and only if $v \in \spanset{S}$.
\end{lemma}

\begin{proof}\proofbreak
    ($\implies$) If $v \notin \spanset{S}$, then since $v \in \spanset{S \cup \left\{v\right\}}$, we know that $\spanset{S} \neq \spanset{S \cup \left\{v\right\}}$.

    ($\impliedby$) Assume $v \in \spanset{S}$. Then $v = c_1v_1 + \cdots + c_ns_n$ for $c_i \in F$ and $s_i \in S$. Since $S \subseteq S \cup \left\{v\right\}$, we have $\spanset{S} \subseteq \spanset{S \cup \left\{v\right\}}$. Let $w \in \spanset{S \cup \left\{v\right\}}$, so $w = d_1t_1 + \cdots + d_mt_n + d_{m+1}v$. Thus, $w = d_1t_1 + \cdots + d_mt_m + d_{m+1}(c_1 + \cdots + c_n)$, so $w \in \spanset{S}$ and $\spanset{S \cup \left\{v\right\}} \subseteq \spanset{S}$. Therefore, $\spanset{S} = \spanset{S \cup \left\{v\right\}}$.
\end{proof}

\section{Linear Independence}

\begin{defn}
    Let $V$ be a vector space over a field $F$, and $S \subseteq V$. We say that $S$ is \emph{linearly dependent} if there exists distinct $s_1, s_2, \ldots, s_n \in S$ and $c_1, c_2, \ldots, c_n \in F$ not all zero such that $c_1s_1 + c_2s_2 + \cdots + c_ns_n \neq \vec{0}$.
\end{defn}

\begin{rmk}
    If $S$ is not linear dependent, then we say that it is \emph{linearly independent}.
\end{rmk}

\begin{exmp}
    Let \[S = \left\{\begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ 0\end{pmatrix}, \begin{pmatrix}1 \\ 1 \\ 0\end{pmatrix} \right\} \subseteq \R^3.\] Since \[1\begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix} + 1\begin{pmatrix}0 \\ 1 \\ 0\end{pmatrix} - 1\begin{pmatrix}1 \\ 1 \\ 0\end{pmatrix} = \vec{0},\] $S$ is linearly dependent.
\end{exmp}

\begin{exmp}
    Let $S = \{1 + x, 1 + x + x^2\} \subseteq P_2(\R)$. Assume there exists $c_1, c_2 \in \R$ such that $c_1(1 + x) + c_2(1 + x + x^2) = 0$. Then $0 = (c_1 + c_2) + (c_1 + c_2)x + c_2x^2$. Therefore, $c_1 + c_2 = 0$ and $c_2 = 0$, which implies that $c_1 = c_2 = 0$, so $S$ is linearly independent.
\end{exmp}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $S = \emptyset$ is linearly independent.
        \item $S = \{\vec{0}\}$ is linearly dependent.
        \item $S = \{v\}$ for any $v \in V, v \neq \vec{0}$ is linearly independent.
    \end{itemize}
\end{exmp}

\begin{prop}\label{linear-dependence-implies-extra-vector}
    Let $S$ be a subset of a vector space $V$ over a field $F$. Then $S$ is linearly dependent if and only if there exists some $v \in S$ such that $v \in \spanset{S - \{v\}}$.
\end{prop}

\begin{proof}\proofbreak
    ($\implies$) Assume $S$ is linearly dependent, then there exists some distinct $s_1, s_2, \cdots, s_n \in S$ and $c_1, c_1, \cdots, c_n \in F$ such that $c_1s_1 + c_2s_2 + \cdots + c_ns_n = \vec{0}$. Therefore, $s_1 = -\frac{c_2}{c_1}s_2 + \cdots + -\frac{c_n}{c_1}s_n$, so $s_1 \in \spanset{S - \{s_1\}}$.

    ($\impliedby$) Assume $v \in \spanset{S - \{v\}}$. Then $v = c_1s_1 + \cdots + c_ns_n$ for some distinct $s_i \in S - \{v\}$. This implies that $c_1s_1 + \cdots + c_ns_n + (-1)v = \vec{0}$, so $S$ is linearly dependent.
\end{proof}

\begin{prop}\label{linear-independence-with-extra-vector}
    Let $S$ be a linearly independent subset of a vector space $V$ over a field $F$, and $v \notin S$. Then $S \union \{v\}$ is linearly independent if and only if $v \notin \spanset{S}$.
\end{prop}

\begin{proof}\proofbreak
    ($\implies$) If $v \in \spanset{S}$, then $v = c_1s_1 + \ldots + c_ns_n$ for some distinct $s_1, \ldots, s_n \in S$ and $c_1, \ldots, c_n \in F$, $v = c_1s_1 + \ldots + c_ns_n$, so $c_1s_1 + \ldots + c_ns_n + (-1)v = \vec{0}$. Therefore, $S \union \{v\}$ is not linearly independent.

    ($\impliedby$) Let $s_1, \ldots, s_n \in S$, and $c_1, \ldots, c_n, c_{n+1} \in F$ such that $c_1s_1 + \ldots + c_ns_n + c_{n+1}v = \vec{0}$. Then since $v \notin S$ it follows that $c_1, \ldots, c_n, c_{n+1}$ must equal zero, or else $v = -\frac{c_1}{c_{n+1}}s_1 + \cdots + -\frac{c_n}{c_{n+1}}s_n$. Therefore, $S \union \{v\}$ must be linearly independent.
\end{proof}

\begin{prop}\label{linearly-independent-subset-existence}
    Any finite subset $S$ of $V$ has a linearly independent subset with the same span.
\end{prop}

\begin{proof}
    If $S$ is linearly independent, then $S$ itself is just such a subset.

    If not, then there must exist $v \in S$ such that $v \in \spanset{S - \{v\}}$ by Proposition \ref{linear-dependence-implies-extra-vector}. Then by Lemma \ref{equal-spans-condition}, $\spanset{S} = \spanset{S - \{v\}}$. Then either $S - \{v\}$ is a linear independent subset of $S$ with the same span, or this process can be repeated. Since $S$ is finite, and $\emptyset$ is linearly independent, eventually a linearly independent subset with the same span will be reached.
\end{proof}

\begin{exmp}
    Let \[S = \left\{\begin{pmatrix}2 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ 0\end{pmatrix}, \begin{pmatrix}2 \\ 2 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 3 \\ 1\end{pmatrix}, \begin{pmatrix}3 \\ 0 \\ 1\end{pmatrix} \right\} \subseteq \R^3.\] Notice that $\spanset{S} = \R^3$. \begin{align*}
        \begin{amatrix}{5}
            2 &0 &2 &0 &0 &0 \\
            0 &1 &2 &3 &0 &0\\
            0 &0 &0 &1 &1 &0
        \end{amatrix}
         & \begin{aligned}
             & \ro{r_1 \rightarrow \frac{1}{2}r_1}  \\
             & \ro{r_2 \rightarrow r_2 - 3r_3} \\
            \end{aligned}
        \begin{amatrix}{5}
            1 &0 &1 &0 &0 &0 \\
            0 &1 &2 &0 &-3 &0\\
            0 &0 &0 &1 &1 &0
        \end{amatrix}
    \end{align*} Let the linear coefficients be $c_1, \ldots, c_5 \in \R$, and the vectors be $s_1, \ldots, s_5 \in S$. Then we can see that both $s_3$ and $s_5$ are linear combinations of $s_1, s_2$ and $s_4$. Therefore, \[\left\{\begin{pmatrix}2 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 3 \\ 1\end{pmatrix} \right\}\] is a linearly independent subset of $S$, whose span is still $\R^3$.
\end{exmp}

\section{Basis}

\begin{defn}
    Let $V$ be a vector space over a field $F$, and $B \subseteq V$. We say that $B$ is a \emph{basis} for $V$ if $B$ is linearly independent, and $B$ generates $V$.
\end{defn}

\begin{defn}
    An ordered basis is a basis $B$, where its elements are ordered in a specific sequence $B = \langle v_1, v_2, \ldots, v_n \rangle$
\end{defn}

\begin{prop}\label{unique-basis-expression}
    Let $B$ be a basis for a vector space $V$. Then every $v \in V$ can be expressed uniquely, up to the order of vectors, as a linear combination of elements in $B$.
\end{prop}

\begin{proof}
    Take $v \in V$, and let $v_i \in B$ be some sequence of all the elements of $B$. Then since $B$ generates $V$, there exists $c_i, d_i \in F$ such that $c_1v_1 + \ldots + c_nv_n = v$, and $d_1v_1 + \ldots + d_nv_n = v$. Then $\vec{0} = v - v = c_1v_1 + \ldots + c_nv_n - d_1v_1 + \ldots + d_nv_n = (c_1 - d_1)v_1 + \ldots + (c_n - d_n)v_n$. Since $B$ is linearly independent, is follows that $(c_1 - d_1), \ldots, (c_n - d_n)$ must all be zero, so $c_i = d_i$, and therefore all expressions for $v \in V$ as combinations of elements of $B$ must be identical.
\end{proof}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item If $V = \{\vec{0}\}$, then necessarily $B = \emptyset$.
        \item In $F_n$, the \emph{standard ordered basis} is $\left\langle \begin{pmatrix}1 \\ 0 \\ \vdots \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ \vdots \\ 0\end{pmatrix}, \ldots, \begin{pmatrix}0 \\ 0 \\ \vdots \\ 1\end{pmatrix}\right\rangle$.
        \item If $V = F^3$, then one possible basis is $B = \left\langle \begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}1 \\ 1 \\ 0\end{pmatrix}, \begin{pmatrix}1 \\ 1 \\ 1\end{pmatrix}\right\rangle$.
        \item The standard ordered basis for $V = P_3(F)$ is $B = \left\langle 1, x, x^2, x^3 \right\rangle$.
        \item If $V = P(F)$, the standard ordered basis is $B = \left\langle 1, x, x^2, \ldots \right\rangle$.
        \item If $V = M_{2\times 2}(F)$, then a basis is $B = \left\langle
        \begin{pmatrix}1 & 0 \\ 0 & 0\end{pmatrix},
        \begin{pmatrix}0 & 1 \\ 0 & 0\end{pmatrix},
        \begin{pmatrix}0 & 0 \\ 1 & 0\end{pmatrix},
        \begin{pmatrix}0 & 0 \\ 0 & 1\end{pmatrix}\right\rangle$.
    \end{itemize}
\end{exmp}

\begin{defn}
    Let $B = \langle v_1, \ldots, v_n\rangle$ be a finite, ordered basis for a vector space $V$ over a field $F$. Given $v \in V$, by Proposition \ref{unique-basis-expression} there exists unique $c_1, \ldots, c_n \in F$ such that $v = c_1v_1 + \ldots + c_nv_n$. The vector $[v]_B \in F^n$ is called the \emph{coordinates} of $v$ with respect to $B$, where
    \[[v]_B = \begin{pmatrix}c_1 \\ c_2 \\ \vdots \\ c_n\end{pmatrix}.\]
\end{defn}

\begin{exmp}
    Let $B = \langle (5, 3), (1, 4)\rangle$. The coordinates of $v = (7, -6)$ with respect to $B$ are then $(c_1, c_2)$ such that $5c_1 + 1c_2 = 7$, and $3c_1 + 4c_2 = -6$. Therefore, $[v]_B = (2, -3)$.
\end{exmp}

\begin{rmk}
    The coordinates with respect to $B$ define a function $f: V \to F^n$ where $f: v \mapsto [v]_B$. This function is a bijection that preserves the linear structure of the vector spaces $V$ and $F^n$. Let $v = c_1v_1 + \cdots + c_nv_n, w = d_1v_1 + \cdots + d_nv_n \in V$. Then $v + w = (c_1 + d_1)v_1 + (c_n + d_n)v_n$, so $[v+w]_B = [v]_B + [w]_B$. Similarly, $[rv]_B = r[v]_B$ for $r \in F$.
\end{rmk}

\begin{lemma}{Exchange lemma}\label{exchange-lemma}\proofbreak
    Let $B = \langle v_1, \ldots, v_n \rangle$ be a basis for a vector space $V$ over a field $F$, and let $a_1, \ldots, a_n \in F$ such that $a_l \neq 0$ for some $l$. Then let $w = a_1v_1 + \ldots + a_nv_n$, and let $B'$ be the set obtained from $B$ by exchanging $v_l$ with $w$. Then $B'$ is a basis for $V$.
\end{lemma}

\begin{proof}
    To prove that $B'$ is a basis, we need to prove that $B'$ is linearly independent, and that $B'$ spans $V$. Without loss of generality, we can assume that $l = 1$.

    To prove that $B'$ is linearly independent, assume $b_1, \ldots, b_n \in F$ such that $b_1w + b_2v_2 + \ldots + b_nv_n = \vec{0}$. Then $\vec{0} = b(a_1v_1 + \ldots a_nv_n) + b_2v_2 + \ldots + b_nv_n = (b_1a_1)v_1 + (b_1a_2 + b_2)v_2 + \ldots + (b_1a_n + b_n)v_n$. Since $B$ is linearly independent, $b_1a_1, b_1a_2 + b_2, \ldots, b_1a_n + b_n$ must all equal zero. By previous assumption, $a_1$ is not equal to zero, which implies that $b_1$ must be zero since $b_1a_1 = 0$, and so $b_2, \ldots, b_n$ must all be zero. Therefore, $B'$ is linearly independent.

    To prove that $B'$ spans $V$, let $v$ be any vector in $V$. Since $B$ spans $V$, there exists some $c_1, \ldots, c_n \in F$ such that $v = c_1v_1 + \cdots + c_nv_n$. Now note that $v_1 = \frac{1}{a_1}w - \frac{a_2}{a_1}v_2 - \cdots - \frac{a_n}{a_1}v_n$. So $v = c_1\left(\frac{1}{a_1}w - \frac{a_2}{a_1}v_2 - \cdots - \frac{a_n}{a_1}v_n\right) + c_2v_2 \cdots + c_nv_n$, so $v \in \spanset{B'}$. Therefore, $B'$ spans $V$, and so $B'$ is a basis for $V$.
\end{proof}

\section{Dimension}

\begin{defn}\label{finite-dimensional-defn}
    A vector space over $F$ is \emph{finite dimensional} if it has a finite basis.
\end{defn}

\begin{lemma}\label{basis-maximal-independent-subset}
    Let $V$ be a vector space over a field $F$, $B$ be a basis for $V$, and $S \subseteq V$ such that $B \subsetneq S$. Then $S$ is linearly dependent.
\end{lemma}

\begin{proof}
    Since $B \subsetneq S$, there exists $v \in S$ such that $v \notin B$. Since $B$ is a basis for $V$, it follows that $v \in \spanset{B}$, and so $v \in \spanset{S - \{v\}}$. By Proposition \ref{linear-dependence-implies-extra-vector}, it follows that $S$ is linearly dependent.
\end{proof}

\begin{thm}{Dimension theorem}\label{dimension-theorem}\proofbreak
    Any two bases of a finite dimensional vector space have the same cardinality.
\end{thm}

\begin{proof}
    Let $V$ be a vector space over a field $F$. Then let $B = \langle v_1, \ldots, v_n \rangle$ be a finite basis of \emph{minimal size} for $V$, and let $C$ be \emph{any other} basis for $V$. Since $B$ is of minimal size, $|C|$ is at least $n$. Let $B_0 = B$, and then take $w_1 \in C$. By the Exchange lemma \ref{exchange-lemma}, there exists $l \in \{1, \ldots, n\}$ such that $v_l$ can be swapped for $w_1$ to form a new basis. Without loss of generality, assume that $l = 1$. Then $B_1 = \langle w_1, v_2, \ldots, v_n \rangle$ is a new basis for $V$. Now take $w_2 \in C$. Since $B_1$ is a basis, we have $a_1, \ldots, a_n \in F$ such that $w_2 = a_1w_1 + a_2v_2 + \cdots + a_nv_n$. Since $C$ is linearly independent, at least one of $a_2, \ldots, a_n$  must be non-zero. Without loss of generality, assume that $a_2 \neq 0$. By the Exchange lemma \ref{exchange-lemma}, we get the basis $B_2 = \langle w_1, w_2, v_3, \ldots, v_n\rangle$. Continue to get $B_n = \langle w_1, \ldots, w_n \rangle$ such that $B_n \subseteq C$. Since $C$ is a basis and therefore linearly independent, by Lemma \ref{basis-maximal-independent-subset} it follows that $B_n = C$, and so $|C| = |B|$.
\end{proof}

\begin{defn}
    The cardinality of a basis for a finite dimensional vector space $V$ is the \emph{dimension} of $V$, denoted by $\dim(V)$.
\end{defn}

\begin{rmk}
    We can think of the dimension as the number of degrees of freedom in $V$.
\end{rmk}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $F^n$ has dimension $n$.
        \item $P_n(F)$ has dimension $n+1$.
        \item $\left\{(-1, 1, 0), (-1, 0, 1)\right\}$ is a basis for $\left\{\left(x, y, z\right) \in \R^3 \compbar x + y + z = 0 \right\}$, so it has dimension $2$.
        \item $\{\vec{0}\}$ has dimension zero, since $\emptyset$ is a basis.
    \end{itemize}
\end{exmp}

\begin{cor}\label{basis-span-independence}
    Let $V$ be a vector space of dimension $n$, and let $S \subseteq V$. Then:
    \begin{enumerate}
        \item If $S$ is linearly independent, then $S$ has at most $n$ elements.
        \item If $S$ is linearly independent, it can be completed to a basis for $V$ (i.e. there exists a basis $B$ for $V$ such that $S \subseteq B$).
        \item If $S$ has exactly $n$ elements, then $S$ is linearly independent if and only if $\spanset{S} = V$.
    \end{enumerate}
\end{cor}

\begin{rmk}
    The third statement implies that if $|S| = n$, to prove that $S$ is a basis, it only needs to be proved that $S$ is linearly independent, or that $S$ spans $V$, not both.
\end{rmk}

\begin{exmp}
    Since $\R^2$ has dimension $2$, to prove that $S = \left\{(5, 3), (1, 4)\right\}$ is a basis for $\R^2$, we only need to prove $S$ is linearly independent. Since they are not scalar multiples of each other, $S$ is linearly independent and so it is a basis for $\R^2$.
\end{exmp}

\begin{proof}\proofbreak
    \begin{enumerate}
        \item Assume that $|S| > n$. Using the Exchange lemma \ref{exchange-lemma}, and the fact that $S$ is linearly independent, we can construct a basis $B_n$ for $V$ such that $B_n \subseteq S$. By Lemma \ref{basis-maximal-independent-subset}, it follows that $B_n = S$, which would imply $|S| = n$. This is a contradiction, and so $|S| \leq n$.
        \item If $V = \spanset{S}$, we are done. If not, there must exist some $v \notin \spanset{S}$. By Proposition \ref{linear-independence-with-extra-vector}, $S \union v$ is linearly independent. Continue until the set has $n$ elements, then since by (1) $n+1$ elements would be linearly dependent, it follows that every $v \in \spanset{\{S \union v_1 \union v_2 \union \ldots\}}$. Then $\{S \union v_1 \union v_2 \union \ldots\}$ must be a basis for $V$.
        \item ($\implies$) If $S$ is linearly independent, then by (2) we can complete $S$ to a basis $B$ such that $S \subseteq B$. Since $|S| = n = |B|$, we have $S = B$, and so $\spanset{S} = V$. \\ ($\impliedby$) Instead, if $\spanset{S} = V$, then by Proposition \ref{linearly-independent-subset-existence} there exists $T \subseteq S$ such that $\spanset{T} = \spanset{S} = V$ and $T$ is linearly independent. Since $|S| = n = |T|$, it follows that $S = T$ and so $S$ must be linearly independent.
    \end{enumerate}
\end{proof}

\begin{rmk}\proofbreak
    \begin{itemize}
        \item If $S$ spans $V$, then $|S|$ is at least $n$.
        \item A basis is a minimal spanning set, and vice versa.
        \item A basis is a maximal linearly independent set, and vice versa.
    \end{itemize}
\end{rmk}

\begin{thm}\label{subspace-is-finite}
    Let $V$ be a finite dimensional vector space with dimension $n$, and $W \subseteq V$ a subspace. Then:
    \begin{enumerate}
        \item $W$ has dimension less than or equal to that of $V$.
        \item $W$ is finite dimensional.
        \item $W = V$ if and only if the dimension of $W$ is equal to that of $V$.
    \end{enumerate}
\end{thm}

\begin{proof}\proofbreak
    \begin{enumerate}
        \item By Corollary \ref{basis-span-independence} (2), we can complete $\emptyset$ to a basis $B$ for $W$. Since $B$ is linearly independent and $B \subseteq V$, by Corollary \ref{basis-span-independence} (1), $|B| \leq n$, so $W$ has dimension less than or equal to that of $V$.
        \item Since $V$ is finite dimensional, and $W$ has dimension less than or equal to that of $V$, it follows that $W$ must also be finite dimensional.
        \item ($\implies$) If the dimensions of $V$ and $W$ are equal, and $B$ is a basis for $W$, we know that $\spanset{B} = W$. Since $B$ is a linearly independent subset of $V$ with $n$ elements, it follows by Corollary \ref{basis-span-independence} (3) that $\spanset{B} = V$, and so $W = V$. \\ ($\impliedby$) If $W = V$, then any basis $B$ for $W$ is also a basis for $V$, and so they have the same dimension.
    \end{enumerate}
\end{proof}

\section{Row and Column Spaces}

\begin{defn}
    Let $\boldsymbol{A}$ be an $m \times n$ matrix over a field $F$. Then the:
    \begin{itemize}
        \item \emph{row space} of $\boldsymbol{A}$ is the subspace of $F^n$ spanned by the rows of $A$,
        \item \emph{column space} of $\boldsymbol{A}$ is the subspace of $F^m$ spanned by the columns of $A$,
        \item \emph{row rank} of $\boldsymbol{A}$ is the dimension of the row space of $\boldsymbol{A}$,
        \item \emph{column rank} of $\boldsymbol{A}$ is the dimension of the column space of $\boldsymbol{A}$.
    \end{itemize}
\end{defn}

\begin{lemma}\label{row-ops-row-space}
    Row operations on a matrix do not change its row space.
\end{lemma}

\begin{proof}
    Let $\boldsymbol{A}$ be a matrix over a field $F$, and let $\boldsymbol{B}$ be the matrix resulting from a single row operation on $\boldsymbol{A}$.
    \begin{itemize}
        \item Swapping rows simply changes the order of rows, and since sets are unordered, the span of the rows cannot be affected.
        \item For any row $r_i$ of $\boldsymbol{A}$ and non-zero $k \in F$, since $kr_i$ is an element of the row space of $\boldsymbol{A}$, all rows of $\boldsymbol{B}$ are elements of the row space of $\boldsymbol{A}$. Therefore, the row space of $\boldsymbol{B}$ is a subset of the row space of $\boldsymbol{A}$. Since $\frac{1}{k}(kr_i)$ is an element of the row space of $\boldsymbol{B}$, it similarly follows that the row space of $\boldsymbol{A}$ is a subset of the row space of $\boldsymbol{B}$. Therefore, the row space of $\boldsymbol{A}$ equals the row space of $\boldsymbol{B}$.
        \item For any rows $r_i, r_j$ of $\boldsymbol{A}$ and non-zero $k \in F$, $r_i + kr_j$ is an element of the row space of $\boldsymbol{A}$, and $(r_i + kr_j) - kr_j$ is an element of the row space of $\boldsymbol{B}$, it similarly follows that the row space of $\boldsymbol{A}$ equals the row space of $\boldsymbol{B}$.
    \end{itemize}
\end{proof}

\begin{cor}
    Let $\boldsymbol{A}$ be an $m \times n$ matrix, and $\boldsymbol{E}$ its reduced row echelon form. Then the row space of $\boldsymbol{A}$ is equal to the row space of $\boldsymbol{E}$, since $\boldsymbol{E}$ can be obtained from $\boldsymbol{A}$ by row operations.
\end{cor}

\begin{exmp}
    \begin{align*}
        A = \begin{amatrix}{3}
            1 &2 &0 &4 \\
            3 &3 &1 &0 \\
            7 &8 &2 &4
        \end{amatrix}
         \begin{aligned}
             & \ro{r_2 \rightarrow r_2 - 3r_1}  \\
             & \ro{r_3 \rightarrow r_3 - 7r_1} \\
        \end{aligned}
         & \begin{amatrix}{3}
            1 &2 &0 &4 \\
            0 &-3 &1 &-12 \\
            0 &-6 &2 &-24
        \end{amatrix}
         \begin{aligned}
             & \ro{r_2 \rightarrow -1/3r_2} \\
        \end{aligned} \\
         & \begin{amatrix}{3}
            1 &2 &0 &4 \\
            0 &1 &-1/3 &4 \\
            0 &-6 &2 &-24
        \end{amatrix}
         \begin{aligned}
             & \ro{r_1 \rightarrow r_1 - 2r_2} \\
             & \ro{r_3 \rightarrow r_3 + 6r_2} \\
        \end{aligned}
         & \begin{amatrix}{3}
            1 &0 &2/3 &-4 \\
            0 &1 &-1/3 &4 \\
            0 &0 &0 &0
        \end{amatrix} = E
    \end{align*}

    Since $\boldsymbol{E}$ was obtained from $\boldsymbol{A}$ by row operations, the row space of $\boldsymbol{A}$ is equal to the row space of $\boldsymbol{E}$, which is $\spanset{\{(1, 0, 2/3, -4), (0, 1, -1/3, 4)\}}$. Since $\boldsymbol{E}$ has two linearly independent rows, the row rank of $\boldsymbol{A}$ is $2$.
\end{exmp}

\begin{lemma}
    Non-zero rows of a matrix in row echelon form are linearly independent.
\end{lemma}

\begin{cor}
    Let $\boldsymbol{A}$ be an $m \times n$ matrix, and $\boldsymbol{E}$ the row echelon form. Then the non-zero rows of $\boldsymbol{E}$ form a basis for the row space of $\boldsymbol{A}$, and the row rank of $\boldsymbol{A}$ is equal to the number of non-zero rows in $\boldsymbol{E}$.
\end{cor}

\begin{lemma}\label{row-ops-column-rank}
    Row operations on a matrix do not change its column rank.
\end{lemma}

\begin{proof}
    Let $\boldsymbol{A}$ be an $m \times n$ matrix, and let $C \subseteq F^n$ such that for every $c \in C$:
    \begin{equation}\label{matrix-solution-star}\tag{$\star$}
        \boldsymbol{A}c = c_1\begin{pmatrix} \boldsymbol{A}_{11} \\ \vdots \\ \boldsymbol{A}_{m1} \end{pmatrix} + \cdots + c_n\begin{pmatrix} \boldsymbol{A}_{1n} \\ \vdots \\ \boldsymbol{A}_{mn} \end{pmatrix} = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}.
    \end{equation}
    This is equivalent to $c \in C$ being a solution to the homogeneous system of linear equations represented by $\boldsymbol{A}$. If $\boldsymbol{A}$ and $\boldsymbol{B}$ are related by row operations, then by Theorem \ref{solutions-unchanged-by-row-ops} their solutions sets are the same. Therefore, $c$ satisfies \ref{matrix-solution-star} for $\boldsymbol{A}$ if and only if it does so for $\boldsymbol{B}$.

    Any particular set of columns in $\boldsymbol{A}$ are linearly dependent if and only if there exists $c \in C$ such that the coefficients in $c$ corresponding to each column are non-zero. Therefore, the same columns are linearly independent in $\boldsymbol{A}$ and $\boldsymbol{B}$, implying that the cardinalities of bases for $\boldsymbol{A}$ and $\boldsymbol{B}$ are the same (since this is equal to the maximum number of linearly independent columns), and so $\boldsymbol{A}$ and $\boldsymbol{B}$ must have the same column rank.
\end{proof}

\begin{cor}\label{row-ops-column-independence}
    A set of columns of $\boldsymbol{A}$ are linearly independent if and only if the corresponding columns are also linearly independent in $\boldsymbol{B}$.
\end{cor}

\begin{defn}
    The \emph{pivot columns} of a matrix $\boldsymbol{A}$ are those corresponding to the leading terms in the reduced row echelon form of $\boldsymbol{A}$.
\end{defn}

\begin{lemma}\label{pivot-columns-form-basis}
    The pivot columns of a matrix $\boldsymbol{A}$ over a field $F$ form a basis for the column space of $\boldsymbol{A}$.
\end{lemma}

\begin{proof}
    Let $\boldsymbol{E}$ be the reduced row echelon form of $\boldsymbol{A}$. Since the pivot columns of $\boldsymbol{E}$ are a subset of the standard basis for $F^n$, they must be linearly independent. By the shape of $\boldsymbol{E}$, the remaining columns must be linear combinations of the pivot columns, so the set of the pivot columns spans the column space of $\boldsymbol{E}$. By Corollary \ref{row-ops-column-independence}, the same follows for $\boldsymbol{A}$, and so the pivot columns form a basis for the column space of $\boldsymbol{A}$.
\end{proof}

\begin{thm}
    Let $\boldsymbol{A}$ be a matrix. Then the row rank of $\boldsymbol{A}$ is equal to its column rank.
\end{thm}

\begin{proof}
    Let $\boldsymbol{E}$ be the reduced row echelon form of $\boldsymbol{A}$. By Lemma \ref{row-ops-row-space}, it follows that the row rank of $\boldsymbol{A}$ is equal to the row rank of $\boldsymbol{E}$. This is of course equal to the number of non-zero rows of $\boldsymbol{E}$, or the number of leading terms. By definition, this is equal to the number of pivot columns of $\boldsymbol{A}$, and by Lemma \ref{pivot-columns-form-basis} this is equal to the column rank of $\boldsymbol{A}$.
\end{proof}

\begin{defn}
    Let $\boldsymbol{A}$ be a matrix. Then the \emph{rank} of $\boldsymbol{A}$ is equal to the row/column rank of $\boldsymbol{A}$.
\end{defn}

\begin{rmk}
    A system of homogeneous linear equations represented by an $m \times n$ matrix $\boldsymbol{A}$ has a unique solution if and only if the rank of $\boldsymbol{A}$ is $n$.

    The set of solutions to the system is a vector subspace of $F^n$ whose dimension is equal to $n$ minus the rank of $\boldsymbol{A}$, which is also equal to the number of free variables in the system.

    A non-homogeneous system represented by $\boldsymbol{A}$ has solutions if and only if the augmentation vector is in the column space of $\boldsymbol{A}$.

    If $m$ is equal to the rank of $\boldsymbol{A}$, then the column space of $\boldsymbol{A} = F^m$, so every possible augmentation vector has a solution.
\end{rmk}

\section{Linear Transformations}

\begin{defn}\label{linear-transformation}
    Let $V$ and $W$ be a vector space over some field $F$. A \emph{linear transformation} $f: v \to w$ is a function satisfying for all $v, v' \in W$ and $r \in F$ the following:
    \begin{itemize}
        \item $f(v + v') = f(v) + f(v')$ --- the function preserves vector addition.
        \item $f(rv) = rf(v)$ --- the function preserves scalar multiplication.
    \end{itemize}
    Taking these properties together, we say that a linear transformation preserves the linear structure of the underlying vector space.

    A linear transformation is also known as a \emph{linear map}, \emph{linear homomorphism}, or simply a \emph{homomorphism}.
\end{defn}

\begin{exmp}\proofbreak
    If $f: \{\vec{0}\} \to V$, with $f: \vec{0} \mapsto \vec{0}$, then $f$ is trivially a linear transformation.

    If $g: V \to \{\vec{0}\}$, with $g: v \mapsto \vec{0}$, then $g$ is also trivially a linear transformation.
\end{exmp}

\begin{rmk}
    A function $f$ is a linear transformation if and only if $f(v + rv') = f(v) + rf(v')$ for all $v, v' \in V$ and $r \in F$.
\end{rmk}

\begin{exmp}
    Let $f: \R^3 \to \R^2$ be the function given by $f\left(
    \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} =
    \begin{pmatrix}
        2x + 3y \\ x + y - 3z
    \end{pmatrix}\right)$. Let $\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}, \begin{pmatrix}
        x' \\ y' \\ z'
    \end{pmatrix} \in \R^3$, and $r \in \R$. Then \[f\left(\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} + r\begin{pmatrix}
        x'\\ y' \\ z'
    \end{pmatrix}\right) = \begin{pmatrix}
        2(x + rx') + 3(y + ry') \\
        (x + rx') + (y + ry') - 3(z + z')
    \end{pmatrix} = \]
    \[\begin{pmatrix}
        2x + 3y \\
        x + y - 3z
    \end{pmatrix} + r\begin{pmatrix}
        2x' + 3y' \\
        x' + y' - 3z'
    \end{pmatrix} = f\left(\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}\right) + rf\left(\begin{pmatrix}
        x'\\ y' \\ z'
    \end{pmatrix}\right).\]
    Therefore, $f$ is a linear transformation.
\end{exmp}

\begin{exmp}
    Let $f: \R^2 \to \R^2$ give a rotation of $\frac{\pi}{2}$ anti-clockwise around the origin.
\end{exmp}

\begin{exmp}\proofbreak
    \begin{itemize}
        \item $f: \R \to \R$ where $f(x) = x^2$ is \emph{not} a linear transformation.
        \item $f: \R \to \R$ with $f(x) = x + 1$ is \emph{not} a linear transformation.
    \end{itemize}
\end{exmp}

\begin{prop}\label{zero-maps-to-zero}
    Let $f: V \to W$ be a linear transformation. Then $f(\vec{0}_V) = \vec{0}_W$.
\end{prop}

\begin{proof}
    Since $f$ preserves scalar multiplication, we have $f(\vec{0}_V) = f(0 \cdot \vec{0}_V) = 0f(\vec{0}_V) = 0$.
\end{proof}

\begin{prop}\label{action-on-basis}
    A linear transformation is uniquely determined by its action on a basis.

    Let $V$ and $W$ be vector spaces over a field $F$, and $B$ be a basis for $V$. For each $b \in B$, let $w_b \in W$. Then there exists a unique linear transformation $f: V \to W$ such that $f(b) = w_b$ for all $b \in B$.
\end{prop}

\begin{exmp}
    Let $f: \R^3 \to \R^2$ be a linear transformation such that $f\left(\begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix}\right) = \begin{pmatrix}
        2 \\ 1
    \end{pmatrix}$, $f\left(\begin{pmatrix}
        0 \\ 1 \\ 0
    \end{pmatrix}\right) = \begin{pmatrix}
        3 \\ 1
    \end{pmatrix}$, and $f\left(\begin{pmatrix}
        0 \\ 0 \\ 1
    \end{pmatrix}\right) = \begin{pmatrix}
        0 \\ -3
    \end{pmatrix}$. Since $f$ is a linear transformation, it follows that \[f\left(\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}\right) = xf\left(\begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix}\right) + yf\left(\begin{pmatrix}
        0 \\ 1 \\ 0
    \end{pmatrix}\right) + zf\left(\begin{pmatrix}
        0 \\ 0 \\ 1
    \end{pmatrix}\right) = \begin{pmatrix}
        2x \\ 1x
    \end{pmatrix} + \begin{pmatrix}
        3y \\ 1y
    \end{pmatrix} + \begin{pmatrix}
        0 \\ -3z
    \end{pmatrix},\] so we have \[f\left(\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}\right) = \begin{pmatrix}
        2x + 3y \\ x + y - 3z
    \end{pmatrix}.\]
\end{exmp}

\begin{proof}\proofbreak

To prove the uniqueness of $f$ (without proving existence), let $v \in V$. By Proposition \ref{unique-basis-expression}, there exists unique $a_i \in F$ such that $v = a_1b_1 + \cdots + a_nb_n$. By the definition of a linear transformation (\ref{linear-transformation}), it follows that if $f$ exists then $f(v) = a_1f(b_1) + \cdots + a_nf(b_n) = a_1w_{b_1} + \cdots + a_nw_{b_n}$, so $f$ is completely determined.

To prove the existence of $f$, we need to show that the function defined by $f(v) = a_1w_{b_1} + \cdots + a_nw_{b_n}$ is a linear transformation and satisfies $f(b) = w_b$ for all $b \in B$. That latter is simply true by construction. For the former, given $v, v' \in V$ and $r \in F$, we have $v = a_1b_1 + \cdots a_nb_n$ and $v' = a'_1b_1 + \cdots + a'_nb_n$ for $a_i, a'_i \in F$. Then $v + rv' = (a_1 + ra'_1)b_1 + \cdots + (a_n + ra'_n)b_n$. It follows that $f(v + rv') = (a_1 + ra'_1)f(b_1) + \cdots + (a_n + ra'_n)f(b_n) = (a_1w_{b_1} + \cdots + a_nw_{b_n}) + r(a'_1w_{b_1} + \cdots + a'_nw_{b_n}) = f(v) + rf(v')$. Therefore, $f$ is a linear transformation.
\end{proof}

\begin{exmp}
    Let $f: \R^2 \to \R^2$ be anti-clockwise rotation about the origin by $\frac{\pi}{2}$. Then, $f(1, 0) = f(0, 1)$ and $f(0, 1)$ must equal $f(-1, 0)$. Then it follows that \[f(x, y) = xf(1, 0) = yf(0, 1) = x(0, 1) + y(-1, 0) = (-y, x).\]
\end{exmp}

\begin{exmp}
    For general anti-clockwise rotation about the origin by $\theta$, it follows that $f(1, 0) = (\cos(\theta), \sin(\theta))$ and $f(0, 1) = (-\sin(\theta), \cos(\theta))$. Therefore, \[f(x, y) = x(\cos(\theta), \sin(\theta)) + y(-\sin(\theta), \cos(\theta)) = (x\cos(\theta) - y\sin(\theta), x\sin(\theta) + y\cos(\theta)).\]
\end{exmp}

\begin{prop}
    Let $f: V \to W$ be a linear transformation over a field $F$, and $U \subseteq V$ a subspace. Then the image of $U$ under $f$ (which is given by $f(U) = \left\{f(u) \compbar u \in U\right\}$) is a subspace (see Definition \ref{subspace-defn}) of $W$.
\end{prop}

\begin{proof}
    Since $U$ must be non-empty, it follows that $f(U)$ is as well. Then since $f$ is a linear transformation, it follows that $f(u) + rf(u') = f(u + ru')$ for every $u, u' \in U$ and $r \in F$, and since $u + ru' \in U$ it follows that $f(U)$ is closed under vector addition and scalar multiplication. Therefore, by Proposition \ref{subspace-proof}, we know that $f(U)$ is a subspace of $W$.
\end{proof}

\begin{defn}
    Let $V, W$ be vector spaces and $f: V \to W$ a linear transformation. The \emph{rank} of $f$ is the dimension of $f(V)$ (the image of $V$ under $f$).
\end{defn}

\begin{exmp}
    Let $f: \R^2 \to \R^3$ be the linear transformation determined by $f(1, 0) = (2, 1, 0)$ and $f(0, 1) = (0, -1, 1)$. Then $f(\R^2) = \spanset{\{(2, 1, 0), (0, -1, 1)\}}$. Since these vectors are linearly independent, they form a basis for $f(\R^2)$, and so the rank of $f$ is 2.
\end{exmp}

\begin{prop}
    Let $V, W$ be vector spaces over a field $F$, $f: V \to W$, and $U \subseteq W$ a subspace. Then the \emph{preimage} or \emph{inverse image} of $U$ under $f$ (which is given by $f^{-1}(U) = \left\{v \in V \compbar f(v) \in U\right\}$) is a subspace of $V$.
\end{prop}

\begin{proof}
    Since by Definition \ref{subspace-defn} we know that $\vec{0}_W \in U$, and by Proposition \ref{zero-maps-to-zero} we know that $f(\vec{0}_V) = \vec{0}_W$, it follows that $\vec{0}_V \in f^{-1}(U)$, and so $f^{-1}(U)$ is non-empty. Let $v, v' \in f^{-1}(U)$ and $r \in F$. Then $f(v), f(v') \in U$, and since $U$ is a subspace $f(v) + rf(v') \in U$. Since $f$ is a linear transformation, it follows that $f(v + rv') \in U$, and so $v + rv' \in f^{-1}(U)$. Therefore, $f^{-1}(U)$ is non-empty and closed under vector addition and scalar multiplication, so $f^{-1}(U)$ is a subspace of $V$ by Proposition \ref{subspace-proof}.
\end{proof}

\begin{defn}
    Let $f: V \to W$ be a linear transformation. Then the \emph{kernel} or \emph{null space} of $f$, commonly denoted by $\ker f$, is the inverse image of $\{\vec{0}_W\}$: \[\ker f = f^{-1}(\{\vec{0}_W\}) = \left\{v \in V \compbar f(v) = \vec{0}_W\right\}.\] The kernel of $f$ is a subspace of $V$, the dimension of which is the \emph{nullity} of $f$.
\end{defn}

\begin{exmp}
    Let $f: \R^2 \to \R^3$ be the linear transformation $f(x, y) = (2x, x-y, y)$. Then $f(x, y) = (0, 0, 0)$ if and only if $2x = 0$, $x - y = 0$, and $y = 0$, which implies $x = y = 0$. Therefore, $\ker f = \{(0, 0)\}$, and the nullity is zero.
\end{exmp}

\begin{exmp}
    Let $f: P_2(\R) \to \R^2$ be the linear transformation $f(ax^2 + bx + c) = (a + b, a + c)$. Then $f(ax^2 + bx + c) = (0, 0)$ if and only if $a + b = 0$ and $a + c = 0$, so $b = c$, and $a = -c$. Therefore, $\ker f = \left\{-cx^2 + cx + c \compbar c \in \R \right\} = \spanset{\{-x^2 + x + 1\}}$, so the nullity of $f$ is one.

    Since the image of $f$ is \[\spanset{\{f(x^2), f(x), f(1)\}} = \spanset{\{(1, 1), (1, 0), (0, 1)\}} = \spanset{\{(1, 0), (0, 1)\}},\] the rank of $f$ is two.
\end{exmp}

\begin{rmk}
    If $f: V \to W$ is a linear transformation, and $B$ is a basis of $V$, then the image of $f$ equals the span of $f(B)$. However, $f(B)$ is not necessarily linearly independent.
\end{rmk}

\begin{prop}\label{kernel-image-are-finite}
    Let $f: V \to W$ be a linear transformation, where $V$ is a finite dimensional vector space with basis $B$. Then both the kernel and image of $f$ are finite dimensional.
\end{prop}

\begin{proof}
    Since the kernel of $f$ is a subspace of $V$, the kernel of $f$ is finite dimensional by Theorem \ref{subspace-is-finite}.

    Since $V$ is finite dimensional, by Definition \ref{finite-dimensional-defn} $B$ is finite, and so $f(B)$ is as well. By Proposition \ref{linearly-independent-subset-existence} it follows that there is a linearly independent subset of $f(B)$ which spans the image of $f$, which is then a basis for the image of $f$ by Corollary \ref{basis-span-independence}. Since $f(B)$ is finite, any subset must be as well, and so the image of $f$ is finite dimensional.
\end{proof}

\begin{thm}\label{rank-nullity}
    Rank-nullity theorem. Let $V, W$ be vector spaces over a field $F$, $V$ be finite dimensional, and $f: V \to W$ a linear transformation. Then the sum of the nullity and rank of $f$ is equal to the dimension of $V$.
\end{thm}

\begin{proof}
    Let $\{v_1, \ldots, v_k\}$ be a basis for $\ker f$, which by Corollary \ref{basis-span-independence} can be completed to a basis $\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n\}$ for $V$.

    Let $v \in V$, so there exists $a_i \in F$ (for $i = 1, \ldots, n$) such that $a1v_1 + \cdots + a_nv_n = v$. Then $f(v) = a_1f(v_1) + \cdots + a_nf(v_n)$. However, $f(v_1), \ldots, f(v_n)$ are all zero, so $f(v) = a_{k+1}f(v_{k+1}) + a_nf(v_n)$. It follows that $f(V) = \spanset{\{f(v_{k+1}), \ldots, f(v_n)\}}$.

    Now assume there exists $a_{k+1}, \ldots, a_n \in F$ not all zero such that $a_{k+1}f(v_{k+1}) + \cdots + a_nf(v_n) = \vec{0}_W$. This would then imply $f(a_{k+1}v_{k+1} + \cdots + a_nv_n) = \vec{0}_W$, so $a_{k+1}v_{k+1} + \cdots + a_nv_n \in \ker f$. Since $\ker f = \spanset{v_1, \ldots, v_k}$, there must then exist $a_1, \ldots, a_n \in F$ such that $a_1v_1 + \cdots + a_kv_k = a_{k+1}v_{k+1} + \cdots + a_nv_n$, so $a_1v_1 + \cdots + a_kv_k + (-a_{k+1})v_{k+1} + \cdots + (-a_n)v_n = \vec{0}_V$. Then $\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n\}$ must be linearly dependent, which contradicts the assumption that it is a basis. Therefore, no such $a_{k+1}, \ldots, a_n \in F$ exist, and so $\{f(v_{k+1}), \ldots, f(v_n)\}$ must be linearly independent.

    Since $\{f(v_{k+1}), \ldots, f(v_n)\}$ is both linearly independent and spans $f(V)$, by definition it is a basis for the image of $f$. Therefore, it follows that the dimension of $V$ is equal to the sum of the nullity and rank of $f$.
\end{proof}

\section{Isomorphisms}

\begin{defn}
    Let $A$ and $B$ be sets, and $f: A \to B$. We say that $f$ is \begin{itemize}
        \item \emph{injective} if $f(a) = f(a') \implies a = a'$ for all $a, a' \in A$,
        \item \emph{surjective} if for all $b \in B$ there exists $a \in A$ such that $f(a) = b$,
        \item and \emph{bijective} if $f$ is both injective and surjective.
    \end{itemize}
\end{defn}

\begin{thm}
    Let $f: A \to B$ be a function, then $f$ is bijective if and only if there exists a function $g: B \to A$ such that $g \circ f = \identity_A$ and $f \circ g = \identity_B$.
\end{thm}

\begin{lemma}
    Let $f: V \to W$ be a linear transformation. Then $f$ is injective if and only if the null space of $f$ is $\{\vec{0}_V\}$.
\end{lemma}

\begin{proof}\proofbreak
    ($\implies$) Suppose $f$ is injective, and let $v \in \ker f$, so $f(v) = \vec{0}_W$. Since $f$ is a linear transformation, we know that $f(\vec{0}_V) = \vec{0}_W$, and since $f$ is injective it follows that $\ker f = \{\vec{0}_V\}$.

    ($\impliedby$) Suppose $\ker f = \{\vec{0}_V\}$, and let $v, v' \in V$ such that $f(v) = f(v')$. Then $f(v) - f(v') = \vec{0}_W$, so $f(v - v') = \vec{0}_W$. It follows that $v - v' = \vec{0}_V$, so $v = v'$. Therefore, $f$ is injective.
\end{proof}

\begin{cor}\label{injective-nullity-rank}
    Let $V, W$ be vector spaces, where $V$ is finite dimensional, and $f: V \to W$ a linear transformation. Then the following are all equivalent:
    \begin{itemize}
        \item $f$ is injective.
        \item The nullity of $f$ is zero.
        \item The rank of $f$ is the dimension of $V$.
        \item If $\{v_1, \ldots, v_n\}$ is a basis for $V$, then $\{f(v_1), \ldots, f(v_n)\}$ is a basis for $f(V)$.
    \end{itemize}
\end{cor}

\begin{defn}
    Let $V, W$ be vector spaces. Then an \emph{isomorphism} $f: V \to W$ is a bijective linear transformation. If an isomorphism exists for $V$ and $W$, then we say that $V$ and $W$ are \emph{isomorphic}.
\end{defn}

\begin{rmk}
    Being isomorphic is an equivalence relation on the collection of all vector spaces over a field $F$.
\end{rmk}

\begin{exmp}
    $f: P_n(F) \to F^{n+1}$ where $f(a_0 + a_1x + \cdots + a_nx^n) = \begin{pmatrix}
        a_0 \\ a_1 \\ \vdots \\ a_n
    \end{pmatrix}$.
\end{exmp}

\begin{exmp}
    $g: M_{2\times2}(F) \to F^4$ where $\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix} \mapsto \begin{pmatrix}
        a \\ b \\ c \\ d
    \end{pmatrix}$.
\end{exmp}

\begin{prop}
    If $f$ is an isomorphism, it is bijective by definition, so there exists an inverse function $f^{-1}: W \to V$. $f^{-1}$ is also a linear transformation.
\end{prop}

\begin{proof}
    Let $f: V \to W$ be an isomorphism between vector spaces $V$ and $W$ over a field $F$, and let $g: W \to V = f^{-1}$. Consider $w, w' \in W$ and $r \in F$. Let $v, v' \in V$ such that $v = g(w)$ and $v' = g(w')$. Since $f$ is bijective, $v, v'$ must be unique. Then $f(v + rv') = f(v) + rf(v') = w + rw'$. Therefore, $g(w + rw') = v + rv' = g(w) + rg(w')$, so $g$ must be a linear transformation.
\end{proof}

\begin{thm}
    Suppose $V$ and $W$ are finite dimensional vector spaces over a field $F$. Then $V$ and $W$ are isomorphic if and only if $V$ and $W$ have the same dimension.
\end{thm}

\begin{proof}\proofbreak
    ($\implies$) Let $f: V \to W$ be an isomorphism. Then by Corollary \ref{injective-nullity-rank}, since $f$ is injective it follows that the nullity of $f$ is zero and the rank of $f$ is the dimension of $W$. By the Rank-nullity theorem \ref{rank-nullity}, it follows that the rank of $f$ is equal to the dimension of $V$, so $V$ and $W$ have the same dimension.

    ($\impliedby$) Let $n$ be the dimension of $V$ and $W$. Then take $\langle v_1, \ldots, v_n \rangle$ and $\langle w_1, \ldots, w_n \rangle$ ordered bases for $V$ and $W$ respectively. Define $f: V \to W$ by $f(v_i) = w_i$ for $i = 1, \ldots, n$, and extend linearly, which gives a unique linear transformation by Proposition \ref{action-on-basis}. Since the image of $f$ is $\spanset{\{f(v_1), \ldots, f(v_n)\}}$, which by construction is equal to $\spanset{\{w_1, \ldots, w_n\}}$, so $f$ is surjective. By Corollary \ref{injective-nullity-rank}, since $\langle w_1, \ldots, w_n \rangle$ is linearly independent (as it is a basis for $W$), the rank of $f$ is $n$, so we have that $f$ is injective. Therefore, $f$ is bijective and so $V$ and $W$ are isomorphic.
\end{proof}

\begin{cor}
    If the dimension of $V$ is $n$, then $V$ is isomorphic to $F^n$.
\end{cor}

\begin{prop}\label{coordinates-are-isomorphism}
    Let $V$ be a finite dimensional vector space over a field $F$, with dimension $n$ and let $B$ be a basis for $V$. The coordinates with respect to $B$ define a function $f: V \to F^n$ where $f: v \mapsto [v]_B$, which is an isomorphism between $V$ and $F^n$.
\end{prop}

\begin{proof}
    Let $B = \langle v_1, \ldots, v_n\rangle$, and let $v = c_1v_1 + \cdots + c_nv_n, w = d_1v_1 + \cdots + d_nv_n \in V$. Then $v + w = (c_1 + d_1)v_1 + (c_n + d_n)v_n$, so $[v+w]_B = [v]_B + [w]_B$. Let $r \in F$, then $rv = rc_1v_1 + \cdots + rc_nv_n$, so $[rv]_B = r[v]_B$. It follows that this function is a linear transformation.

    Since $B$ is linearly independent and spans $V$, by Proposition \ref{unique-basis-expression} the coordinates of every $v \in V$ are unique and since every set of coordinates $x \in F^m$ defines a unique $v \in V$, so $f$ is bijective. It follows that $f$ is an isomorphism.
\end{proof}

\begin{defn}
    Let $V$ and $W$ be finite dimensional vector spaces over a field $F$, $B = \langle v_1, \ldots, v_n\rangle$, and $D = \langle w_1, \ldots, w_n\rangle$ be ordered bases for $V$ and $W$ respectively. Let $f: V \to W$ be a linear transformation. Consider the following $m \times n$ matrix over $F$:
    \[[f]_B^D = \left([f(v_1)]_D \; [f(v_2)]_D \;\cdots\; [f(v_n)]_D \right).\]
    This is the \emph{matrix representation} of $f$ with respect to $B$ and $D$.
\end{defn}

\begin{exmp}
    Let $f: \R^2 \to \R^3$ be the linear transformation given by $f(1, 1) = (0, 2, -1)$ and $f(2, 0) = f(1, 1, 3)$. Let $B = \langle (1, 1), (2, 0) \rangle$ and $D = \langle (0, 2, -1), (1, 1, 3), (0, 0, 1)\rangle$. Then
    \[[f]_B^D = \begin{pmatrix}
        1 & 0 \\ 0 & 1 \\ 0 & 0
    \end{pmatrix}.\] Let $D' = \langle (1, 0, 0), (0, 1, 0), (0, 0, 1)\rangle$ be the standard basis for $\R^3$. Then
    \[[f]_B^{D'} = \begin{pmatrix}
        0 & 1 \\ 2 & 1 \\ -1 & 3
    \end{pmatrix}.\]
\end{exmp}

\begin{defn}
    Let $F$ be a field, and $x, y \in \F^n$. The \emph{dot product} of $x$ and $y$, denoted by $x \cdot y$, is given by
    \[x \cdot y = x_1y_1 + x_2y_2 + \cdots + x_ny_n \in F.\]
\end{defn}

For an $m \times n$ matrix $A$, and a vector $x \in F^n$, we define the product $Ax \in F^m$ to be
\[\begin{pmatrix}
    A_{1,1} & \cdots & A_{1, n} \\
    \vdots & \ddots & \vdots \\
    A_{m,1} & \cdots & A_{m, n} \\
\end{pmatrix}\begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix} = \begin{pmatrix}
    A_{1,1}x_1 + \cdots + A_{1, n}x_n \\
    \vdots \\
    A_{m,1}x_1 + \cdots + A_{m, n}x_n \\
\end{pmatrix}.\]

\begin{rmk}
    $(Ax)_i$ is the dot product of the $i$-th row of $A$ with $x$.
\end{rmk}

\begin{defn}
    Let $V$ and $W$ be vector fields with ordered bases $B = \langle v_1, \ldots, v_n \rangle$ and $D = \langle w_1, \ldots, w_m \rangle$, and let $A$ be an $m \times n$ matrix (where $m = \dim W$ and $n = \dim V$). Let $L_A$ be a linear transformation $L_A: V \to W$ such that $L_A(v_i) = A_{1,i}w_1 + \cdots + A_{m,i}w_m$.
\end{defn}

\begin{rmk}
    By Theorem \ref{action-on-basis}, $L_A$ is unique.
\end{rmk}

\begin{prop}\label{matrix-transformation-duality}
    Let $A$ be an $m \times n$ matrix over a field $F$. For $x \in F^n$, $Ax = L_A(x)$.
\end{prop}

\begin{proof}
    Since $x = x_1e_1 + x_2e_2 + \cdots + x_ne_n$, where $e_i$ is the $i$th element of the standard basis for $F^n$, it follows by linearity that
    \begin{align*}
        L_A(x) = \,&x_1(A_{1, 1}h_1 + \cdots + A_{m, 1}h_m) +\\
        &x_2(A_{1, 2}h_1 + \cdots + A_{m, 2}h_m) + \\
        &\phantom{x_1(A_{1, 1}h_1 +\;\;\;} \vdots \\
        &x_n(A_{1, n}h_1 + \cdots + A_{m, n}h_m),
    \end{align*}
    where $h_i$ is the $i$th element of the standard basis for $F^m$. Therefore, the $i$th component of $L_A(x)$ is $A_{1, 1}x_1 + A_{1, 2}x_2 + \cdots + A_{1, n}x_n$, which is the dot product of the $i$-th row of $A$ with $x$. It follows by the definition of $Ax$ that $L_A(x) = Ax$.
\end{proof}

\begin{thm}
    Let $V,W$ be vector spaces over a field $F$, and let $f: V \to W$ be a linear transformation. Let $B$ and $D$ be ordered bases for $V$, $W$ respectively. Then for all $v \in V$,
    \[[f(v)]_D = [f]_B^D[v]_B.\]
\end{thm}

\begin{proof}
    Let $n$ and $m$ be the dimensions of $V$ and $W$ respectively, let $A = [f]_B^D$, and let $x = [v]_B$. By definition \[Ax = x_1[f(v_1)]_D + x_2[f(v_2)]_D + \cdots + x_n[f(v_n)]_D.\] Since $f$ is a linear transformation, it follows that $Ax = [f(v)]_D$, and since $Ax = [f]_B^D[v]_D$ by definition, it follows that \[[f(v)]_D = [f]_B^D[v]_B.\]
\end{proof}

\begin{rmk}
    Given $V, W, B, D$, we have a bijection $\{f: V \to W\} \to M_{m \times n}(F)$, where $f \mapsto [f]_B^D$.
\end{rmk}

\begin{cor}
    Left multiplication by a matrix is a linear transformation.
\end{cor}

\begin{exmp}
    Let $f: P_2(\R) \to P_3(\R)$ be the linear transformation given by $p(x) \mapsto 2p'(x) + xp(x)$, and let $B = \langle 1, x, x^2\rangle$ and $D = \langle 1, x, x^2, x^3\rangle$. We then have $f(1) = 1x$, $f(x) = 2 + x^2$, and $f(x^2) = 4x + x^3$, so
    \[[f]_B^D = \begin{pmatrix}
        0 & 2 & 0 \\ 1 & 0 & 4 \\ 0 & 1 & 0 \\ 0 & 0 & 1
    \end{pmatrix}.\]
    \[f(3 + 2x + x^2) = \begin{pmatrix}
        0 & 2 & 0\\
        1 & 0 & 4\\
        0 & 1 & 0\\
        0 & 0 & 1\\
    \end{pmatrix}\begin{pmatrix}
        3 \\ 2 \\ 1
    \end{pmatrix} = \begin{pmatrix}
        4 \\ 7 \\ 2 \\ 1
    \end{pmatrix} = 4 + 7x + 2x^2 + x^3.\]
\end{exmp}

\section{Operations on Linear Transformations}

\begin{thm}\label{transformation-matrix-rank-equality}
    Let $f: V \to W$ be a linear transformation, where $V$ and $W$ are finite dimensional vector spaces with dimension $n$ and $m$ respectively. Let $B$ and $D$ be ordered bases for $V$ and $W$ respectively. Then the rank of $f$ is equal to the rank of $[f]_B^D$.
\end{thm}

\begin{proof}
    Let $l$ be the rank of $f$. Recall that the rank of $f$ is the dimension of $f(V) \subseteq W$, and the rank of $[f]_B^D$ is equal to the dimension of the column space of $[f]_B^D$, which is a subset of $F^m$. Let $C \subseteq f(D)$ be an ordered basis for $f(V)$, and let $l = |C|$. By Proposition \ref{coordinates-are-isomorphism}, coordinates define an isomorphism between $V$ and $F^m$, and so by Corollary \ref{injective-nullity-rank}, it follows that $\langle [c_1]_D, [c_2]_D, \ldots, [c_l]_D\rangle$ is linearly independent Since this is a maximal linearly independent subset of the column space of $[f]_B^D$, by Lemma \ref{basis-maximal-independent-subset} it is a basis for the column space of $[f]_B^D$, and so the rank of $f$ is equal to the rank of $[f]_B^D$.
\end{proof}

\begin{defn}
    Let $V$ and $W$ be vector spaces over a field $F$. Define $L(V, W)$ to be the set of all linear transformations from $V$ to $W$.
\end{defn}

\begin{defn}
    Define the following operations for all $f, g \in L(V, W)$, $v \in V$, and $r \in F$:
    \[(f + g)(v) = f(v) + g(v),\]
    \[(rf)(v) = r(f(v)).\]
\end{defn}

\begin{lemma}
    Let $V$ and $W$ be vector spaces over a field $F$. Let $f, g \in L(V, W)$, and $r \in F$. Then $f + g$ and $rf$ are linear transformations.
\end{lemma}

\begin{proof}
    Let $v, v' \in V$, and $s \in F$. Then
    \begin{align*}
        (f + g)(v + sv') &= f(v + sv') + g(v + sv') \\
        &= f(v) + sf(v') + g(v) + g(sv') \\
        &= (f + g)(v) + s(f + g)(v').
    \end{align*}
\end{proof}

\begin{thm}
    Let $V$ and $W$ be vector spaces over a field $F$. $L(V, W)$ forms a vector space over $F$ with the operations defined above.
\end{thm}

\begin{thm}
    Let $V, W, U$ be vector spaces over a field $F$, and let $f: V \to W$ and $g: W \to U$ be linear transformations. Then $g \circ f: V \to U$ is a linear transformation.
\end{thm}

\begin{proof}
    Let $v, v' \in V$ and $r \in F$. Then $(g \circ f)(v + rv') = g(f(v) + rf(v')) = (g \circ f)(v) + r(g \circ f)(v')$.
\end{proof}

\begin{defn}
    Let $P$ be an $l \times m$ matrix over a field $F$ and $Q$ be an $m \times n$ matrix over $F$. Then their product $PQ$ is the $l \times n$ matrix over $F$ given by
    \[(PQ)_{i,j} = \sum_k=1^m P_{i, k}Q_{k, j},\] for $i = 1, \ldots, l$ and $j = 1, \ldots, n$.
\end{defn}

\begin{rmk}
    The product of a matrix and an $n$ dimensional vector defined previously is a special case of this general matrix multiplication, where the vector is treated as an $n \times 1$ matrix.
\end{rmk}

\begin{thm}
    Let $V, W, U$ be vector spaces over a field $F$, and let $f: V \to W$ and $g: W \to U$ be linear transformations. Let $B, C, D$ be ordered bases for $V$, $W$, and $U$ respectively. Then
    \[[g \circ f]_B^D = [g]_C^D[f]_B^C.\]
\end{thm}

\begin{cor}
    Let $A$ be an $l \times m$ matrix over $F$ and $B$ be an $m \times n$ matrix over $F$. Then $L_{AB}: F^n \to F^l$ is given by $L_A \circ L_B$.
\end{cor}

\begin{cor}
    Matrix multiplication is associative, so for matrices $A, B, C$ we have
    \[A(BC) = (AB)C.\]
\end{cor}

\begin{rmk}
    If $A, M \in M_{m\times n}(F)$ and $r \in F$, we define $A + B$ and $rA$ as the $m \times n$ obtained through element-wise addition and multiplication respectively.
\end{rmk}

\begin{thm}
    Consider vector spaces $V, W$ over a field $F$. Let $f, g \in L(V, W)$ and $r \in F$. Let $B, D$ be ordered bases for $V$ and $W$ respectively. Then
    \begin{align*}
        [f + g]_B^D &= [f]_B^D + [g]_B^D, \\
        [rf]_B^D &= r[f]_B^D.
    \end{align*}
\end{thm}

\begin{proof}
    Let $P = [f]_B^D$ and $Q = [g]_B^D$. Then
    \begin{align}
        (f + g)(v_j) = f(v_j) + g(v_j) &= \sum_{i=1}^{m}P_{ij}w_i + \sum_{i = 1}^{m}Q_{ij}w_i \\
        &= \sum_{i=1}^{m}\left(P_{ij} + Q_{ij}\right)w_i.
    \end{align}
    Therefore, $[f + g]_B^D = P + Q = [f]_B^D + [g]_B^D$.

    Similarly,
    \begin{align*}
        (rf)(v_j) = rf(v_j) = r\sum_{i=1}^{m}P_{ij}w_i,
    \end{align*}
    so $[rf]_B^D = rP = r[f]_B^D$.
\end{proof}

\begin{rmk}
    Given $B, D$, the natural map from $L(V, W)$ to $M_{m\times n}(F)$ is an isomorphism.
\end{rmk}

\begin{rmk}
    Note that matrix multiplication is not commutative. For example, if $A \in M_{3 \times 2}(F)$ and $B \in M_{2 \times 5}$, then $AB$ is defined but $BA$ does not exist.

    If $A \in M_{3 \times 2}(F)$ and $B \in M_{2 \times 3}(F)$, then $AB \in M_{3 \times 3}(F)$ but $BA \in M_{2 \times 2}(F)$.

    Even if $A, B \in M_{n \times n}(F)$, $AB$ is not necessarily (or even often) equal to $BA$.
\end{rmk}

\begin{defn}
    Let $F$ be any field. Then the \emph{$n \times n$ identity matrix}, denoted by $I$ or $I_n$ is the matrix with ones on the major diagonal and zeroes elsewhere.
\end{defn}

\begin{exmp}
    \[I_3 = \begin{pmatrix}
        1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
    \end{pmatrix}\]
\end{exmp}

\begin{prop}
    For $A \in M_{m \times n}$, $AI_n = A = I_mA$.
\end{prop}

\begin{rmk}
    Consider $id: V \to V$. Let $B$ be the ordered basis for $V$. Then $[id]_B^B = I$.
\end{rmk}

\begin{defn}
    Let $A \in M_{m \times n}(F)$ and $B \in M_{n \times m}(F)$ such that $AB = I_m$. Then we say $A$ is a \emph{left inverse} of $B$ and $B$ is a \emph{right inverse} of $A$. If we also have $BA = I_n$, then we say $A$ and $B$ are \emph{inverses}, and that $A$ and $B$ are \emph{invertible}, or \emph{non-singular}.
\end{defn}

\begin{exmp}
    \[\begin{pmatrix}
        3 & 1 & 0
    \end{pmatrix}\begin{pmatrix}
        0 \\ 1 \\ 2
    \end{pmatrix} = I_1,\]
    but
    \[\begin{pmatrix}
        0 \\ 1 \\ 2
    \end{pmatrix}\begin{pmatrix}
        3 & 1 & 0
    \end{pmatrix} \neq I_3.\]
\end{exmp}

\begin{lemma}\label{matrix-inverse-unique}
    If a matrix has both a left and right inverse, then the two must coincide, and hence $A$ is invertible. Moreover, if $A$ is invertible, the inverse is unique.
\end{lemma}

\begin{defn}
    We denote the inverse of an invertible matrix $A$ by $A^{-1}$.
\end{defn}

\begin{proof}
    Assume $CA = I_n$ and $AB = I_m$. Then
    \[C = CI_m = C(AB) = (CA)B = I_nB = B.\]
\end{proof}

\begin{rmk}
    If $A$ is invertible, it is enough to check that $AB = I_m$ or $BA = I_n$ to show that $B = A^{-1}$.
\end{rmk}

\begin{thm}\label{matrix-invertible-rank}
    A matrix is invertible if and only if it is an $n \times n$ matrix of rank $n$ for some $n \in N$.
\end{thm}

\begin{proof}
    Suppose $A$ is an invertible $m \times n$ matrix, so $AA^{-1} = I_m$ and $A^{-1}A = I_n$. Consider $L_A: F^n \to F^m$ and $L_{A^{-1}}: F^m \to F^n$. Then $L_{A} \odot L_{A^{-1}} = L_{AA^{-1}} = L_{I_m} = id$. Similarly, $L_{A^{-1}} \odot L_A = id$. Therefore, $L_{A}$ is an isomorphism and so the dimension of $F^m$ and $F^n$ is the same, giving $m = n$.

    Conversely, suppose $A$ is an $n \times n$ matrix of rank $n$. Then $L_A: F^n \to F^n$, and so by Theorem \ref{transformation-matrix-rank-equality} $L_A$ is a bijection and therefore an isomorphism. It follows that
    \[\left[\left(L_A\right)^{-1}\right]_{sdt_n}^{std_n} = A^{-1}.\]
\end{proof}

\begin{exmp}
    \[A = \begin{pmatrix}
        -1 & 1 \\ 2 & -3
    \end{pmatrix}.\]
    Then
    \begin{align*}
        \begin{amatrix}{2}
            -1 &1 &1 &0 \\
            2 &-3 &0 &1
        \end{amatrix}
        & \begin{aligned}
            & \ro{r_2 \rightarrow r_2 + 2r_1}  \\
        \end{aligned}
        \begin{amatrix}{2}
            -1 &1 &1 &0 \\
            0 &-1 &2 &1
        \end{amatrix}
        & \begin{aligned}
            & \ro{r_1 \rightarrow -r_1 - r_2}  \\
            & \ro{r_2 \rightarrow -r_2}  \\
        \end{aligned}
        \begin{amatrix}{2}
            1 &0 &-3 &-1 \\
            0 &1 &-2 &-1
        \end{amatrix}.
    \end{align*}
    It follows that $A$ is invertible, since $I_2$ clearly has rank two, and
    \[A^{-1} = \begin{pmatrix}
        -3 & -1 \\ -2 & -1
    \end{pmatrix}.\]
\end{exmp}

\begin{prop}
    Given a square matrix $A$, perform the row reduction $(A|I_n) \mapsto (E|B)$, where $E$ is the (unique) reduced row echelon form of $A$. Then $A$ is invertible with inverse $B$ if and only if $E = I_n$.
\end{prop}

We will explore two reasons to explain this.

\textbf{Reason 1:} Since
\[\begin{amatrix}{2}
    -1 & 1 & 1 \\ 2 & -3 & 0
\end{amatrix} \mapsto \begin{amatrix}{2}
    1 & 0 & -3 \\ 0 & 1 & -2
\end{amatrix},\]
$(-3, -2)$ is the unique solution to the system
\[\begin{amatrix}{2}
    -1 & 1 & x \\ 2 & -3 & y
\end{amatrix} = \begin{bmatrix}
    1 \\ 0
\end{bmatrix}.\] Similarly, $A(x, y) = (0, 1)$ has only the solution $(-1, 1)$. Therefore,
\[\begin{bmatrix}
    -1 & 1 \\ 2 & -3
\end{bmatrix}\begin{bmatrix}
    -3 & -1 \\ -2 & 1
\end{bmatrix} = I_2.\]

By Theorem \ref{solutions-unchanged-by-row-ops}, if $(A|I_n) \mapsto (I_n|B)$ then the $i$th column of $B$ must be solution to $A\vec{v} = \vec{i}$, where $\vec{i}$ is the $i$th column of $I_n$. Therefore, $AB = I_n$.

\begin{defn}
    An $n \times n$ \emph{elementary} matrix is a matrix that can be obtained from $I_n$ by a single row operation.
\end{defn}

\begin{lemma}
    Suppose $A \mapsto B$ via a single row operation, and $E$ is the corresponding elementary matrix obtained by performing the same row operation on $I_n$. Then $B = EA$.
\end{lemma}

\begin{exmp}
    \[\begin{pmatrix}
        1 & 2 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
    \end{pmatrix}\begin{pmatrix}
        3 & 2 \\ -1 & 4 \\ 0 & -1
    \end{pmatrix} = \begin{pmatrix}
        1 & 10 \\ -1 & 4 \\ 0 & -1
    \end{pmatrix}.\]

    Here, $E$ can be obtained from $I_3$ by $r_1 \to r_1 + 2r_2$, and $B$ can be obtained from $A$ by $r_1 \to r_1 + 2r_2$ as well.
\end{exmp}

\textbf{Reason 2.}

If $(A|A') \mapsto (B|B')$ via elementary row operations, then there must exist elementary matrices $E_1, E_2, \ldots, E_k$ such that
\begin{align*}
    B &= E_kE_{k-1}\cdots E_2E_1A \\
    B' &= E_kE_{k-1}\cdots E_2E_1A'
\end{align*}

In particular, if $(A|I_n) \mapsto (I_n|B')$, then
\begin{align*}
    I_n &= E_kE_{k-1}\cdots E_2E_1A \\
    B' &= E_kE_{k-1}\cdots E_2E_1I_n,
\end{align*}
and so $I_n = B'A$.

\section{Change of Basis}

\begin{exmp}\label{projection-basis-change-exmp}
    Consider the projection of $\R^3 \to \R^3$ onto the plane
    \[H = \left\{\left(x, y, z\right) \in \R^3 \compbar x + y + z = 0\right\}.\] Let $f$ represent this linear transformation. Note that $(1, -1, 0)$ and $(1, 0, -1) \in H$, so $f(1, -1, 0) = (1, -1, 0)$ and $f(1, 0, -1) = (1, 0, -1)$. We can also see that $(x, y, z) \cdot (1, 1, 1) = 0$ for any $(x, y, z) \in H$, so $f(1, 1, 1) = (0, 0, 0)$.
    Consider the ordered basis composed of these three vectors:
    \[B = \langle (1, 1, 1), (1, -1, 0), (1, 0, -1) \rangle.\] Then we can write
    \[[f]_B^B = \begin{bmatrix}
        0 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}.\] In order to find $[f]_{std_3}^{std_3}$, we can perform a \emph{change of basis}.
\end{exmp}

\begin{lemma}\label{inverse-transformation-matrix}
    Let $V, W$ be a finite $n$ dimensional vector spaces over a field $F$, and $B, D$ be ordered bases for $V$ and $W$ respectively. If $f$ is a linear transformation which is an isomorphism from $V$ to $W$ and $B, D$ are ordered bases for $V$ and $W$ respectively, then
    $[f]_B^D$ is invertible with
    \[[f^{-1}]_D^B = \left([f]_B^D\right)^{-1}.\]
\end{lemma}

Let $B$ and $C$ be ordered basis for a vector space $V$, and consider $v \in V$ and the invertible $n \times n$ matrix $[id_V]_B^C$. Then we can see that
\[[v]_C = [id_V]_B^C[v]_B.\]

\begin{defn}
    We call $[id_V]_B^C$ the \emph{change of basis matrix} from $B$ to $C$.
\end{defn}

\begin{exmp}
    Consider the bases
    \begin{align*}
        B &= \langle (1, 1, 1), (1, -1, 0), (1, 0, -1) \rangle \\
        C &= \langle (1, 0, 0), (0, 1, 0), (0, 0, 1) \rangle
    \end{align*}
    for $\R^3$ (note that $C = std_3$). Then
    \begin{align*}
        [id_V]_C^B = \begin{bmatrix}
            1 & 1 & 1 \\
            1 & -1 & 0 \\
            1 & 0 & -1
        \end{bmatrix}.
    \end{align*}

    We can use Gaussian elimination to invert this matrix to find $\left([id_V]_C^B\right)^{-1}$, which by Lemma \ref{inverse-transformation-matrix} is $[id_V]_B^C$.
    \[[id_V]_B^C = \begin{bmatrix}
        1/3 & 1/3 & 1/3 \\
        1/3 & -2/3 & 1/3 \\
        1/3 & 1/3 & -2/3.
    \end{bmatrix}\]

    Consider the transformation from Example \ref{projection-basis-change-exmp}, where we found $[f]_B^B$. We know that
    \[[f]_{std_3}^{std_3} = [f]_C^C = [id_V]_B^C[f]_B^B[id_V]_C^B.\]
    \begin{align*}
        [f]_{std_3}^{std_3} &= \begin{bmatrix}
            1/3 & 1/3 & 1/3 \\
            1/3 & -2/3 & 1/3 \\
            1/3 & 1/3 & -2/3.
        \end{bmatrix}\begin{bmatrix}
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}\begin{bmatrix}
            1 & 1 & 1 \\
            1 & -1 & 0 \\
            1 & 0 & -1
        \end{bmatrix} \\
        &= \begin{bmatrix}
            2/3 & -1/3 & -1/3 \\
            -1/3 & 2/3 & -1/3 \\
            -1/3 & -1/3 & 2/3
        \end{bmatrix}.
    \end{align*}
\end{exmp}

\begin{lemma}
    If $B$ and $C$ are ordered bases for a vector space $V$ and $P$ is the change of basis matrix from $B$ to $C$, then $P^{-1}$ is the change of basis matrix from $C$ to $B$.
\end{lemma}

\begin{thm}
    Let $V$ and $W$ be finite dimensional vector spaces over a field $F$, and $B, B'$ and $D, D'$ be ordered bases for $V$ and $W$ respectively. Let $P$ be the change of basis matrix from $B$ to $B'$, and $Q$ be the change of basis matrix from $D$ to $D'$. Then for a linear transformation $f$,
    \[[f]_{B'}^{D'} = Q[f]_{B}^{D}P^{-1}.\]
\end{thm}

\section{Determinants}

When is an $n \times n$ matrix invertible? When $n = 1$, $[a]$ is invertible when $a \neq 0$, in which case $[a]^{-1} = [1/a]$. When $n = 2$, we can show that
\[\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}\] is invertible when $ad - bc \neq 0$, in which case
\[\begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}^{-1} = \frac{1}{ad - bc}\begin{bmatrix}
    d & -b \\
    c & a
\end{bmatrix}.\]

\begin{rmk}
    Let $A \in M_{n \times n}(F)$, and let $r_1, \ldots, r_n$ be the rows of $A$. We will denote $A$ by $(r_1, \ldots, r_n)$.
\end{rmk}

\begin{defn}\label{det-properties}
    A \emph{determinant} is a function $\det M_{n \times n}(F) \to F$ which satisfies the following properties:
    \begin{itemize}
        \item Multilinearity: it is multilinear on the rows, i.e. linear on each row. If $r_1, \ldots, r_n \in F^n$, $r_i' \in F^n$, and $s \in F$, then
        \begin{align*}
            \det\left(r_1, \ldots, sr_i' + r_i, \ldots, r_n\right) = \det\left(r_1, \ldots, r_i, \ldots, r_n\right) + s\det\left(r_1, \ldots, r_i', \ldots, r_n\right).
        \end{align*}
        \item Alternating: $\det(r_1, \ldots, r_n) = 0$ if $r_i = r_j$ for some $i \neq j$.
        \item Normalized: $\det(I) = 1$.
    \end{itemize}
\end{defn}

Throughout this section, we will show that a function satisfying all of these properties does in fact exist, and furthermore that it must be unique.

\begin{exmp}
    Consider $\det: M_{2 \times 2}(F) \to F$ given by
    \[\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix} \mapsto ad - bc.\]
    We can show that this is a determinant. Firstly, it is multilinear:
    \[\begin{bmatrix}
        a + sa' & b + sb' \\ c & d
    \end{bmatrix} \mapsto (a + sa')d - (b + sb')c = \left(ad - bc\right) + s\left(a'd - b'c\right).\]
    Secondly, it is alternating: if $(a, b) = (c, d)$, then $ad - bc = ab - ba = 0$. Finally, it is normalized:
    \[\begin{bmatrix}
        1 & 0 \\ 0 & 1
    \end{bmatrix} \mapsto 1(1) - 0(0) = 1.\]
\end{exmp}

\begin{prop}\label{det-row-ops}
    Let $\det$ be a determinant function, and $A, B \in M_{n \times n}(F)$ where
    \begin{align*}
        A &= (r_1, \ldots, r_n). \\
    \end{align*}
    Choose two rows $r_i, r_j$ from $A$, $i \neq j$.
    If $B$ can be obtained from $A$ by swapping two rows, then
    \begin{equation}\label{det-row-swap}\tag{$1$}
        \det B = -\det A.
    \end{equation}
    If instead $B$ can be obtained from $A$ by adding $kr_j$ to $r_i$, then
    \begin{equation}\label{det-add-multiple}\tag{$2$}
        \det B = \det A.
    \end{equation}
\end{prop}

\begin{proof}
    In the first case, by the alternating property of $\det$, we know that
    \[\det (r_1, \ldots, r_i + r_j, r_i + r_j, \ldots, r_n) = 0,\]
    and by the multilinearity property we have
    \begin{align*}
        \det (r_1, \ldots, r_i + r_j, r_i + r_j, \ldots, r_n) &= \det (r_1, \ldots, r_i, r_i + r_j, \ldots, r_n) + \det (r_1, \ldots, r_j, r_i + r_j, \ldots, r_n) \\
        &= \det (r_1, \ldots, r_i, r_i, \ldots, r_n) + \det (r_1, \ldots, r_j, r_j, \ldots, r_n)\\
        &\quad\quad + \det (r_1, \ldots, r_i, r_j, \ldots, r_n) + \det (r_1, \ldots, r_j, r_i, \ldots, r_n).
    \end{align*}
    Since the first two terms of the last expression have a repeated row, by the alternating property they must be zero, and so
    \begin{align*}
        0 &= \det (r_1, \ldots, r_i, r_j, \ldots, r_n) + \det (r_1, \ldots, r_j, r_i, \ldots, r_n) \\
        &= \det A + \det B.
    \end{align*}

    In the second case,
    \begin{align*}
        \det B &= \det\left(r_1, \ldots, kr_j + r_i, \ldots, r_j, \ldots, r_n\right) \\
        &= \det\left(r_1, \ldots, r_i, \ldots, r_j, \ldots, r_n\right) + k\det\left(r_1, \ldots, r_j, \ldots, r_j, \ldots, r_n\right) \\
        &= \det A + 0.
    \end{align*}
\end{proof}

\begin{exmp}
    \begin{align*}
        \det \begin{bmatrix}
            -3 & 1 & 2 \\ 0 & 4 & 5 \\ 0 & 0 & 6
        \end{bmatrix} &= (-3)\det\begin{bmatrix}
            1 & -1/3 & -2/3 \\ 0 & 4 & 5 \\ 0 & 0 & 6
        \end{bmatrix} = (-3)(4)\begin{bmatrix}
            1 & -1/3 & -2/3 \\ 0 & 1 & 5/4 \\ 0 & 0 & 6
        \end{bmatrix} \\
        &= (-3)(4)(6)\begin{bmatrix}
            1 & -1/3 & -2/3 \\ 0 & 1 & 5/4 \\ 0 & 0 & 1
        \end{bmatrix} = (-3)(4)(6)\begin{bmatrix}
            1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
        \end{bmatrix} \\
        &= (-3)(4)(6)\det I = -72.
    \end{align*}
\end{exmp}

\begin{rmk}
    Consider a matrix $A$, and perform Gauss-Jordan elimination to get the reduced row echelon form $E$. By multilinearity, multiplying a row by $k \neq 0$ multiplies the determinant by $k$, and by Proposition \ref{det-row-ops} the other row ops either negate the determinant, or leave it unchanged. Therefore, $\det A = k\det E$ for some $k \neq 0$.
\end{rmk}

\begin{prop}\label{det-zero}
    Let $\det$ be a determinant function. Then
    \begin{enumerate}[label=(\arabic*)]
        \item If $A$ has a row of all zeroes, then $\det A = 0$.
        \item $A$ is invertible if and only if $\det A \neq 0$.
        \item The determinant of an upper triangular matrix is the product of the major diagonal entries.
    \end{enumerate}
\end{prop}

\begin{proof}\proofbreak
    \begin{enumerate}[label=(\arabic*)]
        \item Let $A$ be a matrix with $r_i = \vec{0}$. Then consider $B$ obtained from $A$ by adding $r_j$ to $r_i$, so $B$ contains $r_j$ twice. By the alternating property, $\det B = 0$, and by Proposition \ref{det-row-ops} (\ref{det-add-multiple}) we know $\det A = \det B$, so we have $\det A = 0$.
        \item We know that $\det A = k\det E$, where $E$ is the reduced row echelon form of $A$. If $A$ is invertible, then $\det A = k\det E = k\det I = k$, where $k \neq 0$. If $A$ is not invertible then $A$ must have rank $m < n$ by Theorem \ref{matrix-invertible-rank}. It follows by Lemma \ref{row-ops-row-space} that $E$ must contain a row of zeroes. Therefore, $\det E = 0$, and since $\det A = k\det E$ we have $\det A = 0$.
        \item \begin{align*}
            \det\begin{bmatrix}
                a_{11} & a_{12} & \cdots & a_{1n} \\
                0 & a_{22} & \cdots & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & a_{nn}
            \end{bmatrix} &= a_{11}a_{22}\cdots a_{nn}\det\begin{bmatrix}
                1 & a_{12}/a_{11} & \cdots & a_{1n}/a_{11} \\
                0 & 1 & \cdots & a_{2n}/{a_22} \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & 1
            \end{bmatrix} \\
            &= a_{11}a_{22}\cdots a_{nn}\det I = a_{11}a_{22}\cdots a_{nn}.
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{defn}
    Let $A$ be an $m \times n$ matrix. The \emph{transpose} of $A$, denoted by $A^{\transpose}$ is the $n \times m$ matrix given by
    \[\left(A^{\transpose}\right)_{ij} = A_{ji}.\]
\end{defn}

\begin{exmp}
    \[\begin{pmatrix}
        a & b \\
        c & d
    \end{pmatrix}^{\transpose} = \begin{pmatrix}
        a & c \\ & b & d
    \end{pmatrix}.\]

    Note that $\det A = ad - bc$, and $\det A^{\transpose} = ad - cb$, so $\det A = \det A^{\transpose}$.
\end{exmp}

\begin{thm}
    Let $A$ and $B$ be $n \times n$ matrices. Then
    \begin{enumerate}[label=(\arabic*)]
        \item $\left(AB\right)^{\transpose} = B^{\transpose}A^{\transpose}$.
        \item If $A$ is invertible, so is $A^{\transpose}$ and $\left(A^{\transpose}\right)^{-1} = \left(A^{-1}\right)^{\transpose}$.
    \end{enumerate}
\end{thm}
